{"version":3,"file":"static/chunks/ba12c10f-aaafcb074f608624.js","mappings":"gKAuOAA,mCAvOAC,EAAAC,OAAAC,cAAA,CA6BAC,OAAAC,GAAA,CADA,6CAG2CC,EAAAC,EAAU,CAqFrDH,OAAAC,GAAA,CADA,2CAGyCC,EAAAC,EAAW,CAwBpDH,OAAAC,GAAA,CADA,6CAG2CC,EAAAC,EAAW,CAmBtDH,OAAAC,GAAA,CADA,4CAG0CC,EAAAC,EAAW,CAsBrDH,OAAAC,GAAA,CADA,qCAGmCC,EAAAC,EAAW,CAmB9CH,OAAAC,GAAA,CADA,4CAG0CC,EAAAC,EAAW,CAkBrD,IAAAC,EAAA,4BACAC,EAAA,mBAAiCD,EAAM,EACvCE,EAAAN,OAAAC,GAAA,CAAAI,GAEAE,EAAA,cAA2CL,EAAAC,EAAW,CACtDK,YAAA,CACAC,QAAAA,EAAA,uBACAC,MAAAA,CAAA,CACAC,KAAAC,CAAA,CACAC,SAAAA,CAAA,CACAC,MAAAA,CAAA,CACAC,aAAAA,CAAA,CACG,EACH,OAAYC,KAAAZ,EAAAK,QAAAA,EAAAC,MAAAA,CAAA,GACZ,KAAAd,EAAA,IACA,KAAAe,IAAA,CAAAC,EACA,KAAAC,QAAA,CAAAA,EACA,KAAAC,KAAA,CAAAA,EACA,KAAAC,YAAA,CAAAA,CACA,CACA,OAAAE,WAAAC,CAAA,EACA,OAAWhB,EAAAC,EAAW,CAAAgB,SAAA,CAAAD,EAAAb,EACtB,CACA,EACAT,EAAAU,EAMAN,OAAAC,GAAA,CADA,6CAG2CC,EAAAC,EAAW,CAmBtDH,OAAAC,GAAA,CADA,sCAGoCC,EAAAC,EAAW,CAqB/CH,OAAAC,GAAA,CADA,0CAGwCC,EAAAC,EAAY,CAkBHD,EAAAC,EAAY,CAgB7DH,OAAAC,GAAA,CADA,8CAG4CC,EAAAC,EAAY,CAoBxDH,OAAAC,GAAA,CADA,8CAG4CC,EAAAC,EAAY,CAmBxDH,OAAAC,GAAA,CADA,6CAG2CC,EAAAC,EAAY,CAmBvDH,OAAAC,GAAA,CADA,oCAGkCC,EAAAC,EAAY,CAwB9CH,OAAAC,GAAA,CADA,iCAG+BC,EAAAC,EAAY,CAqR3C,IAAAiB,EAAwBC,EAAAC,EAAO,EAC7BD,EAAAE,EAAQ,GACRF,EAAAG,EAAY,CAAAC,YACZJ,EAAAG,EAAY,CAAAE,aACZL,EAAAM,EAAQ,CAEV,IACA,IAAAC,EAAAC,EACA,aAAAA,CAAAA,EAAA,MAAAD,CAAAA,EAAAE,WAAAC,MAAA,SAAAH,EAAAI,QAAA,CAAAC,EAAA,GAAAJ,CACA,EACA,CAAMpB,QAAA,qBAEN,EA4bAyB,EAAsBb,EAAAc,EAAO,CAC7B,IAAQd,EAAAC,EAAQ,EACZD,EAAAe,EAAO,GACPf,EAAAE,EAAS,GACTF,EAAAgB,EAAS,GACThB,EAAAiB,EAAU,GACVjB,EAAAkB,EAAS,CAAClB,EAAAE,EAAS,GAAAW,GACnBb,EAAAmB,EAAQ,CAAAN,GACZ,GAIAO,EAA6BpB,EAAAkB,EAAS,CACpClB,EAAAE,EAAS,GACTF,EAAAkB,EAAS,CAAClB,EAAAE,EAAS,GAAAW,IAKrBQ,EAAqBrB,EAAAsB,EAAS,EAC9BC,KAAQvB,EAAAwB,EAAU,SAClBlC,KAAQU,EAAAE,EAAS,GACjBuB,gBAAAL,EAAAM,QAAA,EACA,GACAC,EAAsB3B,EAAAsB,EAAS,EAC/BC,KAAQvB,EAAAwB,EAAU,UAClBI,MAAS5B,EAAAC,EAAQ,EAAAF,EAAqBC,EAAAG,EAAa,CAAA0B,KAAA,EACnDC,UAAa9B,EAAAE,EAAS,GAAAwB,QAAA,GACtBD,gBAAAL,EAAAM,QAAA,EACA,GACAK,EAAqB/B,EAAAsB,EAAS,EAC9BC,KAAQvB,EAAAwB,EAAU,SAClBQ,KAAQhC,EAAAC,EAAQ,EAAAF,EAAqBC,EAAAG,EAAa,CAAA0B,KAAA,EAClDI,SAAYjC,EAAAE,EAAS,GAAAwB,QAAA,GACrBI,UAAa9B,EAAAE,EAAS,GACtBuB,gBAAAL,EAAAM,QAAA,EACA,GACAQ,EAA0BlC,EAAAsB,EAAS,EACnCC,KAAQvB,EAAAwB,EAAU,cAClBlC,KAAQU,EAAAE,EAAS,GACjBuB,gBAAAL,EAAAM,QAAA,EACA,GACAS,EAAyBnC,EAAAsB,EAAS,EAClCC,KAAQvB,EAAAwB,EAAU,cAClBY,WAAcpC,EAAAE,EAAS,GACvBmC,SAAYrC,EAAAE,EAAS,GACrBoC,MAAStC,EAAAuC,EAAU,GACnBd,gBAAAL,EAAAM,QAAA,GACAc,iBAAoBxC,EAAAiB,EAAU,GAAAS,QAAA,EAC9B,GACAe,EAAmBzC,EAAA0C,EAAqB,SACtC1C,EAAAsB,EAAS,EACXC,KAAUvB,EAAAwB,EAAU,SACpBZ,MAAWZ,EAAAE,EAAS,EACpB,GACEF,EAAAsB,EAAS,EACXC,KAAUvB,EAAAwB,EAAU,SACpBZ,MAAAC,CACA,GACEb,EAAAsB,EAAS,EACXC,KAAUvB,EAAAwB,EAAU,eACpBZ,MAAWZ,EAAAE,EAAS,EACpB,GACEF,EAAAsB,EAAS,EACXC,KAAUvB,EAAAwB,EAAU,eACpBZ,MAAAC,CACA,GACEb,EAAAsB,EAAS,EACXC,KAAUvB,EAAAwB,EAAU,YACpBZ,MAAWZ,EAAAmB,EAAQ,CACbnB,EAAAC,EAAQ,EACND,EAAAsB,EAAS,EACjBC,KAAgBvB,EAAAwB,EAAU,SAC1BlC,KAAgBU,EAAAE,EAAS,EACzB,GACQF,EAAAsB,EAAS,EACjBC,KAAgBvB,EAAAwB,EAAU,UAC1BQ,KAAgBhC,EAAAE,EAAS,GACzB4B,UAAqB9B,EAAAE,EAAS,EAC9B,GACA,EAEA,GACA,EACAyC,EAA2B3C,EAAAsB,EAAS,EACpCC,KAAQvB,EAAAwB,EAAU,gBAClBY,WAAcpC,EAAAE,EAAS,GACvBmC,SAAYrC,EAAAE,EAAS,GACrB0C,OAAAH,EACAhB,gBAAAL,EAAAM,QAAA,EACA,GAGAmB,EAA+B7C,EAAAsB,EAAS,CACxC,CACAwB,KAAU9C,EAAAwB,EAAU,WACpBuB,QAAa/C,EAAAE,EAAS,GACtBuB,gBAAAL,EAAAM,QAAA,EACA,GAGAsB,EAA6BhD,EAAAsB,EAAS,EACtCwB,KAAQ9C,EAAAwB,EAAU,SAClBuB,QAAW/C,EAAAC,EAAQ,EACfD,EAAAE,EAAS,GACTF,EAAAmB,EAAQ,CAACnB,EAAAC,EAAQ,EAAAoB,EAAAM,EAAAI,EAAA,GACrB,EACAN,gBAAAL,EAAAM,QAAA,EACA,GAEAuB,EAAkCjD,EAAAsB,EAAS,EAC3CwB,KAAQ9C,EAAAwB,EAAU,cAClBuB,QAAW/C,EAAAC,EAAQ,EACfD,EAAAE,EAAS,GACTF,EAAAmB,EAAQ,CACNnB,EAAAC,EAAQ,EACdoB,EACAU,EACAG,EACAC,EACAQ,EACA,GAEA,EACAlB,gBAAAL,EAAAM,QAAA,EACA,GAEAwB,EAA6BlD,EAAAsB,EAAS,EACtCwB,KAAQ9C,EAAAwB,EAAU,SAClBuB,QAAW/C,EAAAmB,EAAQ,CAAAwB,GACnBlB,gBAAAL,EAAAM,QAAA,EACA,GAEyB1B,EAAAC,EAAQ,EACjC4C,EACAG,EACAC,EACAC,EACA,EAgxByB,GAAAC,EAAAC,EAAA,EAAiB,CAC1CC,OAAA,QACAC,KAAA,EACA,GAspBAC,gBAiEA,IAAAC,EAA2BxD,EAAAC,EAAQ,EACjCD,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,eACpBkC,GAAQ1D,EAAAE,EAAS,GACjByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,eACpBkC,GAAQ1D,EAAAE,EAAS,GACjB0D,MAAW5D,EAAAE,EAAS,GACpByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,aACpBkC,GAAQ1D,EAAAE,EAAS,GACjByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,UACpBqC,UAAe7D,EAAAE,EAAS,EACxB,GACEF,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,qBACpBY,WAAgBpC,EAAAE,EAAS,GACzBmC,SAAcrC,EAAAE,EAAS,GACvBsC,iBAAsBxC,EAAAiB,EAAU,GAAAS,QAAA,GAChCoC,QAAa9D,EAAAiB,EAAU,GAAAS,QAAA,EACvB,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,qBACpBY,WAAgBpC,EAAAE,EAAS,GACzB6D,eAAoB/D,EAAAE,EAAS,EAC7B,GACEF,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,yBACpBY,WAAgBpC,EAAAE,EAAS,GACzBmC,SAAcrC,EAAAE,EAAS,GACvBoC,MAAWtC,EAAAuC,EAAU,GACrBC,iBAAsBxC,EAAAiB,EAAU,GAAAS,QAAA,GAChCiC,iBAAAvC,EAAAM,QAAA,GACAoC,QAAa9D,EAAAiB,EAAU,GAAAS,QAAA,EACvB,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,qBACpBY,WAAgBpC,EAAAE,EAAS,GACzBmC,SAAcrC,EAAAE,EAAS,GACvBoC,MAAWtC,EAAAuC,EAAU,GACrBC,iBAAsBxC,EAAAiB,EAAU,GAAAS,QAAA,GAChCiC,iBAAAvC,EAAAM,QAAA,GACAoC,QAAa9D,EAAAiB,EAAU,GAAAS,QAAA,GACvBmC,UAAe7D,EAAAE,EAAS,EACxB,GACEF,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,0BACpBY,WAAgBpC,EAAAE,EAAS,GACzB0C,OAAY5C,EAAAuC,EAAU,GACtBC,iBAAsBxC,EAAAiB,EAAU,GAAAS,QAAA,GAChCoC,QAAa9D,EAAAiB,EAAU,GAAAS,QAAA,GACvBsC,YAAiBhE,EAAAiB,EAAU,GAAAS,QAAA,EAC3B,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,sBACpBY,WAAgBpC,EAAAE,EAAS,GACzB2D,UAAe7D,EAAAE,EAAS,GACxBsC,iBAAsBxC,EAAAiB,EAAU,GAAAS,QAAA,GAChCoC,QAAa9D,EAAAiB,EAAU,GAAAS,QAAA,EACvB,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,cACpBlC,KAAUU,EAAAE,EAAS,GACnByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,oBACpBkC,GAAQ1D,EAAAE,EAAS,GACjByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,oBACpBkC,GAAQ1D,EAAAE,EAAS,GACjB0D,MAAW5D,EAAAE,EAAS,GACpByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,kBACpBkC,GAAQ1D,EAAAE,EAAS,GACjByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,yBACpB,GACExB,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,eACpByC,SAAcjE,EAAAE,EAAS,GACvBgE,IAASlE,EAAAE,EAAS,GAClBiE,MAAWnE,EAAAE,EAAS,GAAAwB,QAAA,GACpBiC,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,oBACpByC,SAAcjE,EAAAE,EAAS,GACvB4B,UAAe9B,EAAAE,EAAS,GACxBiE,MAAWnE,EAAAE,EAAS,GACpB+B,SAAcjC,EAAAE,EAAS,GAAAwB,QAAA,GACvBiC,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,SACpB0C,IAASlE,EAAAE,EAAS,GAClB4B,UAAe9B,EAAAE,EAAS,GACxByD,iBAAAvC,EAAAM,QAAA,EACA,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAE,EAAS,GAAAkE,UAAA,UACnBV,GAAQ1D,EAAAE,EAAS,GAAAwB,QAAA,GACjBM,KAAUhC,EAAAuC,EAAU,GACpB8B,UAAerE,EAAAiB,EAAU,GAAAS,QAAA,EACzB,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,cACpB,GACExB,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,eACpB,GACExB,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,UACpB8C,UAAetE,EAAAE,EAAS,GAAAwB,QAAA,GACxB6C,gBAAqBvE,EAAAuC,EAAU,GAAAb,QAAA,EAC/B,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,WACpB+C,gBAAqBvE,EAAAuC,EAAU,GAAAb,QAAA,EAC/B,GACE1B,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,SACpB,GACExB,EAAAyD,EAAe,EACjBlC,KAAUvB,EAAAwB,EAAU,qBACpB+C,gBAAqBvE,EAAAuC,EAAU,EAC/B,GACA,EAwWA,eAAAiC,EAAAC,CAAA,EACA,GAAAA,KAAA,IAAAA,EACA,OAAa7D,MAAA,OAAA8D,MAAA,mBAEb,IAAAC,EAAA,MAAqB,GAAAxB,EAAAyB,EAAA,EAAc,CAAGtF,KAAAmF,CAAA,UACtC,EAAAI,OAAA,CACA,CAAajE,MAAA+D,EAAA/D,KAAA,CAAA8D,MAAA,oBAGbC,CADAA,EAAA,MAAiB,GAAAxB,EAAAyB,EAAA,EAAc,CAAGtF,KAAAwF,SAtUlCxC,CAAA,EACA,IAAAyC,EAAA,SACAC,EAAA,GACAC,EAAA,KACA,SAAAC,EAAAC,CAAA,CAAAC,CAAA,CAAAC,CAAA,EAEA,OAAAF,GACA,QACAH,EAAAI,EACAL,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,kBACA,KAEA,SACA,QACA,QACAP,EAAAI,EACAH,EAAAG,EACAL,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,mBACA,KAEA,SACAR,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,kBACA,KAEA,SACA,QACA,QACA,QACA,QACA,QACA,QACA,QACA,QACA,QACAP,EAAAI,EACAL,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,kBACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,wBACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,GACAP,EAAAQ,IAAA,CAAAF,GACAN,EAAAQ,IAAA,sBAGA,CAEA,CACA,SAAAC,EAAAL,CAAA,CAAAC,CAAA,EACA,OAAAD,GACA,QACAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,8BACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,EAGA,CACA,CACA,SAAAG,EAAAN,CAAA,CAAAC,CAAA,EACA,OAAAD,GACA,QACAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,6BACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,EAGA,CACA,CACA,QAAAF,EAAA,EAAkBA,EAAA9C,EAAAoD,MAAA,CAAkBN,IAAA,CACpC,IAAAD,EAAA7C,CAAA,CAAA8C,EAAA,CAEA,OADAL,CAAA,CAAAA,EAAAW,MAAA,KAEA,WACAR,EAAAC,EAAAC,EAAA,UACA,KACA,2BACA,OAAAD,GACA,QACAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,sBACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,EAGA,CACA,KAEA,iCAEA,MADAH,IAEAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,uBAIA,KAEA,yBAEA,MADAJ,IAEAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,6BAIA,KAEA,+BAEA,MADAJ,IAEAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,gCAIA,KAEA,kCACAL,EAAAC,EAAAC,EAAA,6BACA,KAEA,iCACAI,EAAAL,EAAAC,GACA,KAEA,qBACA,OAAAD,GACA,QACAJ,EAAAO,GAAA,GACAN,EAAAI,EACA,KAEA,UACAL,EAAAQ,IAAA,yBACA,KAEA,SACAP,EAAAI,CAEA,CACA,KAEA,0BAEA,MADAD,GAEAH,EAAAI,EACAL,EAAAO,GAAA,KAIAN,EAAAI,EACAF,EAAAC,EAAAC,EAAA,6BAIA,KAEA,gCACA,OAAAD,GACA,QACAJ,EAAAO,GAAA,GACAP,EAAAQ,IAAA,6BACA,KAEA,SACAP,EAAAI,EACAL,EAAAO,GAAA,GACA,KAEA,SACAN,EAAAI,CAGA,CACA,KAEA,gCACAF,EAAAC,EAAAC,EAAA,4BACA,KAEA,4BACAL,EAAAO,GAAA,GACAN,EAAAI,EACA,KAEA,qBACA,OAAAD,GACA,QACA,QACA,QACA,QACA,QACA,QACA,QACA,QACA,QACA,QACAH,EAAAI,EACA,KAEA,SACA,QACA,QACA,QACA,KAEA,SACAL,EAAAO,GAAA,GACA,6BAAAP,CAAA,CAAAA,EAAAW,MAAA,KACAD,EAAAN,EAAAC,GAEA,8BAAAL,CAAA,CAAAA,EAAAW,MAAA,KACAF,EAAAL,EAAAC,GAEA,KAEA,SACAL,EAAAO,GAAA,GACA,8BAAAP,CAAA,CAAAA,EAAAW,MAAA,KACAF,EAAAL,EAAAC,GAEA,KAEA,SACAL,EAAAO,GAAA,GACA,6BAAAP,CAAA,CAAAA,EAAAW,MAAA,KACAD,EAAAN,EAAAC,GAEA,KAEA,SACAL,EAAAO,GAAA,EAGA,CACA,KAEA,uBACA,IAAAK,EAAArD,EAAAsD,SAAA,CAAAX,EAAAG,EAAA,EACA,SAAAhB,UAAA,CAAAuB,IAAA,OAAAvB,UAAA,CAAAuB,IAAA,OAAAvB,UAAA,CAAAuB,GAQAX,EAAAI,GAPAL,EAAAO,GAAA,GACAP,8BAAAA,CAAA,CAAAA,EAAAW,MAAA,IACAF,EAAAL,EAAAC,GACY,6BAAAL,CAAA,CAAAA,EAAAW,MAAA,KACZD,EAAAN,EAAAC,GAMA,CACA,CACA,CACA,IAAAT,EAAArC,EAAAuD,KAAA,GAAAb,EAAA,GACA,QAAAI,EAAAL,EAAAW,MAAA,GAAiCN,GAAA,EAAQA,IAEzC,OADAL,CAAA,CAAAK,EAAA,EAEA,oBACAT,GAAA,IACA,KAEA,yBACA,8BACA,gCACA,0BACA,iCACA,gCACAA,GAAA,IACA,KAEA,0BACA,+BACA,+BACAA,GAAA,IACA,KAEA,uBACA,IAAAgB,EAAArD,EAAAsD,SAAA,CAAAX,EAAA3C,EAAAoD,MAAA,EACA,OAAAtB,UAAA,CAAAuB,GACAhB,GAAA,OAAAkB,KAAA,CAAAF,EAAAD,MAAA,EACU,QAAAtB,UAAA,CAAAuB,GACVhB,GAAA,QAAAkB,KAAA,CAAAF,EAAAD,MAAA,EACU,OAAAtB,UAAA,CAAAuB,IACVhB,CAAAA,GAAA,OAAAkB,KAAA,CAAAF,EAAAD,MAAA,EAEA,CACA,CAEA,OAAAf,CACA,EAWkCF,EAAA,EAAyB,EAC3DI,OAAA,CACA,CAAajE,MAAA+D,EAAA/D,KAAA,CAAA8D,MAAA,kBAEb,CAAW9D,MAAA,OAAA8D,MAAA,eACX,CASA,SAAAoB,EAAAC,CAAA,EACA,OAAAC,EANAzE,IAAA,CAAA6C,UAAA,WAGA2B,iBAAAA,EAAAxE,IAAA,CA0gC0B,GAAA4B,EAAAC,EAAA,EAAkB,CAC5CC,OAAA,QACAC,KAAA,EACA,GAk5E0B,GAAAH,EAAAC,EAAA,EAAkB,CAAGC,OAAA,QAAAC,KAAA,KA2YrB,GAAAH,EAAAC,EAAA,EAAkB,CAAGC,OAAA,QAAAC,KAAA,KA0gBJzE,EAAAC,EAAY,CAuGvDmH,CAjqPA,CAAAC,EAAAC,KACA,QAAAC,KAAAD,EACA3H,EAAA0H,EAAAE,EAAA,CAAgCC,IAAAF,CAAA,CAAAC,EAAA,CAAAE,WAAA,IAChC,GA6pPA,GACA,CACAC,OAAA,IAAAA,EACAjH,KAAA,IAAAA,CACA,GAMA,IAAAA,EAAA,MACAiC,KAAA,OACAiF,eAAA,CAAoBjF,KAAA,QACpBkF,aAAA,OAAuBnH,KAAAC,CAAA,CAAa,GACpC,EAAamH,QAAAnH,CAAA,GAEboH,YAAA,OAAsBrH,KAAAC,CAAA,CAAa,GACnCA,CAEA,GACAgH,EAAA,EACAK,OAAAC,CAAA,CACC,IACD,IAAAD,EAAiB,GAAAzD,EAAA2D,EAAA,EAASD,GAC1B,OACAtF,KAAA,SACAiF,eAAA,CACAjF,KAAA,OACAqF,OAAAA,EAAAG,UAAA,EAEA,MAAAN,aAAA,CAAyBnH,KAAAC,CAAA,CAAa,EACtC,IAAAoF,EAAA,MAAAH,EAAAjF,GACA,OAAAoF,EAAAD,KAAA,EACA,mBACA,sBACA,MACA,sBACA,uBACA,OAEAgC,QAAA/B,EAAA/D,KAAA,CAEA,UACA,IAAAoG,EAAArC,EAAAD,KAAA,OACA,kCAAsDsC,EAAiB,EACvE,CACA,CACA,EACA,MAAAL,YAAA,CAAwBrH,KAAAC,CAAA,CAAa,CAAA0H,CAAA,EACrC,IAAAC,EAAA,MAAgC,GAAA/D,EAAAyB,EAAA,EAAc,CAAGtF,KAAAC,CAAA,GACjD,IAAA2H,EAAArC,OAAA,CACA,UAAA3F,EAAA,CACAE,QAAA,qDACAC,MAAA6H,EAAArH,KAAA,CACAP,KAAAC,EACAC,SAAAyH,EAAAzH,QAAA,CACAC,MAAAwH,EAAAxH,KAAA,CACAC,aAAAuH,EAAAvH,YAAA,GAGA,IAAAyH,EAAA,MAAqC,GAAAhE,EAAAiE,EAAA,EAAkB,CACvDxG,MAAAsG,EAAAtG,KAAA,CACAgG,OAAAA,CACA,GACA,IAAAO,EAAAtC,OAAA,CACA,UAAA3F,EAAA,CACAE,QAAA,sDACAC,MAAA8H,EAAAtH,KAAA,CACAP,KAAAC,EACAC,SAAAyH,EAAAzH,QAAA,CACAC,MAAAwH,EAAAxH,KAAA,CACAC,aAAAuH,EAAAvH,YAAA,GAGA,OAAAyH,EAAAvG,KAAA,CAEA,CACA,EAudAjC,OAAAC,GAAA,CADA,0CAGwCC,EAAAwI,EAAiB,CAkKzD,IAAAC,EAAyCtH,EAAAuH,EAAc,EACvD5H,KAAQK,EAAAE,EAAS,GACjBsH,QAAWxH,EAAAE,EAAS,EACpB,GACAuH,EAAuBzH,EAAAuH,EAAc,EACrCG,MAAS1H,EAAA2H,EAAW,CAAC3H,EAAAsB,EAAS,KAAGsG,KAAA,GACjC,GAEAC,EAAoB7H,EAAAsB,EAAS,EAC7BwG,OAAU9H,EAAAE,EAAS,GACnB6H,OAAU/H,EAAA2H,EAAW,CAAAF,EACrB,GACAO,EAA+BhI,EAAAuH,EAAc,EAC7CU,aAAgBjI,EAAA2H,EAAW,CAAC3H,EAAAsB,EAAS,KAAGsG,KAAA,IACxCM,QAAWlI,EAAA2H,EAAW,CAAC3H,EAAAsB,EAAS,KAAGsG,KAAA,IACnCO,QAAWnI,EAAA2H,EAAW,CAClB3H,EAAAuH,EAAc,EAClBa,YAAmBpI,EAAA2H,EAAW,CAAC3H,EAAAiB,EAAU,GACzC,IAEAoH,UAAarI,EAAA2H,EAAW,CACpB3H,EAAAuH,EAAc,EAClBe,UAAiBtI,EAAA2H,EAAW,CAAC3H,EAAAiB,EAAU,IACvCmH,YAAmBpI,EAAA2H,EAAW,CAAC3H,EAAAiB,EAAU,GACzC,IAEAsH,MAASvI,EAAA2H,EAAW,CAChB3H,EAAAuH,EAAc,EAClBa,YAAmBpI,EAAA2H,EAAW,CAAC3H,EAAAiB,EAAU,GACzC,GAEA,GACAuH,EAAAC,MAAA,EACAC,gBAAmB1I,EAAAE,EAAS,GAC5ByI,aAAAX,EACAY,WAAAtB,EACAuB,aAAgB7I,EAAA2H,EAAW,CAAC3H,EAAAE,EAAS,GACrC,GACA,IAAA4I,EAAAN,EAAAC,MAAA,EACAM,WAAc/I,EAAA2H,EAAW,CAAC3H,EAAAE,EAAS,GACnC,GACA8I,EAAiBhJ,EAAAsB,EAAS,EAC1B3B,KAAQK,EAAAE,EAAS,GACjB+I,YAAejJ,EAAA2H,EAAW,CAAC3H,EAAAE,EAAS,IACpC2G,YAAe7G,EAAAsB,EAAS,EACxBC,KAAUvB,EAAAwB,EAAU,WACpB0H,WAAgBlJ,EAAA2H,EAAW,CAAC3H,EAAAsB,EAAS,KAAGsG,KAAA,GACxC,GAAGA,KAAA,EACH,GAACA,KAAA,GACDkB,EAAAL,MAAA,EACAF,MAASvI,EAAAmB,EAAQ,CAAA6H,EACjB,GACA,IAAAG,EAAwBnJ,EAAAsB,EAAS,EACjCC,KAAQvB,EAAAwB,EAAU,SAClBlC,KAAQU,EAAAE,EAAS,EACjB,GAAC0H,KAAA,GACDwB,EAAyBpJ,EAAAsB,EAAS,EAClCC,KAAQvB,EAAAwB,EAAU,UAClBQ,KAAQhC,EAAAqJ,EAAS,GACjBC,SAAYtJ,EAAAE,EAAS,EACrB,GAAC0H,KAAA,GACD2B,EAA6BvJ,EAAAsB,EAAS,EAItCkI,IAAOxJ,EAAAE,EAAS,GAIhBoJ,SAAYtJ,EAAA2H,EAAW,CAAC3H,EAAAE,EAAS,GACjC,GAAC0H,KAAA,GACD6B,EAAAF,EAAAd,MAAA,EACAnJ,KAAQU,EAAAE,EAAS,EACjB,GACAwJ,EAAAH,EAAAd,MAAA,EACAkB,KAAQ3J,EAAAqJ,EAAS,EACjB,GACAO,EAA6B5J,EAAAsB,EAAS,EACtCC,KAAQvB,EAAAwB,EAAU,aAClBqI,SAAY7J,EAAAC,EAAQ,EAAAwJ,EAAAC,EAAA,CACpB,GAAC9B,KAAA,GACDY,EAAAC,MAAA,EACA1F,QAAW/C,EAAAmB,EAAQ,CACfnB,EAAAC,EAAQ,EAAAkJ,EAAAC,EAAAQ,EAAA,GAEZE,QAAW9J,EAAAiB,EAAU,GAAA8I,OAAA,KAAArI,QAAA,EACrB,GAACsI,EAAA,CACDxB,EAAAC,MAAA,EACAwB,WAAgBjK,EAAAuC,EAAU,EAC1B,IAKA,IAAA2H,EAA2BlK,EAAAsB,EAAS,EACpC6I,QAAWnK,EAAAwB,EAAU,CAFrB,OAGAkC,GAAM1D,EAAAC,EAAQ,EAAED,EAAAE,EAAS,GAAIF,EAAAgB,EAAS,GAAAoJ,GAAA,IACtC,GAACC,KAAA,CAAAxC,GAAAyC,MAAA,GACDC,EAA4BvK,EAAAsB,EAAS,EACrC6I,QAAWnK,EAAAwB,EAAU,CANrB,OAOAkC,GAAM1D,EAAAC,EAAQ,EAAED,EAAAE,EAAS,GAAIF,EAAAgB,EAAS,GAAAoJ,GAAA,KACtCzF,OA9FA8C,CA+FA,GAAC6C,MAAA,GACDE,EAAyBxK,EAAAsB,EAAS,EAClC6I,QAAWnK,EAAAwB,EAAU,CAXrB,OAYAkC,GAAM1D,EAAAC,EAAQ,EAAED,EAAAE,EAAS,GAAIF,EAAAgB,EAAS,GAAAoJ,GAAA,KACtCvK,MAASG,EAAAsB,EAAS,EAClBmJ,KAAUzK,EAAAgB,EAAS,GAAAoJ,GAAA,GACnBhL,QAAaY,EAAAE,EAAS,GACtB8B,KAAUhC,EAAA2H,EAAW,CAAC3H,EAAAuC,EAAU,GAChC,EACA,GAAC+H,MAAA,GACDI,EAAgC1K,EAAAsB,EAAS,EACzC6I,QAAWnK,EAAAwB,EAAU,CApBrB,MAqBA,GAAC6I,KAAA,CACCrK,EAAAsB,EAAS,EACXwG,OAAY9H,EAAAE,EAAS,GACrB6H,OAAY/H,EAAA2H,EAAW,CAAAF,EACvB,IACA6C,MAAA,GAC2BtK,EAAAC,EAAQ,EACnCiK,EACAQ,EACAH,EACAC,EACA,EAua+C3L,EAAAC,EAAY,CAmP3D,IAAA6L,EAAA,MACAxL,YAAA,CACAyL,IAAAA,EAAA,YACAC,YAAAA,CAAA,CACAC,QAAAA,CAAA,CACAC,KAAAA,CAAA,CACAC,MAAAC,CAAA,CACAC,2BAAAA,CAAA,CACAC,gCAAAA,CAAA,CACG,EACH,KAAAP,GAAA,CAAAA,EACA,KAAAC,WAAA,CAAAA,EACA,KAAAC,OAAA,CAAAA,EACA,KAAAC,IAAA,CAAAA,EACA,KAAAC,KAAA,CAAAC,EACA,KAAAC,0BAAA,CAAAA,EACA,KAAAC,+BAAA,CAAAA,CACA,CACA,MAAAC,aAAA,CACAC,YAAAA,CAAA,CACA,GAAAC,EACG,EACH,IAAA/K,EAAAC,EAAA+K,EAAAC,EAAAC,EACA,IAAAC,EAAA,MAA+B,GAAAvI,EAAAwI,EAAA,EAAO,KAAAZ,IAAA,EACtCa,EAAA,MAAkC,GAAAzI,EAAAwI,EAAA,EAAO,KAAAb,OAAA,EACzCe,EAAA,MAAsC,GAAA1I,EAAAwI,EAAA,EAAO,KAAAd,WAAA,EAC7CiB,EAAA,aAAAvL,CAAAA,EAAA,KAAA2K,0BAAA,SAAA3K,EAAAwL,IAAA,OACAnB,IAAA,KAAAA,GAAA,CACAlH,GAAA4H,EAAAU,MAAA,CACAC,SAAAX,EAAAW,QAAA,CACAlB,KAAA,CAAc,GAAAW,CAAA,IAAAJ,EAAAP,IAAA,EACdD,QAAA,CAAiB,GAAAc,CAAA,IAAAN,EAAAR,OAAA,EACjBD,YAAAgB,EACAK,gBAAAZ,EAAAa,QAAA,CACAC,QAAAd,EAAAc,OAAA,CACA9H,UAAAgH,EAAAhH,SAAA,EACK,EACLsG,EAAA,MAAApK,CAAAA,EAAAsL,MAAAA,EAAA,OAAAA,EAAAlB,GAAA,EAAApK,EAAA,KAAAoK,GAAA,CACAE,EAAA,CAAAgB,MAAAA,EAAA,OAAAA,EAAAhB,OAAA,WAAAgB,EAAAhB,OAAA,EAA0H,GAAAc,CAAA,IAAAN,EAAAR,OAAA,EAC1HC,EAAA,CAAAe,MAAAA,EAAA,OAAAA,EAAAf,IAAA,WAAAe,EAAAf,IAAA,EACA,GAAAW,CAAA,CACA,GAAAJ,EAAAP,IAAA,CACArH,GAAA4H,EAAAU,MAAA,CACAC,SAAAX,EAAAW,QAAA,CACAG,QAAAd,EAAAc,OAAA,CACA9H,UAAAgH,EAAAhH,SAAA,EAEAuG,EAAA,MAAAU,CAAAA,EAAAO,MAAAA,EAAA,OAAAA,EAAAjB,WAAA,EAAAU,EAAAM,EACAZ,EAAA,MAAAO,CAAAA,EAAA,KAAAR,KAAA,EAAAQ,EAAA/K,WAAAuK,KAAA,CACAxL,EAAA,MAAAyL,EAAAL,EAAA,CACA9C,OAAA,OACAgD,QAAA,CACA,kCACA,GAAAA,CAAA,EAEAC,KAAAsB,KAAAC,SAAA,CAAAvB,GACAF,YAAAA,EACA0B,OAAAlB,CACA,GACA,IAAA7L,EAAAgN,EAAA,CACA,YACA,MAAAf,CAAAA,EAAA,MAAAjM,EAAAF,IAAA,IAAAmM,EAAA,sCAGA,IAAAjM,EAAAuL,IAAA,CACA,2CAEA,YAAA0B,qBAAA,CAAAjN,EAAAuL,IAAA,CACA,CACA,MAAA2B,kBAAApB,CAAA,EACA,IAAA/K,EAAAC,EAAA+K,EAAAC,EAAAC,EACA,IAAAC,EAAA,MAA+B,GAAAvI,EAAAwI,EAAA,EAAO,KAAAZ,IAAA,EACtCa,EAAA,MAAkC,GAAAzI,EAAAwI,EAAA,EAAO,KAAAb,OAAA,EACzCe,EAAA,MAAsC,GAAA1I,EAAAwI,EAAA,EAAO,KAAAd,WAAA,EAC7CiB,EAAA,aAAAvL,CAAAA,EAAA,KAAA4K,+BAAA,SAAA5K,EAAAwL,IAAA,OACAnB,IAAA,KAAAA,GAAA,CACAlH,GAAA4H,EAAAU,MAAA,CACAjB,KAAA,CAAc,GAAAW,CAAA,IAAAJ,EAAAP,IAAA,EACdD,QAAA,CAAiB,GAAAc,CAAA,IAAAN,EAAAR,OAAA,EACjBD,YAAAgB,EACAK,gBAAAZ,EAAAa,QAAA,EACK,EACLvB,EAAA,MAAApK,CAAAA,EAAAsL,MAAAA,EAAA,OAAAA,EAAAlB,GAAA,EAAApK,EAAA,GAAgG,KAAAoK,GAAA,CAAS,GAAGU,EAAAU,MAAA,CAAe,SAC3HlB,EAAA,CAAAgB,MAAAA,EAAA,OAAAA,EAAAhB,OAAA,WAAAgB,EAAAhB,OAAA,EAA0H,GAAAc,CAAA,IAAAN,EAAAR,OAAA,EAC1HD,EAAA,MAAAU,CAAAA,EAAAO,MAAAA,EAAA,OAAAA,EAAAjB,WAAA,EAAAU,EAAAM,EACAZ,EAAA,MAAAO,CAAAA,EAAA,KAAAR,KAAA,EAAAQ,EAAA/K,WAAAuK,KAAA,CACAxL,EAAA,MAAAyL,EAAAL,EAAA,CACA9C,OAAA,MACAgD,QAAAA,EACAD,YAAAA,CACA,GACA,GAAArL,MAAAA,EAAAmN,MAAA,CACA,YAEA,IAAAnN,EAAAgN,EAAA,CACA,YACA,MAAAf,CAAAA,EAAA,MAAAjM,EAAAF,IAAA,IAAAmM,EAAA,sCAGA,IAAAjM,EAAAuL,IAAA,CACA,2CAEA,YAAA0B,qBAAA,CAAAjN,EAAAuL,IAAA,CACA,CACA,EAGA6B,EAAA,cAAAjC,EACAxL,YAAAmM,EAAA,EAA0B,EAC1B,MAAAA,EACA,CACAmB,sBAAAI,CAAA,EACA,MAAW,GAAA1J,EAAA2J,EAAA,EAAqB,CAChCD,OAAAA,EACAjG,OAAApD,CACA,GAAKuJ,WAAA,CACL,IAAAxJ,gBAAA,CACA,MAAAyJ,UAAAC,CAAA,CAAAC,CAAA,EACA,IAAAD,EAAApI,OAAA,CACA,MAAAoI,EAAApN,KAAA,CAEAqN,EAAAC,OAAA,CAAAF,EAAArM,KAAA,CACA,CACA,GAEA,CACA,EA6UA,SAAAwM,EAAA,CACAnB,SAAAA,CAAA,CACC,EACD,IAAA7M,EAAA6M,CAAA,CAAAA,EAAAvG,MAAA,IACA,IAAAtG,GAGAA,cAAAA,EAAA0D,IAAA,CAFA,SAKA,IAAAuK,EAAAjO,EAAAkO,KAAA,CAAAC,MAAA,EAAAC,EAAAzH,EAAA0H,IACA1H,eAAAA,EAAAxE,IAAA,CAAAkM,EAAAD,EACG,IACHE,EAAAtO,EAAAkO,KAAA,CAAAzH,KAAA,CAAAwH,EAAA,GAAAM,MAAA,CAAA7H,GACA,OAAA4H,EAAAhI,MAAA,IAAAgI,EAAAE,KAAA,IAAA7H,qBAAAA,EAAArB,KAAA,CACA,CA2CA,IAAAmJ,EAAuB7N,EAAAsB,EAAU,EACjCC,KAAQvB,EAAAwB,EAAW,SACnBlC,KAAQU,EAAAE,EAAU,GAClBwE,MAAS1E,EAAA8N,EAAQ,uBAAApM,QAAA,GACjBiC,iBAAAvC,EAAAM,QAAA,EACA,GACAqM,EAA4B/N,EAAAsB,EAAU,EACtCC,KAAQvB,EAAAwB,EAAW,cACnBlC,KAAQU,EAAAE,EAAU,GAClBwE,MAAS1E,EAAA8N,EAAQ,uBAAApM,QAAA,GACjBiC,iBAAAvC,EAAAM,QAAA,EACA,GACAsM,EAA4BhO,EAAAsB,EAAU,EACtCC,KAAQvB,EAAAwB,EAAW,eACnByC,SAAYjE,EAAAE,EAAU,GACtBgE,IAAOlE,EAAAE,EAAU,GACjBiE,MAASnE,EAAAE,EAAU,GAAAwB,QAAA,GACnBiC,iBAAAvC,EAAAM,QAAA,EACA,GACAuM,EAAiCjO,EAAAsB,EAAU,EAC3CC,KAAQvB,EAAAwB,EAAW,oBACnByC,SAAYjE,EAAAE,EAAU,GACtB4B,UAAa9B,EAAAE,EAAU,GACvBiE,MAASnE,EAAAE,EAAU,GACnB+B,SAAYjC,EAAAE,EAAU,GAAAwB,QAAA,GACtBiC,iBAAAvC,EAAAM,QAAA,EACA,GACAwM,GAAuBlO,EAAAsB,EAAU,EACjCC,KAAQvB,EAAAwB,EAAW,SACnBM,UAAa9B,EAAAE,EAAU,GACvB+B,SAAYjC,EAAAE,EAAU,GAAAwB,QAAA,GACtBwC,IAAOlE,EAAAE,EAAU,GACjByD,iBAAAvC,EAAAM,QAAA,EACA,GACAyM,GAA4BnO,EAAAsB,EAAU,EACtCC,KAAQvB,EAAAwB,EAAW,cACnB,GACA4M,GAAuBpO,EAAAsB,EAAU,EACjCC,KAAQvB,EAAAE,EAAU,GAAAkE,UAAA,UAClBV,GAAM1D,EAAAE,EAAU,GAAAwB,QAAA,GAChBM,KAAQhC,EAAAuC,EAAW,EACnB,GACA8L,GAAA,CACErO,EAAAsB,EAAU,EACZC,KAAUvB,EAAAwB,EAAW,iBACrBa,SAAcrC,EAAAE,EAAU,GACxBkC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,oBACtBc,MAAWtC,EAAAuC,EAAW,GAAAb,QAAA,GACtBkB,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,EACxB,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAwB,EAAW,iBACrBa,SAAcrC,EAAAE,EAAU,GACxBkC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,oBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,GACxB6M,qBAAAnN,EAAAM,QAAA,EACA,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAwB,EAAW,iBACrBa,SAAcrC,EAAAE,EAAU,GACxBkC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,qBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAuC,EAAW,GACvBsB,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,GACxB6M,qBAAAnN,EAAAM,QAAA,GACAsC,YAAiBhE,EAAAiB,EAAW,GAAAS,QAAA,EAC5B,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAwB,EAAW,iBACrBa,SAAcrC,EAAAE,EAAU,GACxBkC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,iBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAE,EAAU,GACzBqO,qBAAAnN,EAAAM,QAAA,EACA,GACA,CACA8M,GAAA,CACExO,EAAAsB,EAAU,EACZC,KAAUvB,EAAAE,EAAU,GAAAkE,UAAA,UACpBhC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,oBACtBc,MAAWtC,EAAAuC,EAAW,GAAAb,QAAA,GACtBkB,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,EACxB,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAE,EAAU,GAAAkE,UAAA,UACpBhC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,oBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,GACxB6M,qBAAAnN,EAAAM,QAAA,EACA,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAE,EAAU,GAAAkE,UAAA,UACpBhC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,qBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAuC,EAAW,GACvBsB,UAAe7D,EAAAsO,EAAS,GAAA5M,QAAA,GACxB6M,qBAAAnN,EAAAM,QAAA,GACAsC,YAAiBhE,EAAAiB,EAAW,GAAAS,QAAA,EAC5B,GACE1B,EAAAsB,EAAU,EACZC,KAAUvB,EAAAE,EAAU,GAAAkE,UAAA,UACpBhC,WAAgBpC,EAAAE,EAAU,GAC1BwE,MAAW1E,EAAAwB,EAAW,iBACtBc,MAAWtC,EAAAuC,EAAW,GACtBK,OAAY5C,EAAAsO,EAAS,GAAA5M,QAAA,GACrBmC,UAAe7D,EAAAE,EAAU,GACzBqO,qBAAAnN,EAAAM,QAAA,EACA,GACA,CACsB1B,EAAAsB,EAAU,EAChCoC,GAAM1D,EAAAE,EAAU,GAChB4C,KAAQ9C,EAAA8N,EAAQ,gCAChB3B,SAAYnM,EAAAuC,EAAW,GAAAb,QAAA,GACvB4L,MAAStN,EAAAmB,EAAS,CACdnB,EAAAC,EAAS,EACb4N,EACAE,EACAC,EACAC,EACAC,GACAC,GACAC,MACAC,MACAG,GACA,EAEA","sources":["webpack://_N_E/./node_modules/ai/dist/index.mjs"],"sourcesContent":["var __defProp = Object.defineProperty;\nvar __export = (target, all) => {\n  for (var name17 in all)\n    __defProp(target, name17, { get: all[name17], enumerable: true });\n};\n\n// src/index.ts\nimport { gateway as gateway2, createGateway } from \"@ai-sdk/gateway\";\nimport {\n  asSchema as asSchema5,\n  createIdGenerator as createIdGenerator5,\n  dynamicTool as dynamicTool2,\n  generateId as generateId2,\n  jsonSchema as jsonSchema2,\n  tool as tool2,\n  zodSchema\n} from \"@ai-sdk/provider-utils\";\n\n// src/generate-text/generate-text.ts\nimport {\n  createIdGenerator,\n  executeTool,\n  getErrorMessage as getErrorMessage5\n} from \"@ai-sdk/provider-utils\";\n\n// src/error/no-output-specified-error.ts\nimport { AISDKError } from \"@ai-sdk/provider\";\nvar name = \"AI_NoOutputSpecifiedError\";\nvar marker = `vercel.ai.error.${name}`;\nvar symbol = Symbol.for(marker);\nvar _a;\nvar NoOutputSpecifiedError = class extends AISDKError {\n  // used in isInstance\n  constructor({ message = \"No output specified.\" } = {}) {\n    super({ name, message });\n    this[_a] = true;\n  }\n  static isInstance(error) {\n    return AISDKError.hasMarker(error, marker);\n  }\n};\n_a = symbol;\n\n// src/logger/log-warnings.ts\nfunction formatWarning(warning) {\n  const prefix = \"AI SDK Warning:\";\n  switch (warning.type) {\n    case \"unsupported-setting\": {\n      let message = `${prefix} The \"${warning.setting}\" setting is not supported by this model`;\n      if (warning.details) {\n        message += ` - ${warning.details}`;\n      }\n      return message;\n    }\n    case \"unsupported-tool\": {\n      const toolName = \"name\" in warning.tool ? warning.tool.name : \"unknown tool\";\n      let message = `${prefix} The tool \"${toolName}\" is not supported by this model`;\n      if (warning.details) {\n        message += ` - ${warning.details}`;\n      }\n      return message;\n    }\n    case \"other\": {\n      return `${prefix} ${warning.message}`;\n    }\n    default: {\n      return `${prefix} ${JSON.stringify(warning, null, 2)}`;\n    }\n  }\n}\nvar FIRST_WARNING_INFO_MESSAGE = \"AI SDK Warning System: To turn off warning logging, set the AI_SDK_LOG_WARNINGS global to false.\";\nvar hasLoggedBefore = false;\nvar logWarnings = (warnings) => {\n  if (warnings.length === 0) {\n    return;\n  }\n  const logger = globalThis.AI_SDK_LOG_WARNINGS;\n  if (logger === false) {\n    return;\n  }\n  if (typeof logger === \"function\") {\n    logger(warnings);\n    return;\n  }\n  if (!hasLoggedBefore) {\n    hasLoggedBefore = true;\n    console.info(FIRST_WARNING_INFO_MESSAGE);\n  }\n  for (const warning of warnings) {\n    console.warn(formatWarning(warning));\n  }\n};\n\n// src/model/resolve-model.ts\nimport { gateway } from \"@ai-sdk/gateway\";\n\n// src/error/index.ts\nimport {\n  AISDKError as AISDKError17,\n  APICallError,\n  EmptyResponseBodyError,\n  InvalidPromptError,\n  InvalidResponseDataError,\n  JSONParseError,\n  LoadAPIKeyError,\n  NoContentGeneratedError,\n  NoSuchModelError,\n  TooManyEmbeddingValuesForCallError,\n  TypeValidationError,\n  UnsupportedFunctionalityError\n} from \"@ai-sdk/provider\";\n\n// src/error/invalid-argument-error.ts\nimport { AISDKError as AISDKError2 } from \"@ai-sdk/provider\";\nvar name2 = \"AI_InvalidArgumentError\";\nvar marker2 = `vercel.ai.error.${name2}`;\nvar symbol2 = Symbol.for(marker2);\nvar _a2;\nvar InvalidArgumentError = class extends AISDKError2 {\n  constructor({\n    parameter,\n    value,\n    message\n  }) {\n    super({\n      name: name2,\n      message: `Invalid argument for parameter ${parameter}: ${message}`\n    });\n    this[_a2] = true;\n    this.parameter = parameter;\n    this.value = value;\n  }\n  static isInstance(error) {\n    return AISDKError2.hasMarker(error, marker2);\n  }\n};\n_a2 = symbol2;\n\n// src/error/invalid-stream-part-error.ts\nimport { AISDKError as AISDKError3 } from \"@ai-sdk/provider\";\nvar name3 = \"AI_InvalidStreamPartError\";\nvar marker3 = `vercel.ai.error.${name3}`;\nvar symbol3 = Symbol.for(marker3);\nvar _a3;\nvar InvalidStreamPartError = class extends AISDKError3 {\n  constructor({\n    chunk,\n    message\n  }) {\n    super({ name: name3, message });\n    this[_a3] = true;\n    this.chunk = chunk;\n  }\n  static isInstance(error) {\n    return AISDKError3.hasMarker(error, marker3);\n  }\n};\n_a3 = symbol3;\n\n// src/error/invalid-tool-input-error.ts\nimport { AISDKError as AISDKError4, getErrorMessage } from \"@ai-sdk/provider\";\nvar name4 = \"AI_InvalidToolInputError\";\nvar marker4 = `vercel.ai.error.${name4}`;\nvar symbol4 = Symbol.for(marker4);\nvar _a4;\nvar InvalidToolInputError = class extends AISDKError4 {\n  constructor({\n    toolInput,\n    toolName,\n    cause,\n    message = `Invalid input for tool ${toolName}: ${getErrorMessage(cause)}`\n  }) {\n    super({ name: name4, message, cause });\n    this[_a4] = true;\n    this.toolInput = toolInput;\n    this.toolName = toolName;\n  }\n  static isInstance(error) {\n    return AISDKError4.hasMarker(error, marker4);\n  }\n};\n_a4 = symbol4;\n\n// src/error/mcp-client-error.ts\nimport { AISDKError as AISDKError5 } from \"@ai-sdk/provider\";\nvar name5 = \"AI_MCPClientError\";\nvar marker5 = `vercel.ai.error.${name5}`;\nvar symbol5 = Symbol.for(marker5);\nvar _a5;\nvar MCPClientError = class extends AISDKError5 {\n  constructor({\n    name: name17 = \"MCPClientError\",\n    message,\n    cause\n  }) {\n    super({ name: name17, message, cause });\n    this[_a5] = true;\n  }\n  static isInstance(error) {\n    return AISDKError5.hasMarker(error, marker5);\n  }\n};\n_a5 = symbol5;\n\n// src/error/no-image-generated-error.ts\nimport { AISDKError as AISDKError6 } from \"@ai-sdk/provider\";\nvar name6 = \"AI_NoImageGeneratedError\";\nvar marker6 = `vercel.ai.error.${name6}`;\nvar symbol6 = Symbol.for(marker6);\nvar _a6;\nvar NoImageGeneratedError = class extends AISDKError6 {\n  constructor({\n    message = \"No image generated.\",\n    cause,\n    responses\n  }) {\n    super({ name: name6, message, cause });\n    this[_a6] = true;\n    this.responses = responses;\n  }\n  static isInstance(error) {\n    return AISDKError6.hasMarker(error, marker6);\n  }\n};\n_a6 = symbol6;\n\n// src/error/no-object-generated-error.ts\nimport { AISDKError as AISDKError7 } from \"@ai-sdk/provider\";\nvar name7 = \"AI_NoObjectGeneratedError\";\nvar marker7 = `vercel.ai.error.${name7}`;\nvar symbol7 = Symbol.for(marker7);\nvar _a7;\nvar NoObjectGeneratedError = class extends AISDKError7 {\n  constructor({\n    message = \"No object generated.\",\n    cause,\n    text: text2,\n    response,\n    usage,\n    finishReason\n  }) {\n    super({ name: name7, message, cause });\n    this[_a7] = true;\n    this.text = text2;\n    this.response = response;\n    this.usage = usage;\n    this.finishReason = finishReason;\n  }\n  static isInstance(error) {\n    return AISDKError7.hasMarker(error, marker7);\n  }\n};\n_a7 = symbol7;\n\n// src/error/no-output-generated-error.ts\nimport { AISDKError as AISDKError8 } from \"@ai-sdk/provider\";\nvar name8 = \"AI_NoOutputGeneratedError\";\nvar marker8 = `vercel.ai.error.${name8}`;\nvar symbol8 = Symbol.for(marker8);\nvar _a8;\nvar NoOutputGeneratedError = class extends AISDKError8 {\n  // used in isInstance\n  constructor({\n    message = \"No output generated.\",\n    cause\n  } = {}) {\n    super({ name: name8, message, cause });\n    this[_a8] = true;\n  }\n  static isInstance(error) {\n    return AISDKError8.hasMarker(error, marker8);\n  }\n};\n_a8 = symbol8;\n\n// src/error/no-such-tool-error.ts\nimport { AISDKError as AISDKError9 } from \"@ai-sdk/provider\";\nvar name9 = \"AI_NoSuchToolError\";\nvar marker9 = `vercel.ai.error.${name9}`;\nvar symbol9 = Symbol.for(marker9);\nvar _a9;\nvar NoSuchToolError = class extends AISDKError9 {\n  constructor({\n    toolName,\n    availableTools = void 0,\n    message = `Model tried to call unavailable tool '${toolName}'. ${availableTools === void 0 ? \"No tools are available.\" : `Available tools: ${availableTools.join(\", \")}.`}`\n  }) {\n    super({ name: name9, message });\n    this[_a9] = true;\n    this.toolName = toolName;\n    this.availableTools = availableTools;\n  }\n  static isInstance(error) {\n    return AISDKError9.hasMarker(error, marker9);\n  }\n};\n_a9 = symbol9;\n\n// src/error/tool-call-repair-error.ts\nimport { AISDKError as AISDKError10, getErrorMessage as getErrorMessage2 } from \"@ai-sdk/provider\";\nvar name10 = \"AI_ToolCallRepairError\";\nvar marker10 = `vercel.ai.error.${name10}`;\nvar symbol10 = Symbol.for(marker10);\nvar _a10;\nvar ToolCallRepairError = class extends AISDKError10 {\n  constructor({\n    cause,\n    originalError,\n    message = `Error repairing tool call: ${getErrorMessage2(cause)}`\n  }) {\n    super({ name: name10, message, cause });\n    this[_a10] = true;\n    this.originalError = originalError;\n  }\n  static isInstance(error) {\n    return AISDKError10.hasMarker(error, marker10);\n  }\n};\n_a10 = symbol10;\n\n// src/error/unsupported-model-version-error.ts\nimport { AISDKError as AISDKError11 } from \"@ai-sdk/provider\";\nvar UnsupportedModelVersionError = class extends AISDKError11 {\n  constructor(options) {\n    super({\n      name: \"AI_UnsupportedModelVersionError\",\n      message: `Unsupported model version ${options.version} for provider \"${options.provider}\" and model \"${options.modelId}\". AI SDK 5 only supports models that implement specification version \"v2\".`\n    });\n    this.version = options.version;\n    this.provider = options.provider;\n    this.modelId = options.modelId;\n  }\n};\n\n// src/prompt/invalid-data-content-error.ts\nimport { AISDKError as AISDKError12 } from \"@ai-sdk/provider\";\nvar name11 = \"AI_InvalidDataContentError\";\nvar marker11 = `vercel.ai.error.${name11}`;\nvar symbol11 = Symbol.for(marker11);\nvar _a11;\nvar InvalidDataContentError = class extends AISDKError12 {\n  constructor({\n    content,\n    cause,\n    message = `Invalid data content. Expected a base64 string, Uint8Array, ArrayBuffer, or Buffer, but got ${typeof content}.`\n  }) {\n    super({ name: name11, message, cause });\n    this[_a11] = true;\n    this.content = content;\n  }\n  static isInstance(error) {\n    return AISDKError12.hasMarker(error, marker11);\n  }\n};\n_a11 = symbol11;\n\n// src/prompt/invalid-message-role-error.ts\nimport { AISDKError as AISDKError13 } from \"@ai-sdk/provider\";\nvar name12 = \"AI_InvalidMessageRoleError\";\nvar marker12 = `vercel.ai.error.${name12}`;\nvar symbol12 = Symbol.for(marker12);\nvar _a12;\nvar InvalidMessageRoleError = class extends AISDKError13 {\n  constructor({\n    role,\n    message = `Invalid message role: '${role}'. Must be one of: \"system\", \"user\", \"assistant\", \"tool\".`\n  }) {\n    super({ name: name12, message });\n    this[_a12] = true;\n    this.role = role;\n  }\n  static isInstance(error) {\n    return AISDKError13.hasMarker(error, marker12);\n  }\n};\n_a12 = symbol12;\n\n// src/prompt/message-conversion-error.ts\nimport { AISDKError as AISDKError14 } from \"@ai-sdk/provider\";\nvar name13 = \"AI_MessageConversionError\";\nvar marker13 = `vercel.ai.error.${name13}`;\nvar symbol13 = Symbol.for(marker13);\nvar _a13;\nvar MessageConversionError = class extends AISDKError14 {\n  constructor({\n    originalMessage,\n    message\n  }) {\n    super({ name: name13, message });\n    this[_a13] = true;\n    this.originalMessage = originalMessage;\n  }\n  static isInstance(error) {\n    return AISDKError14.hasMarker(error, marker13);\n  }\n};\n_a13 = symbol13;\n\n// src/util/download/download-error.ts\nimport { AISDKError as AISDKError15 } from \"@ai-sdk/provider\";\nvar name14 = \"AI_DownloadError\";\nvar marker14 = `vercel.ai.error.${name14}`;\nvar symbol14 = Symbol.for(marker14);\nvar _a14;\nvar DownloadError = class extends AISDKError15 {\n  constructor({\n    url,\n    statusCode,\n    statusText,\n    cause,\n    message = cause == null ? `Failed to download ${url}: ${statusCode} ${statusText}` : `Failed to download ${url}: ${cause}`\n  }) {\n    super({ name: name14, message, cause });\n    this[_a14] = true;\n    this.url = url;\n    this.statusCode = statusCode;\n    this.statusText = statusText;\n  }\n  static isInstance(error) {\n    return AISDKError15.hasMarker(error, marker14);\n  }\n};\n_a14 = symbol14;\n\n// src/util/retry-error.ts\nimport { AISDKError as AISDKError16 } from \"@ai-sdk/provider\";\nvar name15 = \"AI_RetryError\";\nvar marker15 = `vercel.ai.error.${name15}`;\nvar symbol15 = Symbol.for(marker15);\nvar _a15;\nvar RetryError = class extends AISDKError16 {\n  constructor({\n    message,\n    reason,\n    errors\n  }) {\n    super({ name: name15, message });\n    this[_a15] = true;\n    this.reason = reason;\n    this.errors = errors;\n    this.lastError = errors[errors.length - 1];\n  }\n  static isInstance(error) {\n    return AISDKError16.hasMarker(error, marker15);\n  }\n};\n_a15 = symbol15;\n\n// src/model/resolve-model.ts\nfunction resolveLanguageModel(model) {\n  if (typeof model !== \"string\") {\n    if (model.specificationVersion !== \"v2\") {\n      throw new UnsupportedModelVersionError({\n        version: model.specificationVersion,\n        provider: model.provider,\n        modelId: model.modelId\n      });\n    }\n    return model;\n  }\n  return getGlobalProvider().languageModel(model);\n}\nfunction resolveEmbeddingModel(model) {\n  if (typeof model !== \"string\") {\n    if (model.specificationVersion !== \"v2\") {\n      throw new UnsupportedModelVersionError({\n        version: model.specificationVersion,\n        provider: model.provider,\n        modelId: model.modelId\n      });\n    }\n    return model;\n  }\n  return getGlobalProvider().textEmbeddingModel(\n    model\n  );\n}\nfunction getGlobalProvider() {\n  var _a17;\n  return (_a17 = globalThis.AI_SDK_DEFAULT_PROVIDER) != null ? _a17 : gateway;\n}\n\n// src/prompt/convert-to-language-model-prompt.ts\nimport {\n  isUrlSupported\n} from \"@ai-sdk/provider-utils\";\n\n// src/util/detect-media-type.ts\nimport { convertBase64ToUint8Array } from \"@ai-sdk/provider-utils\";\nvar imageMediaTypeSignatures = [\n  {\n    mediaType: \"image/gif\",\n    bytesPrefix: [71, 73, 70],\n    base64Prefix: \"R0lG\"\n  },\n  {\n    mediaType: \"image/png\",\n    bytesPrefix: [137, 80, 78, 71],\n    base64Prefix: \"iVBORw\"\n  },\n  {\n    mediaType: \"image/jpeg\",\n    bytesPrefix: [255, 216],\n    base64Prefix: \"/9j/\"\n  },\n  {\n    mediaType: \"image/webp\",\n    bytesPrefix: [82, 73, 70, 70],\n    base64Prefix: \"UklGRg\"\n  },\n  {\n    mediaType: \"image/bmp\",\n    bytesPrefix: [66, 77],\n    base64Prefix: \"Qk\"\n  },\n  {\n    mediaType: \"image/tiff\",\n    bytesPrefix: [73, 73, 42, 0],\n    base64Prefix: \"SUkqAA\"\n  },\n  {\n    mediaType: \"image/tiff\",\n    bytesPrefix: [77, 77, 0, 42],\n    base64Prefix: \"TU0AKg\"\n  },\n  {\n    mediaType: \"image/avif\",\n    bytesPrefix: [\n      0,\n      0,\n      0,\n      32,\n      102,\n      116,\n      121,\n      112,\n      97,\n      118,\n      105,\n      102\n    ],\n    base64Prefix: \"AAAAIGZ0eXBhdmlm\"\n  },\n  {\n    mediaType: \"image/heic\",\n    bytesPrefix: [\n      0,\n      0,\n      0,\n      32,\n      102,\n      116,\n      121,\n      112,\n      104,\n      101,\n      105,\n      99\n    ],\n    base64Prefix: \"AAAAIGZ0eXBoZWlj\"\n  }\n];\nvar audioMediaTypeSignatures = [\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 251],\n    base64Prefix: \"//s=\"\n  },\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 250],\n    base64Prefix: \"//o=\"\n  },\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 243],\n    base64Prefix: \"//M=\"\n  },\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 242],\n    base64Prefix: \"//I=\"\n  },\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 227],\n    base64Prefix: \"/+M=\"\n  },\n  {\n    mediaType: \"audio/mpeg\",\n    bytesPrefix: [255, 226],\n    base64Prefix: \"/+I=\"\n  },\n  {\n    mediaType: \"audio/wav\",\n    bytesPrefix: [82, 73, 70, 70],\n    base64Prefix: \"UklGR\"\n  },\n  {\n    mediaType: \"audio/ogg\",\n    bytesPrefix: [79, 103, 103, 83],\n    base64Prefix: \"T2dnUw\"\n  },\n  {\n    mediaType: \"audio/flac\",\n    bytesPrefix: [102, 76, 97, 67],\n    base64Prefix: \"ZkxhQw\"\n  },\n  {\n    mediaType: \"audio/aac\",\n    bytesPrefix: [64, 21, 0, 0],\n    base64Prefix: \"QBUA\"\n  },\n  {\n    mediaType: \"audio/mp4\",\n    bytesPrefix: [102, 116, 121, 112],\n    base64Prefix: \"ZnR5cA\"\n  },\n  {\n    mediaType: \"audio/webm\",\n    bytesPrefix: [26, 69, 223, 163],\n    base64Prefix: \"GkXf\"\n  }\n];\nvar stripID3 = (data) => {\n  const bytes = typeof data === \"string\" ? convertBase64ToUint8Array(data) : data;\n  const id3Size = (bytes[6] & 127) << 21 | (bytes[7] & 127) << 14 | (bytes[8] & 127) << 7 | bytes[9] & 127;\n  return bytes.slice(id3Size + 10);\n};\nfunction stripID3TagsIfPresent(data) {\n  const hasId3 = typeof data === \"string\" && data.startsWith(\"SUQz\") || typeof data !== \"string\" && data.length > 10 && data[0] === 73 && // 'I'\n  data[1] === 68 && // 'D'\n  data[2] === 51;\n  return hasId3 ? stripID3(data) : data;\n}\nfunction detectMediaType({\n  data,\n  signatures\n}) {\n  const processedData = stripID3TagsIfPresent(data);\n  for (const signature of signatures) {\n    if (typeof processedData === \"string\" ? processedData.startsWith(signature.base64Prefix) : processedData.length >= signature.bytesPrefix.length && signature.bytesPrefix.every(\n      (byte, index) => processedData[index] === byte\n    )) {\n      return signature.mediaType;\n    }\n  }\n  return void 0;\n}\n\n// src/util/download/download.ts\nvar download = async ({ url }) => {\n  var _a17;\n  const urlText = url.toString();\n  try {\n    const response = await fetch(urlText);\n    if (!response.ok) {\n      throw new DownloadError({\n        url: urlText,\n        statusCode: response.status,\n        statusText: response.statusText\n      });\n    }\n    return {\n      data: new Uint8Array(await response.arrayBuffer()),\n      mediaType: (_a17 = response.headers.get(\"content-type\")) != null ? _a17 : void 0\n    };\n  } catch (error) {\n    if (DownloadError.isInstance(error)) {\n      throw error;\n    }\n    throw new DownloadError({ url: urlText, cause: error });\n  }\n};\n\n// src/util/download/download-function.ts\nvar createDefaultDownloadFunction = (download2 = download) => (requestedDownloads) => Promise.all(\n  requestedDownloads.map(\n    async (requestedDownload) => requestedDownload.isUrlSupportedByModel ? null : download2(requestedDownload)\n  )\n);\n\n// src/prompt/data-content.ts\nimport { AISDKError as AISDKError18 } from \"@ai-sdk/provider\";\nimport {\n  convertBase64ToUint8Array as convertBase64ToUint8Array2,\n  convertUint8ArrayToBase64\n} from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\n\n// src/prompt/split-data-url.ts\nfunction splitDataUrl(dataUrl) {\n  try {\n    const [header, base64Content] = dataUrl.split(\",\");\n    return {\n      mediaType: header.split(\";\")[0].split(\":\")[1],\n      base64Content\n    };\n  } catch (error) {\n    return {\n      mediaType: void 0,\n      base64Content: void 0\n    };\n  }\n}\n\n// src/prompt/data-content.ts\nvar dataContentSchema = z.union([\n  z.string(),\n  z.instanceof(Uint8Array),\n  z.instanceof(ArrayBuffer),\n  z.custom(\n    // Buffer might not be available in some environments such as CloudFlare:\n    (value) => {\n      var _a17, _b;\n      return (_b = (_a17 = globalThis.Buffer) == null ? void 0 : _a17.isBuffer(value)) != null ? _b : false;\n    },\n    { message: \"Must be a Buffer\" }\n  )\n]);\nfunction convertToLanguageModelV2DataContent(content) {\n  if (content instanceof Uint8Array) {\n    return { data: content, mediaType: void 0 };\n  }\n  if (content instanceof ArrayBuffer) {\n    return { data: new Uint8Array(content), mediaType: void 0 };\n  }\n  if (typeof content === \"string\") {\n    try {\n      content = new URL(content);\n    } catch (error) {\n    }\n  }\n  if (content instanceof URL && content.protocol === \"data:\") {\n    const { mediaType: dataUrlMediaType, base64Content } = splitDataUrl(\n      content.toString()\n    );\n    if (dataUrlMediaType == null || base64Content == null) {\n      throw new AISDKError18({\n        name: \"InvalidDataContentError\",\n        message: `Invalid data URL format in content ${content.toString()}`\n      });\n    }\n    return { data: base64Content, mediaType: dataUrlMediaType };\n  }\n  return { data: content, mediaType: void 0 };\n}\nfunction convertDataContentToBase64String(content) {\n  if (typeof content === \"string\") {\n    return content;\n  }\n  if (content instanceof ArrayBuffer) {\n    return convertUint8ArrayToBase64(new Uint8Array(content));\n  }\n  return convertUint8ArrayToBase64(content);\n}\nfunction convertDataContentToUint8Array(content) {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n  if (typeof content === \"string\") {\n    try {\n      return convertBase64ToUint8Array2(content);\n    } catch (error) {\n      throw new InvalidDataContentError({\n        message: \"Invalid data content. Content string is not a base64-encoded media.\",\n        content,\n        cause: error\n      });\n    }\n  }\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n  throw new InvalidDataContentError({ content });\n}\n\n// src/prompt/convert-to-language-model-prompt.ts\nasync function convertToLanguageModelPrompt({\n  prompt,\n  supportedUrls,\n  download: download2 = createDefaultDownloadFunction()\n}) {\n  const downloadedAssets = await downloadAssets(\n    prompt.messages,\n    download2,\n    supportedUrls\n  );\n  return [\n    ...prompt.system != null ? [{ role: \"system\", content: prompt.system }] : [],\n    ...prompt.messages.map(\n      (message) => convertToLanguageModelMessage({ message, downloadedAssets })\n    )\n  ];\n}\nfunction convertToLanguageModelMessage({\n  message,\n  downloadedAssets\n}) {\n  const role = message.role;\n  switch (role) {\n    case \"system\": {\n      return {\n        role: \"system\",\n        content: message.content,\n        providerOptions: message.providerOptions\n      };\n    }\n    case \"user\": {\n      if (typeof message.content === \"string\") {\n        return {\n          role: \"user\",\n          content: [{ type: \"text\", text: message.content }],\n          providerOptions: message.providerOptions\n        };\n      }\n      return {\n        role: \"user\",\n        content: message.content.map((part) => convertPartToLanguageModelPart(part, downloadedAssets)).filter((part) => part.type !== \"text\" || part.text !== \"\"),\n        providerOptions: message.providerOptions\n      };\n    }\n    case \"assistant\": {\n      if (typeof message.content === \"string\") {\n        return {\n          role: \"assistant\",\n          content: [{ type: \"text\", text: message.content }],\n          providerOptions: message.providerOptions\n        };\n      }\n      return {\n        role: \"assistant\",\n        content: message.content.filter(\n          // remove empty text parts:\n          (part) => part.type !== \"text\" || part.text !== \"\"\n        ).map((part) => {\n          const providerOptions = part.providerOptions;\n          switch (part.type) {\n            case \"file\": {\n              const { data, mediaType } = convertToLanguageModelV2DataContent(\n                part.data\n              );\n              return {\n                type: \"file\",\n                data,\n                filename: part.filename,\n                mediaType: mediaType != null ? mediaType : part.mediaType,\n                providerOptions\n              };\n            }\n            case \"reasoning\": {\n              return {\n                type: \"reasoning\",\n                text: part.text,\n                providerOptions\n              };\n            }\n            case \"text\": {\n              return {\n                type: \"text\",\n                text: part.text,\n                providerOptions\n              };\n            }\n            case \"tool-call\": {\n              return {\n                type: \"tool-call\",\n                toolCallId: part.toolCallId,\n                toolName: part.toolName,\n                input: part.input,\n                providerExecuted: part.providerExecuted,\n                providerOptions\n              };\n            }\n            case \"tool-result\": {\n              return {\n                type: \"tool-result\",\n                toolCallId: part.toolCallId,\n                toolName: part.toolName,\n                output: part.output,\n                providerOptions\n              };\n            }\n          }\n        }),\n        providerOptions: message.providerOptions\n      };\n    }\n    case \"tool\": {\n      return {\n        role: \"tool\",\n        content: message.content.map((part) => ({\n          type: \"tool-result\",\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          output: part.output,\n          providerOptions: part.providerOptions\n        })),\n        providerOptions: message.providerOptions\n      };\n    }\n    default: {\n      const _exhaustiveCheck = role;\n      throw new InvalidMessageRoleError({ role: _exhaustiveCheck });\n    }\n  }\n}\nasync function downloadAssets(messages, download2, supportedUrls) {\n  const plannedDownloads = messages.filter((message) => message.role === \"user\").map((message) => message.content).filter(\n    (content) => Array.isArray(content)\n  ).flat().filter(\n    (part) => part.type === \"image\" || part.type === \"file\"\n  ).map((part) => {\n    var _a17;\n    const mediaType = (_a17 = part.mediaType) != null ? _a17 : part.type === \"image\" ? \"image/*\" : void 0;\n    let data = part.type === \"image\" ? part.image : part.data;\n    if (typeof data === \"string\") {\n      try {\n        data = new URL(data);\n      } catch (ignored) {\n      }\n    }\n    return { mediaType, data };\n  }).filter(\n    (part) => part.data instanceof URL\n  ).map((part) => ({\n    url: part.data,\n    isUrlSupportedByModel: part.mediaType != null && isUrlSupported({\n      url: part.data.toString(),\n      mediaType: part.mediaType,\n      supportedUrls\n    })\n  }));\n  const downloadedFiles = await download2(plannedDownloads);\n  return Object.fromEntries(\n    downloadedFiles.filter(\n      (downloadedFile) => (downloadedFile == null ? void 0 : downloadedFile.data) != null\n    ).map(({ data, mediaType }, index) => [\n      plannedDownloads[index].url.toString(),\n      { data, mediaType }\n    ])\n  );\n}\nfunction convertPartToLanguageModelPart(part, downloadedAssets) {\n  var _a17;\n  if (part.type === \"text\") {\n    return {\n      type: \"text\",\n      text: part.text,\n      providerOptions: part.providerOptions\n    };\n  }\n  let originalData;\n  const type = part.type;\n  switch (type) {\n    case \"image\":\n      originalData = part.image;\n      break;\n    case \"file\":\n      originalData = part.data;\n      break;\n    default:\n      throw new Error(`Unsupported part type: ${type}`);\n  }\n  const { data: convertedData, mediaType: convertedMediaType } = convertToLanguageModelV2DataContent(originalData);\n  let mediaType = convertedMediaType != null ? convertedMediaType : part.mediaType;\n  let data = convertedData;\n  if (data instanceof URL) {\n    const downloadedFile = downloadedAssets[data.toString()];\n    if (downloadedFile) {\n      data = downloadedFile.data;\n      mediaType != null ? mediaType : mediaType = downloadedFile.mediaType;\n    }\n  }\n  switch (type) {\n    case \"image\": {\n      if (data instanceof Uint8Array || typeof data === \"string\") {\n        mediaType = (_a17 = detectMediaType({ data, signatures: imageMediaTypeSignatures })) != null ? _a17 : mediaType;\n      }\n      return {\n        type: \"file\",\n        mediaType: mediaType != null ? mediaType : \"image/*\",\n        // any image\n        filename: void 0,\n        data,\n        providerOptions: part.providerOptions\n      };\n    }\n    case \"file\": {\n      if (mediaType == null) {\n        throw new Error(`Media type is missing for file part`);\n      }\n      return {\n        type: \"file\",\n        mediaType,\n        filename: part.filename,\n        data,\n        providerOptions: part.providerOptions\n      };\n    }\n  }\n}\n\n// src/prompt/prepare-call-settings.ts\nfunction prepareCallSettings({\n  maxOutputTokens,\n  temperature,\n  topP,\n  topK,\n  presencePenalty,\n  frequencyPenalty,\n  seed,\n  stopSequences\n}) {\n  if (maxOutputTokens != null) {\n    if (!Number.isInteger(maxOutputTokens)) {\n      throw new InvalidArgumentError({\n        parameter: \"maxOutputTokens\",\n        value: maxOutputTokens,\n        message: \"maxOutputTokens must be an integer\"\n      });\n    }\n    if (maxOutputTokens < 1) {\n      throw new InvalidArgumentError({\n        parameter: \"maxOutputTokens\",\n        value: maxOutputTokens,\n        message: \"maxOutputTokens must be >= 1\"\n      });\n    }\n  }\n  if (temperature != null) {\n    if (typeof temperature !== \"number\") {\n      throw new InvalidArgumentError({\n        parameter: \"temperature\",\n        value: temperature,\n        message: \"temperature must be a number\"\n      });\n    }\n  }\n  if (topP != null) {\n    if (typeof topP !== \"number\") {\n      throw new InvalidArgumentError({\n        parameter: \"topP\",\n        value: topP,\n        message: \"topP must be a number\"\n      });\n    }\n  }\n  if (topK != null) {\n    if (typeof topK !== \"number\") {\n      throw new InvalidArgumentError({\n        parameter: \"topK\",\n        value: topK,\n        message: \"topK must be a number\"\n      });\n    }\n  }\n  if (presencePenalty != null) {\n    if (typeof presencePenalty !== \"number\") {\n      throw new InvalidArgumentError({\n        parameter: \"presencePenalty\",\n        value: presencePenalty,\n        message: \"presencePenalty must be a number\"\n      });\n    }\n  }\n  if (frequencyPenalty != null) {\n    if (typeof frequencyPenalty !== \"number\") {\n      throw new InvalidArgumentError({\n        parameter: \"frequencyPenalty\",\n        value: frequencyPenalty,\n        message: \"frequencyPenalty must be a number\"\n      });\n    }\n  }\n  if (seed != null) {\n    if (!Number.isInteger(seed)) {\n      throw new InvalidArgumentError({\n        parameter: \"seed\",\n        value: seed,\n        message: \"seed must be an integer\"\n      });\n    }\n  }\n  return {\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    stopSequences,\n    seed\n  };\n}\n\n// src/prompt/prepare-tools-and-tool-choice.ts\nimport { asSchema } from \"@ai-sdk/provider-utils\";\n\n// src/util/is-non-empty-object.ts\nfunction isNonEmptyObject(object2) {\n  return object2 != null && Object.keys(object2).length > 0;\n}\n\n// src/prompt/prepare-tools-and-tool-choice.ts\nfunction prepareToolsAndToolChoice({\n  tools,\n  toolChoice,\n  activeTools\n}) {\n  if (!isNonEmptyObject(tools)) {\n    return {\n      tools: void 0,\n      toolChoice: void 0\n    };\n  }\n  const filteredTools = activeTools != null ? Object.entries(tools).filter(\n    ([name17]) => activeTools.includes(name17)\n  ) : Object.entries(tools);\n  return {\n    tools: filteredTools.map(([name17, tool3]) => {\n      const toolType = tool3.type;\n      switch (toolType) {\n        case void 0:\n        case \"dynamic\":\n        case \"function\":\n          return {\n            type: \"function\",\n            name: name17,\n            description: tool3.description,\n            inputSchema: asSchema(tool3.inputSchema).jsonSchema,\n            providerOptions: tool3.providerOptions\n          };\n        case \"provider-defined\":\n          return {\n            type: \"provider-defined\",\n            name: name17,\n            id: tool3.id,\n            args: tool3.args\n          };\n        default: {\n          const exhaustiveCheck = toolType;\n          throw new Error(`Unsupported tool type: ${exhaustiveCheck}`);\n        }\n      }\n    }),\n    toolChoice: toolChoice == null ? { type: \"auto\" } : typeof toolChoice === \"string\" ? { type: toolChoice } : { type: \"tool\", toolName: toolChoice.toolName }\n  };\n}\n\n// src/prompt/standardize-prompt.ts\nimport { InvalidPromptError as InvalidPromptError2 } from \"@ai-sdk/provider\";\nimport { safeValidateTypes } from \"@ai-sdk/provider-utils\";\nimport { z as z6 } from \"zod/v4\";\n\n// src/prompt/message.ts\nimport { z as z5 } from \"zod/v4\";\n\n// src/types/provider-metadata.ts\nimport { z as z3 } from \"zod/v4\";\n\n// src/types/json-value.ts\nimport { z as z2 } from \"zod/v4\";\nvar jsonValueSchema = z2.lazy(\n  () => z2.union([\n    z2.null(),\n    z2.string(),\n    z2.number(),\n    z2.boolean(),\n    z2.record(z2.string(), jsonValueSchema),\n    z2.array(jsonValueSchema)\n  ])\n);\n\n// src/types/provider-metadata.ts\nvar providerMetadataSchema = z3.record(\n  z3.string(),\n  z3.record(z3.string(), jsonValueSchema)\n);\n\n// src/prompt/content-part.ts\nimport { z as z4 } from \"zod/v4\";\nvar textPartSchema = z4.object({\n  type: z4.literal(\"text\"),\n  text: z4.string(),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar imagePartSchema = z4.object({\n  type: z4.literal(\"image\"),\n  image: z4.union([dataContentSchema, z4.instanceof(URL)]),\n  mediaType: z4.string().optional(),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar filePartSchema = z4.object({\n  type: z4.literal(\"file\"),\n  data: z4.union([dataContentSchema, z4.instanceof(URL)]),\n  filename: z4.string().optional(),\n  mediaType: z4.string(),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar reasoningPartSchema = z4.object({\n  type: z4.literal(\"reasoning\"),\n  text: z4.string(),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar toolCallPartSchema = z4.object({\n  type: z4.literal(\"tool-call\"),\n  toolCallId: z4.string(),\n  toolName: z4.string(),\n  input: z4.unknown(),\n  providerOptions: providerMetadataSchema.optional(),\n  providerExecuted: z4.boolean().optional()\n});\nvar outputSchema = z4.discriminatedUnion(\"type\", [\n  z4.object({\n    type: z4.literal(\"text\"),\n    value: z4.string()\n  }),\n  z4.object({\n    type: z4.literal(\"json\"),\n    value: jsonValueSchema\n  }),\n  z4.object({\n    type: z4.literal(\"error-text\"),\n    value: z4.string()\n  }),\n  z4.object({\n    type: z4.literal(\"error-json\"),\n    value: jsonValueSchema\n  }),\n  z4.object({\n    type: z4.literal(\"content\"),\n    value: z4.array(\n      z4.union([\n        z4.object({\n          type: z4.literal(\"text\"),\n          text: z4.string()\n        }),\n        z4.object({\n          type: z4.literal(\"media\"),\n          data: z4.string(),\n          mediaType: z4.string()\n        })\n      ])\n    )\n  })\n]);\nvar toolResultPartSchema = z4.object({\n  type: z4.literal(\"tool-result\"),\n  toolCallId: z4.string(),\n  toolName: z4.string(),\n  output: outputSchema,\n  providerOptions: providerMetadataSchema.optional()\n});\n\n// src/prompt/message.ts\nvar systemModelMessageSchema = z5.object(\n  {\n    role: z5.literal(\"system\"),\n    content: z5.string(),\n    providerOptions: providerMetadataSchema.optional()\n  }\n);\nvar coreSystemMessageSchema = systemModelMessageSchema;\nvar userModelMessageSchema = z5.object({\n  role: z5.literal(\"user\"),\n  content: z5.union([\n    z5.string(),\n    z5.array(z5.union([textPartSchema, imagePartSchema, filePartSchema]))\n  ]),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar coreUserMessageSchema = userModelMessageSchema;\nvar assistantModelMessageSchema = z5.object({\n  role: z5.literal(\"assistant\"),\n  content: z5.union([\n    z5.string(),\n    z5.array(\n      z5.union([\n        textPartSchema,\n        filePartSchema,\n        reasoningPartSchema,\n        toolCallPartSchema,\n        toolResultPartSchema\n      ])\n    )\n  ]),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar coreAssistantMessageSchema = assistantModelMessageSchema;\nvar toolModelMessageSchema = z5.object({\n  role: z5.literal(\"tool\"),\n  content: z5.array(toolResultPartSchema),\n  providerOptions: providerMetadataSchema.optional()\n});\nvar coreToolMessageSchema = toolModelMessageSchema;\nvar modelMessageSchema = z5.union([\n  systemModelMessageSchema,\n  userModelMessageSchema,\n  assistantModelMessageSchema,\n  toolModelMessageSchema\n]);\nvar coreMessageSchema = modelMessageSchema;\n\n// src/prompt/standardize-prompt.ts\nasync function standardizePrompt(prompt) {\n  if (prompt.prompt == null && prompt.messages == null) {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"prompt or messages must be defined\"\n    });\n  }\n  if (prompt.prompt != null && prompt.messages != null) {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"prompt and messages cannot be defined at the same time\"\n    });\n  }\n  if (prompt.system != null && typeof prompt.system !== \"string\") {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"system must be a string\"\n    });\n  }\n  let messages;\n  if (prompt.prompt != null && typeof prompt.prompt === \"string\") {\n    messages = [{ role: \"user\", content: prompt.prompt }];\n  } else if (prompt.prompt != null && Array.isArray(prompt.prompt)) {\n    messages = prompt.prompt;\n  } else if (prompt.messages != null) {\n    messages = prompt.messages;\n  } else {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"prompt or messages must be defined\"\n    });\n  }\n  if (messages.length === 0) {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"messages must not be empty\"\n    });\n  }\n  const validationResult = await safeValidateTypes({\n    value: messages,\n    schema: z6.array(modelMessageSchema)\n  });\n  if (!validationResult.success) {\n    throw new InvalidPromptError2({\n      prompt,\n      message: \"The messages must be a ModelMessage[]. If you have passed a UIMessage[], you can use convertToModelMessages to convert them.\",\n      cause: validationResult.error\n    });\n  }\n  return {\n    messages,\n    system: prompt.system\n  };\n}\n\n// src/prompt/wrap-gateway-error.ts\nimport {\n  GatewayAuthenticationError,\n  GatewayModelNotFoundError\n} from \"@ai-sdk/gateway\";\nimport { AISDKError as AISDKError19 } from \"@ai-sdk/provider\";\nfunction wrapGatewayError(error) {\n  if (GatewayAuthenticationError.isInstance(error) || GatewayModelNotFoundError.isInstance(error)) {\n    return new AISDKError19({\n      name: \"GatewayError\",\n      message: \"Vercel AI Gateway access failed. If you want to use AI SDK providers directly, use the providers, e.g. @ai-sdk/openai, or register a different global default provider.\",\n      cause: error\n    });\n  }\n  return error;\n}\n\n// src/telemetry/assemble-operation-name.ts\nfunction assembleOperationName({\n  operationId,\n  telemetry\n}) {\n  return {\n    // standardized operation and resource name:\n    \"operation.name\": `${operationId}${(telemetry == null ? void 0 : telemetry.functionId) != null ? ` ${telemetry.functionId}` : \"\"}`,\n    \"resource.name\": telemetry == null ? void 0 : telemetry.functionId,\n    // detailed, AI SDK specific data:\n    \"ai.operationId\": operationId,\n    \"ai.telemetry.functionId\": telemetry == null ? void 0 : telemetry.functionId\n  };\n}\n\n// src/telemetry/get-base-telemetry-attributes.ts\nfunction getBaseTelemetryAttributes({\n  model,\n  settings,\n  telemetry,\n  headers\n}) {\n  var _a17;\n  return {\n    \"ai.model.provider\": model.provider,\n    \"ai.model.id\": model.modelId,\n    // settings:\n    ...Object.entries(settings).reduce((attributes, [key, value]) => {\n      attributes[`ai.settings.${key}`] = value;\n      return attributes;\n    }, {}),\n    // add metadata as attributes:\n    ...Object.entries((_a17 = telemetry == null ? void 0 : telemetry.metadata) != null ? _a17 : {}).reduce(\n      (attributes, [key, value]) => {\n        attributes[`ai.telemetry.metadata.${key}`] = value;\n        return attributes;\n      },\n      {}\n    ),\n    // request headers\n    ...Object.entries(headers != null ? headers : {}).reduce((attributes, [key, value]) => {\n      if (value !== void 0) {\n        attributes[`ai.request.headers.${key}`] = value;\n      }\n      return attributes;\n    }, {})\n  };\n}\n\n// src/telemetry/get-tracer.ts\nimport { trace } from \"@opentelemetry/api\";\n\n// src/telemetry/noop-tracer.ts\nvar noopTracer = {\n  startSpan() {\n    return noopSpan;\n  },\n  startActiveSpan(name17, arg1, arg2, arg3) {\n    if (typeof arg1 === \"function\") {\n      return arg1(noopSpan);\n    }\n    if (typeof arg2 === \"function\") {\n      return arg2(noopSpan);\n    }\n    if (typeof arg3 === \"function\") {\n      return arg3(noopSpan);\n    }\n  }\n};\nvar noopSpan = {\n  spanContext() {\n    return noopSpanContext;\n  },\n  setAttribute() {\n    return this;\n  },\n  setAttributes() {\n    return this;\n  },\n  addEvent() {\n    return this;\n  },\n  addLink() {\n    return this;\n  },\n  addLinks() {\n    return this;\n  },\n  setStatus() {\n    return this;\n  },\n  updateName() {\n    return this;\n  },\n  end() {\n    return this;\n  },\n  isRecording() {\n    return false;\n  },\n  recordException() {\n    return this;\n  }\n};\nvar noopSpanContext = {\n  traceId: \"\",\n  spanId: \"\",\n  traceFlags: 0\n};\n\n// src/telemetry/get-tracer.ts\nfunction getTracer({\n  isEnabled = false,\n  tracer\n} = {}) {\n  if (!isEnabled) {\n    return noopTracer;\n  }\n  if (tracer) {\n    return tracer;\n  }\n  return trace.getTracer(\"ai\");\n}\n\n// src/telemetry/record-span.ts\nimport { SpanStatusCode } from \"@opentelemetry/api\";\nfunction recordSpan({\n  name: name17,\n  tracer,\n  attributes,\n  fn,\n  endWhenDone = true\n}) {\n  return tracer.startActiveSpan(name17, { attributes }, async (span) => {\n    try {\n      const result = await fn(span);\n      if (endWhenDone) {\n        span.end();\n      }\n      return result;\n    } catch (error) {\n      try {\n        recordErrorOnSpan(span, error);\n      } finally {\n        span.end();\n      }\n      throw error;\n    }\n  });\n}\nfunction recordErrorOnSpan(span, error) {\n  if (error instanceof Error) {\n    span.recordException({\n      name: error.name,\n      message: error.message,\n      stack: error.stack\n    });\n    span.setStatus({\n      code: SpanStatusCode.ERROR,\n      message: error.message\n    });\n  } else {\n    span.setStatus({ code: SpanStatusCode.ERROR });\n  }\n}\n\n// src/telemetry/select-telemetry-attributes.ts\nfunction selectTelemetryAttributes({\n  telemetry,\n  attributes\n}) {\n  if ((telemetry == null ? void 0 : telemetry.isEnabled) !== true) {\n    return {};\n  }\n  return Object.entries(attributes).reduce((attributes2, [key, value]) => {\n    if (value == null) {\n      return attributes2;\n    }\n    if (typeof value === \"object\" && \"input\" in value && typeof value.input === \"function\") {\n      if ((telemetry == null ? void 0 : telemetry.recordInputs) === false) {\n        return attributes2;\n      }\n      const result = value.input();\n      return result == null ? attributes2 : { ...attributes2, [key]: result };\n    }\n    if (typeof value === \"object\" && \"output\" in value && typeof value.output === \"function\") {\n      if ((telemetry == null ? void 0 : telemetry.recordOutputs) === false) {\n        return attributes2;\n      }\n      const result = value.output();\n      return result == null ? attributes2 : { ...attributes2, [key]: result };\n    }\n    return { ...attributes2, [key]: value };\n  }, {});\n}\n\n// src/telemetry/stringify-for-telemetry.ts\nfunction stringifyForTelemetry(prompt) {\n  return JSON.stringify(\n    prompt.map((message) => ({\n      ...message,\n      content: typeof message.content === \"string\" ? message.content : message.content.map(\n        (part) => part.type === \"file\" ? {\n          ...part,\n          data: part.data instanceof Uint8Array ? convertDataContentToBase64String(part.data) : part.data\n        } : part\n      )\n    }))\n  );\n}\n\n// src/types/usage.ts\nfunction addLanguageModelUsage(usage1, usage2) {\n  return {\n    inputTokens: addTokenCounts(usage1.inputTokens, usage2.inputTokens),\n    outputTokens: addTokenCounts(usage1.outputTokens, usage2.outputTokens),\n    totalTokens: addTokenCounts(usage1.totalTokens, usage2.totalTokens),\n    reasoningTokens: addTokenCounts(\n      usage1.reasoningTokens,\n      usage2.reasoningTokens\n    ),\n    cachedInputTokens: addTokenCounts(\n      usage1.cachedInputTokens,\n      usage2.cachedInputTokens\n    )\n  };\n}\nfunction addTokenCounts(tokenCount1, tokenCount2) {\n  return tokenCount1 == null && tokenCount2 == null ? void 0 : (tokenCount1 != null ? tokenCount1 : 0) + (tokenCount2 != null ? tokenCount2 : 0);\n}\n\n// src/util/as-array.ts\nfunction asArray(value) {\n  return value === void 0 ? [] : Array.isArray(value) ? value : [value];\n}\n\n// src/util/retry-with-exponential-backoff.ts\nimport { APICallError as APICallError2 } from \"@ai-sdk/provider\";\nimport { delay, getErrorMessage as getErrorMessage3, isAbortError } from \"@ai-sdk/provider-utils\";\nfunction getRetryDelayInMs({\n  error,\n  exponentialBackoffDelay\n}) {\n  const headers = error.responseHeaders;\n  if (!headers)\n    return exponentialBackoffDelay;\n  let ms;\n  const retryAfterMs = headers[\"retry-after-ms\"];\n  if (retryAfterMs) {\n    const timeoutMs = parseFloat(retryAfterMs);\n    if (!Number.isNaN(timeoutMs)) {\n      ms = timeoutMs;\n    }\n  }\n  const retryAfter = headers[\"retry-after\"];\n  if (retryAfter && ms === void 0) {\n    const timeoutSeconds = parseFloat(retryAfter);\n    if (!Number.isNaN(timeoutSeconds)) {\n      ms = timeoutSeconds * 1e3;\n    } else {\n      ms = Date.parse(retryAfter) - Date.now();\n    }\n  }\n  if (ms != null && !Number.isNaN(ms) && 0 <= ms && (ms < 60 * 1e3 || ms < exponentialBackoffDelay)) {\n    return ms;\n  }\n  return exponentialBackoffDelay;\n}\nvar retryWithExponentialBackoffRespectingRetryHeaders = ({\n  maxRetries = 2,\n  initialDelayInMs = 2e3,\n  backoffFactor = 2,\n  abortSignal\n} = {}) => async (f) => _retryWithExponentialBackoff(f, {\n  maxRetries,\n  delayInMs: initialDelayInMs,\n  backoffFactor,\n  abortSignal\n});\nasync function _retryWithExponentialBackoff(f, {\n  maxRetries,\n  delayInMs,\n  backoffFactor,\n  abortSignal\n}, errors = []) {\n  try {\n    return await f();\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error;\n    }\n    if (maxRetries === 0) {\n      throw error;\n    }\n    const errorMessage = getErrorMessage3(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n    if (tryNumber > maxRetries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} attempts. Last error: ${errorMessage}`,\n        reason: \"maxRetriesExceeded\",\n        errors: newErrors\n      });\n    }\n    if (error instanceof Error && APICallError2.isInstance(error) && error.isRetryable === true && tryNumber <= maxRetries) {\n      await delay(\n        getRetryDelayInMs({\n          error,\n          exponentialBackoffDelay: delayInMs\n        }),\n        { abortSignal }\n      );\n      return _retryWithExponentialBackoff(\n        f,\n        {\n          maxRetries,\n          delayInMs: backoffFactor * delayInMs,\n          backoffFactor,\n          abortSignal\n        },\n        newErrors\n      );\n    }\n    if (tryNumber === 1) {\n      throw error;\n    }\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempts with non-retryable error: '${errorMessage}'`,\n      reason: \"errorNotRetryable\",\n      errors: newErrors\n    });\n  }\n}\n\n// src/util/prepare-retries.ts\nfunction prepareRetries({\n  maxRetries,\n  abortSignal\n}) {\n  if (maxRetries != null) {\n    if (!Number.isInteger(maxRetries)) {\n      throw new InvalidArgumentError({\n        parameter: \"maxRetries\",\n        value: maxRetries,\n        message: \"maxRetries must be an integer\"\n      });\n    }\n    if (maxRetries < 0) {\n      throw new InvalidArgumentError({\n        parameter: \"maxRetries\",\n        value: maxRetries,\n        message: \"maxRetries must be >= 0\"\n      });\n    }\n  }\n  const maxRetriesResult = maxRetries != null ? maxRetries : 2;\n  return {\n    maxRetries: maxRetriesResult,\n    retry: retryWithExponentialBackoffRespectingRetryHeaders({\n      maxRetries: maxRetriesResult,\n      abortSignal\n    })\n  };\n}\n\n// src/generate-text/extract-text-content.ts\nfunction extractTextContent(content) {\n  const parts = content.filter(\n    (content2) => content2.type === \"text\"\n  );\n  if (parts.length === 0) {\n    return void 0;\n  }\n  return parts.map((content2) => content2.text).join(\"\");\n}\n\n// src/generate-text/generated-file.ts\nimport {\n  convertBase64ToUint8Array as convertBase64ToUint8Array3,\n  convertUint8ArrayToBase64 as convertUint8ArrayToBase642\n} from \"@ai-sdk/provider-utils\";\nvar DefaultGeneratedFile = class {\n  constructor({\n    data,\n    mediaType\n  }) {\n    const isUint8Array = data instanceof Uint8Array;\n    this.base64Data = isUint8Array ? void 0 : data;\n    this.uint8ArrayData = isUint8Array ? data : void 0;\n    this.mediaType = mediaType;\n  }\n  // lazy conversion with caching to avoid unnecessary conversion overhead:\n  get base64() {\n    if (this.base64Data == null) {\n      this.base64Data = convertUint8ArrayToBase642(this.uint8ArrayData);\n    }\n    return this.base64Data;\n  }\n  // lazy conversion with caching to avoid unnecessary conversion overhead:\n  get uint8Array() {\n    if (this.uint8ArrayData == null) {\n      this.uint8ArrayData = convertBase64ToUint8Array3(this.base64Data);\n    }\n    return this.uint8ArrayData;\n  }\n};\nvar DefaultGeneratedFileWithType = class extends DefaultGeneratedFile {\n  constructor(options) {\n    super(options);\n    this.type = \"file\";\n  }\n};\n\n// src/generate-text/parse-tool-call.ts\nimport {\n  asSchema as asSchema2,\n  safeParseJSON,\n  safeValidateTypes as safeValidateTypes2\n} from \"@ai-sdk/provider-utils\";\nasync function parseToolCall({\n  toolCall,\n  tools,\n  repairToolCall,\n  system,\n  messages\n}) {\n  try {\n    if (tools == null) {\n      throw new NoSuchToolError({ toolName: toolCall.toolName });\n    }\n    try {\n      return await doParseToolCall({ toolCall, tools });\n    } catch (error) {\n      if (repairToolCall == null || !(NoSuchToolError.isInstance(error) || InvalidToolInputError.isInstance(error))) {\n        throw error;\n      }\n      let repairedToolCall = null;\n      try {\n        repairedToolCall = await repairToolCall({\n          toolCall,\n          tools,\n          inputSchema: ({ toolName }) => {\n            const { inputSchema } = tools[toolName];\n            return asSchema2(inputSchema).jsonSchema;\n          },\n          system,\n          messages,\n          error\n        });\n      } catch (repairError) {\n        throw new ToolCallRepairError({\n          cause: repairError,\n          originalError: error\n        });\n      }\n      if (repairedToolCall == null) {\n        throw error;\n      }\n      return await doParseToolCall({ toolCall: repairedToolCall, tools });\n    }\n  } catch (error) {\n    const parsedInput = await safeParseJSON({ text: toolCall.input });\n    const input = parsedInput.success ? parsedInput.value : toolCall.input;\n    return {\n      type: \"tool-call\",\n      toolCallId: toolCall.toolCallId,\n      toolName: toolCall.toolName,\n      input,\n      dynamic: true,\n      invalid: true,\n      error\n    };\n  }\n}\nasync function doParseToolCall({\n  toolCall,\n  tools\n}) {\n  const toolName = toolCall.toolName;\n  const tool3 = tools[toolName];\n  if (tool3 == null) {\n    throw new NoSuchToolError({\n      toolName: toolCall.toolName,\n      availableTools: Object.keys(tools)\n    });\n  }\n  const schema = asSchema2(tool3.inputSchema);\n  const parseResult = toolCall.input.trim() === \"\" ? await safeValidateTypes2({ value: {}, schema }) : await safeParseJSON({ text: toolCall.input, schema });\n  if (parseResult.success === false) {\n    throw new InvalidToolInputError({\n      toolName,\n      toolInput: toolCall.input,\n      cause: parseResult.error\n    });\n  }\n  return tool3.type === \"dynamic\" ? {\n    type: \"tool-call\",\n    toolCallId: toolCall.toolCallId,\n    toolName: toolCall.toolName,\n    input: parseResult.value,\n    providerExecuted: toolCall.providerExecuted,\n    providerMetadata: toolCall.providerMetadata,\n    dynamic: true\n  } : {\n    type: \"tool-call\",\n    toolCallId: toolCall.toolCallId,\n    toolName,\n    input: parseResult.value,\n    providerExecuted: toolCall.providerExecuted,\n    providerMetadata: toolCall.providerMetadata\n  };\n}\n\n// src/generate-text/step-result.ts\nvar DefaultStepResult = class {\n  constructor({\n    content,\n    finishReason,\n    usage,\n    warnings,\n    request,\n    response,\n    providerMetadata\n  }) {\n    this.content = content;\n    this.finishReason = finishReason;\n    this.usage = usage;\n    this.warnings = warnings;\n    this.request = request;\n    this.response = response;\n    this.providerMetadata = providerMetadata;\n  }\n  get text() {\n    return this.content.filter((part) => part.type === \"text\").map((part) => part.text).join(\"\");\n  }\n  get reasoning() {\n    return this.content.filter((part) => part.type === \"reasoning\");\n  }\n  get reasoningText() {\n    return this.reasoning.length === 0 ? void 0 : this.reasoning.map((part) => part.text).join(\"\");\n  }\n  get files() {\n    return this.content.filter((part) => part.type === \"file\").map((part) => part.file);\n  }\n  get sources() {\n    return this.content.filter((part) => part.type === \"source\");\n  }\n  get toolCalls() {\n    return this.content.filter((part) => part.type === \"tool-call\");\n  }\n  get staticToolCalls() {\n    return this.toolCalls.filter(\n      (toolCall) => toolCall.dynamic === false\n    );\n  }\n  get dynamicToolCalls() {\n    return this.toolCalls.filter(\n      (toolCall) => toolCall.dynamic === true\n    );\n  }\n  get toolResults() {\n    return this.content.filter((part) => part.type === \"tool-result\");\n  }\n  get staticToolResults() {\n    return this.toolResults.filter(\n      (toolResult) => toolResult.dynamic === false\n    );\n  }\n  get dynamicToolResults() {\n    return this.toolResults.filter(\n      (toolResult) => toolResult.dynamic === true\n    );\n  }\n};\n\n// src/generate-text/stop-condition.ts\nfunction stepCountIs(stepCount) {\n  return ({ steps }) => steps.length === stepCount;\n}\nfunction hasToolCall(toolName) {\n  return ({ steps }) => {\n    var _a17, _b, _c;\n    return (_c = (_b = (_a17 = steps[steps.length - 1]) == null ? void 0 : _a17.toolCalls) == null ? void 0 : _b.some(\n      (toolCall) => toolCall.toolName === toolName\n    )) != null ? _c : false;\n  };\n}\nasync function isStopConditionMet({\n  stopConditions,\n  steps\n}) {\n  return (await Promise.all(stopConditions.map((condition) => condition({ steps })))).some((result) => result);\n}\n\n// src/prompt/create-tool-model-output.ts\nimport {\n  getErrorMessage as getErrorMessage4\n} from \"@ai-sdk/provider\";\nfunction createToolModelOutput({\n  output,\n  tool: tool3,\n  errorMode\n}) {\n  if (errorMode === \"text\") {\n    return { type: \"error-text\", value: getErrorMessage4(output) };\n  } else if (errorMode === \"json\") {\n    return { type: \"error-json\", value: toJSONValue(output) };\n  }\n  if (tool3 == null ? void 0 : tool3.toModelOutput) {\n    return tool3.toModelOutput(output);\n  }\n  return typeof output === \"string\" ? { type: \"text\", value: output } : { type: \"json\", value: toJSONValue(output) };\n}\nfunction toJSONValue(value) {\n  return value === void 0 ? null : value;\n}\n\n// src/generate-text/to-response-messages.ts\nfunction toResponseMessages({\n  content: inputContent,\n  tools\n}) {\n  const responseMessages = [];\n  const content = inputContent.filter((part) => part.type !== \"source\").filter(\n    (part) => (part.type !== \"tool-result\" || part.providerExecuted) && (part.type !== \"tool-error\" || part.providerExecuted)\n  ).filter((part) => part.type !== \"text\" || part.text.length > 0).map((part) => {\n    switch (part.type) {\n      case \"text\":\n        return {\n          type: \"text\",\n          text: part.text,\n          providerOptions: part.providerMetadata\n        };\n      case \"reasoning\":\n        return {\n          type: \"reasoning\",\n          text: part.text,\n          providerOptions: part.providerMetadata\n        };\n      case \"file\":\n        return {\n          type: \"file\",\n          data: part.file.base64,\n          mediaType: part.file.mediaType,\n          providerOptions: part.providerMetadata\n        };\n      case \"tool-call\":\n        return {\n          type: \"tool-call\",\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          input: part.input,\n          providerExecuted: part.providerExecuted,\n          providerOptions: part.providerMetadata\n        };\n      case \"tool-result\":\n        return {\n          type: \"tool-result\",\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          output: createToolModelOutput({\n            tool: tools == null ? void 0 : tools[part.toolName],\n            output: part.output,\n            errorMode: \"none\"\n          }),\n          providerExecuted: true,\n          providerOptions: part.providerMetadata\n        };\n      case \"tool-error\":\n        return {\n          type: \"tool-result\",\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          output: createToolModelOutput({\n            tool: tools == null ? void 0 : tools[part.toolName],\n            output: part.error,\n            errorMode: \"json\"\n          }),\n          providerOptions: part.providerMetadata\n        };\n    }\n  });\n  if (content.length > 0) {\n    responseMessages.push({\n      role: \"assistant\",\n      content\n    });\n  }\n  const toolResultContent = inputContent.filter((part) => part.type === \"tool-result\" || part.type === \"tool-error\").filter((part) => !part.providerExecuted).map((toolResult) => ({\n    type: \"tool-result\",\n    toolCallId: toolResult.toolCallId,\n    toolName: toolResult.toolName,\n    output: createToolModelOutput({\n      tool: tools == null ? void 0 : tools[toolResult.toolName],\n      output: toolResult.type === \"tool-result\" ? toolResult.output : toolResult.error,\n      errorMode: toolResult.type === \"tool-error\" ? \"text\" : \"none\"\n    })\n  }));\n  if (toolResultContent.length > 0) {\n    responseMessages.push({\n      role: \"tool\",\n      content: toolResultContent\n    });\n  }\n  return responseMessages;\n}\n\n// src/generate-text/generate-text.ts\nvar originalGenerateId = createIdGenerator({\n  prefix: \"aitxt\",\n  size: 24\n});\nasync function generateText({\n  model: modelArg,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers,\n  stopWhen = stepCountIs(1),\n  experimental_output: output,\n  experimental_telemetry: telemetry,\n  providerOptions,\n  experimental_activeTools,\n  activeTools = experimental_activeTools,\n  experimental_prepareStep,\n  prepareStep = experimental_prepareStep,\n  experimental_repairToolCall: repairToolCall,\n  experimental_download: download2,\n  experimental_context,\n  _internal: {\n    generateId: generateId3 = originalGenerateId,\n    currentDate = () => /* @__PURE__ */ new Date()\n  } = {},\n  onStepFinish,\n  ...settings\n}) {\n  const model = resolveLanguageModel(modelArg);\n  const stopConditions = asArray(stopWhen);\n  const { maxRetries, retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const callSettings = prepareCallSettings(settings);\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...callSettings, maxRetries }\n  });\n  const initialPrompt = await standardizePrompt({\n    system,\n    prompt,\n    messages\n  });\n  const tracer = getTracer(telemetry);\n  try {\n    return await recordSpan({\n      name: \"ai.generateText\",\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({\n            operationId: \"ai.generateText\",\n            telemetry\n          }),\n          ...baseTelemetryAttributes,\n          // model:\n          \"ai.model.provider\": model.provider,\n          \"ai.model.id\": model.modelId,\n          // specific settings that only make sense on the outer level:\n          \"ai.prompt\": {\n            input: () => JSON.stringify({ system, prompt, messages })\n          }\n        }\n      }),\n      tracer,\n      fn: async (span) => {\n        var _a17, _b, _c, _d, _e, _f, _g;\n        const callSettings2 = prepareCallSettings(settings);\n        let currentModelResponse;\n        let clientToolCalls = [];\n        let clientToolOutputs = [];\n        const responseMessages = [];\n        const steps = [];\n        do {\n          const stepInputMessages = [\n            ...initialPrompt.messages,\n            ...responseMessages\n          ];\n          const prepareStepResult = await (prepareStep == null ? void 0 : prepareStep({\n            model,\n            steps,\n            stepNumber: steps.length,\n            messages: stepInputMessages\n          }));\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: {\n              system: (_a17 = prepareStepResult == null ? void 0 : prepareStepResult.system) != null ? _a17 : initialPrompt.system,\n              messages: (_b = prepareStepResult == null ? void 0 : prepareStepResult.messages) != null ? _b : stepInputMessages\n            },\n            supportedUrls: await model.supportedUrls,\n            download: download2\n          });\n          const stepModel = resolveLanguageModel(\n            (_c = prepareStepResult == null ? void 0 : prepareStepResult.model) != null ? _c : model\n          );\n          const { toolChoice: stepToolChoice, tools: stepTools } = prepareToolsAndToolChoice({\n            tools,\n            toolChoice: (_d = prepareStepResult == null ? void 0 : prepareStepResult.toolChoice) != null ? _d : toolChoice,\n            activeTools: (_e = prepareStepResult == null ? void 0 : prepareStepResult.activeTools) != null ? _e : activeTools\n          });\n          currentModelResponse = await retry(\n            () => {\n              var _a18;\n              return recordSpan({\n                name: \"ai.generateText.doGenerate\",\n                attributes: selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    ...assembleOperationName({\n                      operationId: \"ai.generateText.doGenerate\",\n                      telemetry\n                    }),\n                    ...baseTelemetryAttributes,\n                    // model:\n                    \"ai.model.provider\": stepModel.provider,\n                    \"ai.model.id\": stepModel.modelId,\n                    // prompt:\n                    \"ai.prompt.messages\": {\n                      input: () => stringifyForTelemetry(promptMessages)\n                    },\n                    \"ai.prompt.tools\": {\n                      // convert the language model level tools:\n                      input: () => stepTools == null ? void 0 : stepTools.map((tool3) => JSON.stringify(tool3))\n                    },\n                    \"ai.prompt.toolChoice\": {\n                      input: () => stepToolChoice != null ? JSON.stringify(stepToolChoice) : void 0\n                    },\n                    // standardized gen-ai llm span attributes:\n                    \"gen_ai.system\": stepModel.provider,\n                    \"gen_ai.request.model\": stepModel.modelId,\n                    \"gen_ai.request.frequency_penalty\": settings.frequencyPenalty,\n                    \"gen_ai.request.max_tokens\": settings.maxOutputTokens,\n                    \"gen_ai.request.presence_penalty\": settings.presencePenalty,\n                    \"gen_ai.request.stop_sequences\": settings.stopSequences,\n                    \"gen_ai.request.temperature\": (_a18 = settings.temperature) != null ? _a18 : void 0,\n                    \"gen_ai.request.top_k\": settings.topK,\n                    \"gen_ai.request.top_p\": settings.topP\n                  }\n                }),\n                tracer,\n                fn: async (span2) => {\n                  var _a19, _b2, _c2, _d2, _e2, _f2, _g2, _h;\n                  const result = await stepModel.doGenerate({\n                    ...callSettings2,\n                    tools: stepTools,\n                    toolChoice: stepToolChoice,\n                    responseFormat: output == null ? void 0 : output.responseFormat,\n                    prompt: promptMessages,\n                    providerOptions,\n                    abortSignal,\n                    headers\n                  });\n                  const responseData = {\n                    id: (_b2 = (_a19 = result.response) == null ? void 0 : _a19.id) != null ? _b2 : generateId3(),\n                    timestamp: (_d2 = (_c2 = result.response) == null ? void 0 : _c2.timestamp) != null ? _d2 : currentDate(),\n                    modelId: (_f2 = (_e2 = result.response) == null ? void 0 : _e2.modelId) != null ? _f2 : stepModel.modelId,\n                    headers: (_g2 = result.response) == null ? void 0 : _g2.headers,\n                    body: (_h = result.response) == null ? void 0 : _h.body\n                  };\n                  span2.setAttributes(\n                    selectTelemetryAttributes({\n                      telemetry,\n                      attributes: {\n                        \"ai.response.finishReason\": result.finishReason,\n                        \"ai.response.text\": {\n                          output: () => extractTextContent(result.content)\n                        },\n                        \"ai.response.toolCalls\": {\n                          output: () => {\n                            const toolCalls = asToolCalls(result.content);\n                            return toolCalls == null ? void 0 : JSON.stringify(toolCalls);\n                          }\n                        },\n                        \"ai.response.id\": responseData.id,\n                        \"ai.response.model\": responseData.modelId,\n                        \"ai.response.timestamp\": responseData.timestamp.toISOString(),\n                        \"ai.response.providerMetadata\": JSON.stringify(\n                          result.providerMetadata\n                        ),\n                        // TODO rename telemetry attributes to inputTokens and outputTokens\n                        \"ai.usage.promptTokens\": result.usage.inputTokens,\n                        \"ai.usage.completionTokens\": result.usage.outputTokens,\n                        // standardized gen-ai llm span attributes:\n                        \"gen_ai.response.finish_reasons\": [result.finishReason],\n                        \"gen_ai.response.id\": responseData.id,\n                        \"gen_ai.response.model\": responseData.modelId,\n                        \"gen_ai.usage.input_tokens\": result.usage.inputTokens,\n                        \"gen_ai.usage.output_tokens\": result.usage.outputTokens\n                      }\n                    })\n                  );\n                  return { ...result, response: responseData };\n                }\n              });\n            }\n          );\n          const stepToolCalls = await Promise.all(\n            currentModelResponse.content.filter(\n              (part) => part.type === \"tool-call\"\n            ).map(\n              (toolCall) => parseToolCall({\n                toolCall,\n                tools,\n                repairToolCall,\n                system,\n                messages: stepInputMessages\n              })\n            )\n          );\n          for (const toolCall of stepToolCalls) {\n            if (toolCall.invalid) {\n              continue;\n            }\n            const tool3 = tools[toolCall.toolName];\n            if ((tool3 == null ? void 0 : tool3.onInputAvailable) != null) {\n              await tool3.onInputAvailable({\n                input: toolCall.input,\n                toolCallId: toolCall.toolCallId,\n                messages: stepInputMessages,\n                abortSignal,\n                experimental_context\n              });\n            }\n          }\n          const invalidToolCalls = stepToolCalls.filter(\n            (toolCall) => toolCall.invalid && toolCall.dynamic\n          );\n          clientToolOutputs = [];\n          for (const toolCall of invalidToolCalls) {\n            clientToolOutputs.push({\n              type: \"tool-error\",\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              input: toolCall.input,\n              error: getErrorMessage5(toolCall.error),\n              dynamic: true\n            });\n          }\n          clientToolCalls = stepToolCalls.filter(\n            (toolCall) => !toolCall.providerExecuted\n          );\n          if (tools != null) {\n            clientToolOutputs.push(\n              ...await executeTools({\n                toolCalls: clientToolCalls.filter(\n                  (toolCall) => !toolCall.invalid\n                ),\n                tools,\n                tracer,\n                telemetry,\n                messages: stepInputMessages,\n                abortSignal,\n                experimental_context\n              })\n            );\n          }\n          const stepContent = asContent({\n            content: currentModelResponse.content,\n            toolCalls: stepToolCalls,\n            toolOutputs: clientToolOutputs\n          });\n          responseMessages.push(\n            ...toResponseMessages({\n              content: stepContent,\n              tools\n            })\n          );\n          const currentStepResult = new DefaultStepResult({\n            content: stepContent,\n            finishReason: currentModelResponse.finishReason,\n            usage: currentModelResponse.usage,\n            warnings: currentModelResponse.warnings,\n            providerMetadata: currentModelResponse.providerMetadata,\n            request: (_f = currentModelResponse.request) != null ? _f : {},\n            response: {\n              ...currentModelResponse.response,\n              // deep clone msgs to avoid mutating past messages in multi-step:\n              messages: structuredClone(responseMessages)\n            }\n          });\n          logWarnings((_g = currentModelResponse.warnings) != null ? _g : []);\n          steps.push(currentStepResult);\n          await (onStepFinish == null ? void 0 : onStepFinish(currentStepResult));\n        } while (\n          // there are tool calls:\n          clientToolCalls.length > 0 && // all current tool calls have outputs (incl. execution errors):\n          clientToolOutputs.length === clientToolCalls.length && // continue until a stop condition is met:\n          !await isStopConditionMet({ stopConditions, steps })\n        );\n        span.setAttributes(\n          selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              \"ai.response.finishReason\": currentModelResponse.finishReason,\n              \"ai.response.text\": {\n                output: () => extractTextContent(currentModelResponse.content)\n              },\n              \"ai.response.toolCalls\": {\n                output: () => {\n                  const toolCalls = asToolCalls(currentModelResponse.content);\n                  return toolCalls == null ? void 0 : JSON.stringify(toolCalls);\n                }\n              },\n              \"ai.response.providerMetadata\": JSON.stringify(\n                currentModelResponse.providerMetadata\n              ),\n              // TODO rename telemetry attributes to inputTokens and outputTokens\n              \"ai.usage.promptTokens\": currentModelResponse.usage.inputTokens,\n              \"ai.usage.completionTokens\": currentModelResponse.usage.outputTokens\n            }\n          })\n        );\n        const lastStep = steps[steps.length - 1];\n        return new DefaultGenerateTextResult({\n          steps,\n          resolvedOutput: await (output == null ? void 0 : output.parseOutput(\n            { text: lastStep.text },\n            {\n              response: lastStep.response,\n              usage: lastStep.usage,\n              finishReason: lastStep.finishReason\n            }\n          ))\n        });\n      }\n    });\n  } catch (error) {\n    throw wrapGatewayError(error);\n  }\n}\nasync function executeTools({\n  toolCalls,\n  tools,\n  tracer,\n  telemetry,\n  messages,\n  abortSignal,\n  experimental_context\n}) {\n  const toolOutputs = await Promise.all(\n    toolCalls.map(async ({ toolCallId, toolName, input }) => {\n      const tool3 = tools[toolName];\n      if ((tool3 == null ? void 0 : tool3.execute) == null) {\n        return void 0;\n      }\n      return recordSpan({\n        name: \"ai.toolCall\",\n        attributes: selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            ...assembleOperationName({\n              operationId: \"ai.toolCall\",\n              telemetry\n            }),\n            \"ai.toolCall.name\": toolName,\n            \"ai.toolCall.id\": toolCallId,\n            \"ai.toolCall.args\": {\n              output: () => JSON.stringify(input)\n            }\n          }\n        }),\n        tracer,\n        fn: async (span) => {\n          try {\n            const stream = executeTool({\n              execute: tool3.execute.bind(tool3),\n              input,\n              options: {\n                toolCallId,\n                messages,\n                abortSignal,\n                experimental_context\n              }\n            });\n            let output;\n            for await (const part of stream) {\n              if (part.type === \"final\") {\n                output = part.output;\n              }\n            }\n            try {\n              span.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    \"ai.toolCall.result\": {\n                      output: () => JSON.stringify(output)\n                    }\n                  }\n                })\n              );\n            } catch (ignored) {\n            }\n            return {\n              type: \"tool-result\",\n              toolCallId,\n              toolName,\n              input,\n              output,\n              dynamic: tool3.type === \"dynamic\"\n            };\n          } catch (error) {\n            recordErrorOnSpan(span, error);\n            return {\n              type: \"tool-error\",\n              toolCallId,\n              toolName,\n              input,\n              error,\n              dynamic: tool3.type === \"dynamic\"\n            };\n          }\n        }\n      });\n    })\n  );\n  return toolOutputs.filter(\n    (output) => output != null\n  );\n}\nvar DefaultGenerateTextResult = class {\n  constructor(options) {\n    this.steps = options.steps;\n    this.resolvedOutput = options.resolvedOutput;\n  }\n  get finalStep() {\n    return this.steps[this.steps.length - 1];\n  }\n  get content() {\n    return this.finalStep.content;\n  }\n  get text() {\n    return this.finalStep.text;\n  }\n  get files() {\n    return this.finalStep.files;\n  }\n  get reasoningText() {\n    return this.finalStep.reasoningText;\n  }\n  get reasoning() {\n    return this.finalStep.reasoning;\n  }\n  get toolCalls() {\n    return this.finalStep.toolCalls;\n  }\n  get staticToolCalls() {\n    return this.finalStep.staticToolCalls;\n  }\n  get dynamicToolCalls() {\n    return this.finalStep.dynamicToolCalls;\n  }\n  get toolResults() {\n    return this.finalStep.toolResults;\n  }\n  get staticToolResults() {\n    return this.finalStep.staticToolResults;\n  }\n  get dynamicToolResults() {\n    return this.finalStep.dynamicToolResults;\n  }\n  get sources() {\n    return this.finalStep.sources;\n  }\n  get finishReason() {\n    return this.finalStep.finishReason;\n  }\n  get warnings() {\n    return this.finalStep.warnings;\n  }\n  get providerMetadata() {\n    return this.finalStep.providerMetadata;\n  }\n  get response() {\n    return this.finalStep.response;\n  }\n  get request() {\n    return this.finalStep.request;\n  }\n  get usage() {\n    return this.finalStep.usage;\n  }\n  get totalUsage() {\n    return this.steps.reduce(\n      (totalUsage, step) => {\n        return addLanguageModelUsage(totalUsage, step.usage);\n      },\n      {\n        inputTokens: void 0,\n        outputTokens: void 0,\n        totalTokens: void 0,\n        reasoningTokens: void 0,\n        cachedInputTokens: void 0\n      }\n    );\n  }\n  get experimental_output() {\n    if (this.resolvedOutput == null) {\n      throw new NoOutputSpecifiedError();\n    }\n    return this.resolvedOutput;\n  }\n};\nfunction asToolCalls(content) {\n  const parts = content.filter(\n    (part) => part.type === \"tool-call\"\n  );\n  if (parts.length === 0) {\n    return void 0;\n  }\n  return parts.map((toolCall) => ({\n    toolCallId: toolCall.toolCallId,\n    toolName: toolCall.toolName,\n    input: toolCall.input\n  }));\n}\nfunction asContent({\n  content,\n  toolCalls,\n  toolOutputs\n}) {\n  return [\n    ...content.map((part) => {\n      switch (part.type) {\n        case \"text\":\n        case \"reasoning\":\n        case \"source\":\n          return part;\n        case \"file\": {\n          return {\n            type: \"file\",\n            file: new DefaultGeneratedFile(part)\n          };\n        }\n        case \"tool-call\": {\n          return toolCalls.find(\n            (toolCall) => toolCall.toolCallId === part.toolCallId\n          );\n        }\n        case \"tool-result\": {\n          const toolCall = toolCalls.find(\n            (toolCall2) => toolCall2.toolCallId === part.toolCallId\n          );\n          if (toolCall == null) {\n            throw new Error(`Tool call ${part.toolCallId} not found.`);\n          }\n          if (part.isError) {\n            return {\n              type: \"tool-error\",\n              toolCallId: part.toolCallId,\n              toolName: part.toolName,\n              input: toolCall.input,\n              error: part.result,\n              providerExecuted: true,\n              dynamic: toolCall.dynamic\n            };\n          }\n          return {\n            type: \"tool-result\",\n            toolCallId: part.toolCallId,\n            toolName: part.toolName,\n            input: toolCall.input,\n            output: part.result,\n            providerExecuted: true,\n            dynamic: toolCall.dynamic\n          };\n        }\n      }\n    }),\n    ...toolOutputs\n  ];\n}\n\n// src/generate-text/stream-text.ts\nimport {\n  getErrorMessage as getErrorMessage7\n} from \"@ai-sdk/provider\";\nimport {\n  createIdGenerator as createIdGenerator2,\n  isAbortError as isAbortError2\n} from \"@ai-sdk/provider-utils\";\n\n// src/util/prepare-headers.ts\nfunction prepareHeaders(headers, defaultHeaders) {\n  const responseHeaders = new Headers(headers != null ? headers : {});\n  for (const [key, value] of Object.entries(defaultHeaders)) {\n    if (!responseHeaders.has(key)) {\n      responseHeaders.set(key, value);\n    }\n  }\n  return responseHeaders;\n}\n\n// src/text-stream/create-text-stream-response.ts\nfunction createTextStreamResponse({\n  status,\n  statusText,\n  headers,\n  textStream\n}) {\n  return new Response(textStream.pipeThrough(new TextEncoderStream()), {\n    status: status != null ? status : 200,\n    statusText,\n    headers: prepareHeaders(headers, {\n      \"content-type\": \"text/plain; charset=utf-8\"\n    })\n  });\n}\n\n// src/util/write-to-server-response.ts\nfunction writeToServerResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  stream\n}) {\n  response.writeHead(status != null ? status : 200, statusText, headers);\n  const reader = stream.getReader();\n  const read = async () => {\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done)\n          break;\n        response.write(value);\n      }\n    } catch (error) {\n      throw error;\n    } finally {\n      response.end();\n    }\n  };\n  read();\n}\n\n// src/text-stream/pipe-text-stream-to-response.ts\nfunction pipeTextStreamToResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  textStream\n}) {\n  writeToServerResponse({\n    response,\n    status,\n    statusText,\n    headers: Object.fromEntries(\n      prepareHeaders(headers, {\n        \"content-type\": \"text/plain; charset=utf-8\"\n      }).entries()\n    ),\n    stream: textStream.pipeThrough(new TextEncoderStream())\n  });\n}\n\n// src/ui-message-stream/json-to-sse-transform-stream.ts\nvar JsonToSseTransformStream = class extends TransformStream {\n  constructor() {\n    super({\n      transform(part, controller) {\n        controller.enqueue(`data: ${JSON.stringify(part)}\n\n`);\n      },\n      flush(controller) {\n        controller.enqueue(\"data: [DONE]\\n\\n\");\n      }\n    });\n  }\n};\n\n// src/ui-message-stream/ui-message-stream-headers.ts\nvar UI_MESSAGE_STREAM_HEADERS = {\n  \"content-type\": \"text/event-stream\",\n  \"cache-control\": \"no-cache\",\n  connection: \"keep-alive\",\n  \"x-vercel-ai-ui-message-stream\": \"v1\",\n  \"x-accel-buffering\": \"no\"\n  // disable nginx buffering\n};\n\n// src/ui-message-stream/create-ui-message-stream-response.ts\nfunction createUIMessageStreamResponse({\n  status,\n  statusText,\n  headers,\n  stream,\n  consumeSseStream\n}) {\n  let sseStream = stream.pipeThrough(new JsonToSseTransformStream());\n  if (consumeSseStream) {\n    const [stream1, stream2] = sseStream.tee();\n    sseStream = stream1;\n    consumeSseStream({ stream: stream2 });\n  }\n  return new Response(sseStream.pipeThrough(new TextEncoderStream()), {\n    status,\n    statusText,\n    headers: prepareHeaders(headers, UI_MESSAGE_STREAM_HEADERS)\n  });\n}\n\n// src/ui-message-stream/get-response-ui-message-id.ts\nfunction getResponseUIMessageId({\n  originalMessages,\n  responseMessageId\n}) {\n  if (originalMessages == null) {\n    return void 0;\n  }\n  const lastMessage = originalMessages[originalMessages.length - 1];\n  return (lastMessage == null ? void 0 : lastMessage.role) === \"assistant\" ? lastMessage.id : typeof responseMessageId === \"function\" ? responseMessageId() : responseMessageId;\n}\n\n// src/ui/process-ui-message-stream.ts\nimport {\n  validateTypes\n} from \"@ai-sdk/provider-utils\";\n\n// src/ui-message-stream/ui-message-chunks.ts\nimport { z as z7 } from \"zod/v4\";\nvar uiMessageChunkSchema = z7.union([\n  z7.strictObject({\n    type: z7.literal(\"text-start\"),\n    id: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"text-delta\"),\n    id: z7.string(),\n    delta: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"text-end\"),\n    id: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"error\"),\n    errorText: z7.string()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-input-start\"),\n    toolCallId: z7.string(),\n    toolName: z7.string(),\n    providerExecuted: z7.boolean().optional(),\n    dynamic: z7.boolean().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-input-delta\"),\n    toolCallId: z7.string(),\n    inputTextDelta: z7.string()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-input-available\"),\n    toolCallId: z7.string(),\n    toolName: z7.string(),\n    input: z7.unknown(),\n    providerExecuted: z7.boolean().optional(),\n    providerMetadata: providerMetadataSchema.optional(),\n    dynamic: z7.boolean().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-input-error\"),\n    toolCallId: z7.string(),\n    toolName: z7.string(),\n    input: z7.unknown(),\n    providerExecuted: z7.boolean().optional(),\n    providerMetadata: providerMetadataSchema.optional(),\n    dynamic: z7.boolean().optional(),\n    errorText: z7.string()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-output-available\"),\n    toolCallId: z7.string(),\n    output: z7.unknown(),\n    providerExecuted: z7.boolean().optional(),\n    dynamic: z7.boolean().optional(),\n    preliminary: z7.boolean().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"tool-output-error\"),\n    toolCallId: z7.string(),\n    errorText: z7.string(),\n    providerExecuted: z7.boolean().optional(),\n    dynamic: z7.boolean().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"reasoning\"),\n    text: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"reasoning-start\"),\n    id: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"reasoning-delta\"),\n    id: z7.string(),\n    delta: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"reasoning-end\"),\n    id: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"reasoning-part-finish\")\n  }),\n  z7.strictObject({\n    type: z7.literal(\"source-url\"),\n    sourceId: z7.string(),\n    url: z7.string(),\n    title: z7.string().optional(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"source-document\"),\n    sourceId: z7.string(),\n    mediaType: z7.string(),\n    title: z7.string(),\n    filename: z7.string().optional(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"file\"),\n    url: z7.string(),\n    mediaType: z7.string(),\n    providerMetadata: providerMetadataSchema.optional()\n  }),\n  z7.strictObject({\n    type: z7.string().startsWith(\"data-\"),\n    id: z7.string().optional(),\n    data: z7.unknown(),\n    transient: z7.boolean().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"start-step\")\n  }),\n  z7.strictObject({\n    type: z7.literal(\"finish-step\")\n  }),\n  z7.strictObject({\n    type: z7.literal(\"start\"),\n    messageId: z7.string().optional(),\n    messageMetadata: z7.unknown().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"finish\"),\n    messageMetadata: z7.unknown().optional()\n  }),\n  z7.strictObject({\n    type: z7.literal(\"abort\")\n  }),\n  z7.strictObject({\n    type: z7.literal(\"message-metadata\"),\n    messageMetadata: z7.unknown()\n  })\n]);\nfunction isDataUIMessageChunk(chunk) {\n  return chunk.type.startsWith(\"data-\");\n}\n\n// src/util/merge-objects.ts\nfunction mergeObjects(base, overrides) {\n  if (base === void 0 && overrides === void 0) {\n    return void 0;\n  }\n  if (base === void 0) {\n    return overrides;\n  }\n  if (overrides === void 0) {\n    return base;\n  }\n  const result = { ...base };\n  for (const key in overrides) {\n    if (Object.prototype.hasOwnProperty.call(overrides, key)) {\n      const overridesValue = overrides[key];\n      if (overridesValue === void 0)\n        continue;\n      const baseValue = key in base ? base[key] : void 0;\n      const isSourceObject = overridesValue !== null && typeof overridesValue === \"object\" && !Array.isArray(overridesValue) && !(overridesValue instanceof Date) && !(overridesValue instanceof RegExp);\n      const isTargetObject = baseValue !== null && baseValue !== void 0 && typeof baseValue === \"object\" && !Array.isArray(baseValue) && !(baseValue instanceof Date) && !(baseValue instanceof RegExp);\n      if (isSourceObject && isTargetObject) {\n        result[key] = mergeObjects(\n          baseValue,\n          overridesValue\n        );\n      } else {\n        result[key] = overridesValue;\n      }\n    }\n  }\n  return result;\n}\n\n// src/util/parse-partial-json.ts\nimport { safeParseJSON as safeParseJSON2 } from \"@ai-sdk/provider-utils\";\n\n// src/util/fix-json.ts\nfunction fixJson(input) {\n  const stack = [\"ROOT\"];\n  let lastValidIndex = -1;\n  let literalStart = null;\n  function processValueStart(char, i, swapState) {\n    {\n      switch (char) {\n        case '\"': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_STRING\");\n          break;\n        }\n        case \"f\":\n        case \"t\":\n        case \"n\": {\n          lastValidIndex = i;\n          literalStart = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_LITERAL\");\n          break;\n        }\n        case \"-\": {\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_NUMBER\");\n          break;\n        }\n        case \"0\":\n        case \"1\":\n        case \"2\":\n        case \"3\":\n        case \"4\":\n        case \"5\":\n        case \"6\":\n        case \"7\":\n        case \"8\":\n        case \"9\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_NUMBER\");\n          break;\n        }\n        case \"{\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_OBJECT_START\");\n          break;\n        }\n        case \"[\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_ARRAY_START\");\n          break;\n        }\n      }\n    }\n  }\n  function processAfterObjectValue(char, i) {\n    switch (char) {\n      case \",\": {\n        stack.pop();\n        stack.push(\"INSIDE_OBJECT_AFTER_COMMA\");\n        break;\n      }\n      case \"}\": {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n  function processAfterArrayValue(char, i) {\n    switch (char) {\n      case \",\": {\n        stack.pop();\n        stack.push(\"INSIDE_ARRAY_AFTER_COMMA\");\n        break;\n      }\n      case \"]\": {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n  for (let i = 0; i < input.length; i++) {\n    const char = input[i];\n    const currentState = stack[stack.length - 1];\n    switch (currentState) {\n      case \"ROOT\":\n        processValueStart(char, i, \"FINISH\");\n        break;\n      case \"INSIDE_OBJECT_START\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_KEY\");\n            break;\n          }\n          case \"}\": {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_OBJECT_AFTER_COMMA\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_KEY\");\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_OBJECT_KEY\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_AFTER_KEY\");\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_OBJECT_AFTER_KEY\": {\n        switch (char) {\n          case \":\": {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_BEFORE_VALUE\");\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_OBJECT_BEFORE_VALUE\": {\n        processValueStart(char, i, \"INSIDE_OBJECT_AFTER_VALUE\");\n        break;\n      }\n      case \"INSIDE_OBJECT_AFTER_VALUE\": {\n        processAfterObjectValue(char, i);\n        break;\n      }\n      case \"INSIDE_STRING\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            lastValidIndex = i;\n            break;\n          }\n          case \"\\\\\": {\n            stack.push(\"INSIDE_STRING_ESCAPE\");\n            break;\n          }\n          default: {\n            lastValidIndex = i;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_ARRAY_START\": {\n        switch (char) {\n          case \"]\": {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n          default: {\n            lastValidIndex = i;\n            processValueStart(char, i, \"INSIDE_ARRAY_AFTER_VALUE\");\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_ARRAY_AFTER_VALUE\": {\n        switch (char) {\n          case \",\": {\n            stack.pop();\n            stack.push(\"INSIDE_ARRAY_AFTER_COMMA\");\n            break;\n          }\n          case \"]\": {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n          default: {\n            lastValidIndex = i;\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_ARRAY_AFTER_COMMA\": {\n        processValueStart(char, i, \"INSIDE_ARRAY_AFTER_VALUE\");\n        break;\n      }\n      case \"INSIDE_STRING_ESCAPE\": {\n        stack.pop();\n        lastValidIndex = i;\n        break;\n      }\n      case \"INSIDE_NUMBER\": {\n        switch (char) {\n          case \"0\":\n          case \"1\":\n          case \"2\":\n          case \"3\":\n          case \"4\":\n          case \"5\":\n          case \"6\":\n          case \"7\":\n          case \"8\":\n          case \"9\": {\n            lastValidIndex = i;\n            break;\n          }\n          case \"e\":\n          case \"E\":\n          case \"-\":\n          case \".\": {\n            break;\n          }\n          case \",\": {\n            stack.pop();\n            if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n              processAfterArrayValue(char, i);\n            }\n            if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n              processAfterObjectValue(char, i);\n            }\n            break;\n          }\n          case \"}\": {\n            stack.pop();\n            if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n              processAfterObjectValue(char, i);\n            }\n            break;\n          }\n          case \"]\": {\n            stack.pop();\n            if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n              processAfterArrayValue(char, i);\n            }\n            break;\n          }\n          default: {\n            stack.pop();\n            break;\n          }\n        }\n        break;\n      }\n      case \"INSIDE_LITERAL\": {\n        const partialLiteral = input.substring(literalStart, i + 1);\n        if (!\"false\".startsWith(partialLiteral) && !\"true\".startsWith(partialLiteral) && !\"null\".startsWith(partialLiteral)) {\n          stack.pop();\n          if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n            processAfterObjectValue(char, i);\n          } else if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n            processAfterArrayValue(char, i);\n          }\n        } else {\n          lastValidIndex = i;\n        }\n        break;\n      }\n    }\n  }\n  let result = input.slice(0, lastValidIndex + 1);\n  for (let i = stack.length - 1; i >= 0; i--) {\n    const state = stack[i];\n    switch (state) {\n      case \"INSIDE_STRING\": {\n        result += '\"';\n        break;\n      }\n      case \"INSIDE_OBJECT_KEY\":\n      case \"INSIDE_OBJECT_AFTER_KEY\":\n      case \"INSIDE_OBJECT_AFTER_COMMA\":\n      case \"INSIDE_OBJECT_START\":\n      case \"INSIDE_OBJECT_BEFORE_VALUE\":\n      case \"INSIDE_OBJECT_AFTER_VALUE\": {\n        result += \"}\";\n        break;\n      }\n      case \"INSIDE_ARRAY_START\":\n      case \"INSIDE_ARRAY_AFTER_COMMA\":\n      case \"INSIDE_ARRAY_AFTER_VALUE\": {\n        result += \"]\";\n        break;\n      }\n      case \"INSIDE_LITERAL\": {\n        const partialLiteral = input.substring(literalStart, input.length);\n        if (\"true\".startsWith(partialLiteral)) {\n          result += \"true\".slice(partialLiteral.length);\n        } else if (\"false\".startsWith(partialLiteral)) {\n          result += \"false\".slice(partialLiteral.length);\n        } else if (\"null\".startsWith(partialLiteral)) {\n          result += \"null\".slice(partialLiteral.length);\n        }\n      }\n    }\n  }\n  return result;\n}\n\n// src/util/parse-partial-json.ts\nasync function parsePartialJson(jsonText) {\n  if (jsonText === void 0) {\n    return { value: void 0, state: \"undefined-input\" };\n  }\n  let result = await safeParseJSON2({ text: jsonText });\n  if (result.success) {\n    return { value: result.value, state: \"successful-parse\" };\n  }\n  result = await safeParseJSON2({ text: fixJson(jsonText) });\n  if (result.success) {\n    return { value: result.value, state: \"repaired-parse\" };\n  }\n  return { value: void 0, state: \"failed-parse\" };\n}\n\n// src/ui/ui-messages.ts\nfunction isToolUIPart(part) {\n  return part.type.startsWith(\"tool-\");\n}\nfunction isDynamicToolUIPart(part) {\n  return part.type === \"dynamic-tool\";\n}\nfunction isToolOrDynamicToolUIPart(part) {\n  return isToolUIPart(part) || isDynamicToolUIPart(part);\n}\nfunction getToolName(part) {\n  return part.type.split(\"-\").slice(1).join(\"-\");\n}\nfunction getToolOrDynamicToolName(part) {\n  return isDynamicToolUIPart(part) ? part.toolName : getToolName(part);\n}\n\n// src/ui/process-ui-message-stream.ts\nfunction createStreamingUIMessageState({\n  lastMessage,\n  messageId\n}) {\n  return {\n    message: (lastMessage == null ? void 0 : lastMessage.role) === \"assistant\" ? lastMessage : {\n      id: messageId,\n      metadata: void 0,\n      role: \"assistant\",\n      parts: []\n    },\n    activeTextParts: {},\n    activeReasoningParts: {},\n    partialToolCalls: {}\n  };\n}\nfunction processUIMessageStream({\n  stream,\n  messageMetadataSchema,\n  dataPartSchemas,\n  runUpdateMessageJob,\n  onError,\n  onToolCall,\n  onData\n}) {\n  return stream.pipeThrough(\n    new TransformStream({\n      async transform(chunk, controller) {\n        await runUpdateMessageJob(async ({ state, write }) => {\n          var _a17, _b, _c, _d;\n          function getToolInvocation(toolCallId) {\n            const toolInvocations = state.message.parts.filter(isToolUIPart);\n            const toolInvocation = toolInvocations.find(\n              (invocation) => invocation.toolCallId === toolCallId\n            );\n            if (toolInvocation == null) {\n              throw new Error(\n                \"tool-output-error must be preceded by a tool-input-available\"\n              );\n            }\n            return toolInvocation;\n          }\n          function getDynamicToolInvocation(toolCallId) {\n            const toolInvocations = state.message.parts.filter(\n              (part) => part.type === \"dynamic-tool\"\n            );\n            const toolInvocation = toolInvocations.find(\n              (invocation) => invocation.toolCallId === toolCallId\n            );\n            if (toolInvocation == null) {\n              throw new Error(\n                \"tool-output-error must be preceded by a tool-input-available\"\n              );\n            }\n            return toolInvocation;\n          }\n          function updateToolPart(options) {\n            var _a18;\n            const part = state.message.parts.find(\n              (part2) => isToolUIPart(part2) && part2.toolCallId === options.toolCallId\n            );\n            const anyOptions = options;\n            const anyPart = part;\n            if (part != null) {\n              part.state = options.state;\n              anyPart.input = anyOptions.input;\n              anyPart.output = anyOptions.output;\n              anyPart.errorText = anyOptions.errorText;\n              anyPart.rawInput = anyOptions.rawInput;\n              anyPart.preliminary = anyOptions.preliminary;\n              anyPart.providerExecuted = (_a18 = anyOptions.providerExecuted) != null ? _a18 : part.providerExecuted;\n              if (anyOptions.providerMetadata != null && part.state === \"input-available\") {\n                part.callProviderMetadata = anyOptions.providerMetadata;\n              }\n            } else {\n              state.message.parts.push({\n                type: `tool-${options.toolName}`,\n                toolCallId: options.toolCallId,\n                state: options.state,\n                input: anyOptions.input,\n                output: anyOptions.output,\n                rawInput: anyOptions.rawInput,\n                errorText: anyOptions.errorText,\n                providerExecuted: anyOptions.providerExecuted,\n                preliminary: anyOptions.preliminary,\n                ...anyOptions.providerMetadata != null ? { callProviderMetadata: anyOptions.providerMetadata } : {}\n              });\n            }\n          }\n          function updateDynamicToolPart(options) {\n            var _a18;\n            const part = state.message.parts.find(\n              (part2) => part2.type === \"dynamic-tool\" && part2.toolCallId === options.toolCallId\n            );\n            const anyOptions = options;\n            const anyPart = part;\n            if (part != null) {\n              part.state = options.state;\n              anyPart.toolName = options.toolName;\n              anyPart.input = anyOptions.input;\n              anyPart.output = anyOptions.output;\n              anyPart.errorText = anyOptions.errorText;\n              anyPart.rawInput = (_a18 = anyOptions.rawInput) != null ? _a18 : anyPart.rawInput;\n              anyPart.preliminary = anyOptions.preliminary;\n              if (anyOptions.providerMetadata != null && part.state === \"input-available\") {\n                part.callProviderMetadata = anyOptions.providerMetadata;\n              }\n            } else {\n              state.message.parts.push({\n                type: \"dynamic-tool\",\n                toolName: options.toolName,\n                toolCallId: options.toolCallId,\n                state: options.state,\n                input: anyOptions.input,\n                output: anyOptions.output,\n                errorText: anyOptions.errorText,\n                preliminary: anyOptions.preliminary,\n                ...anyOptions.providerMetadata != null ? { callProviderMetadata: anyOptions.providerMetadata } : {}\n              });\n            }\n          }\n          async function updateMessageMetadata(metadata) {\n            if (metadata != null) {\n              const mergedMetadata = state.message.metadata != null ? mergeObjects(state.message.metadata, metadata) : metadata;\n              if (messageMetadataSchema != null) {\n                await validateTypes({\n                  value: mergedMetadata,\n                  schema: messageMetadataSchema\n                });\n              }\n              state.message.metadata = mergedMetadata;\n            }\n          }\n          switch (chunk.type) {\n            case \"text-start\": {\n              const textPart = {\n                type: \"text\",\n                text: \"\",\n                providerMetadata: chunk.providerMetadata,\n                state: \"streaming\"\n              };\n              state.activeTextParts[chunk.id] = textPart;\n              state.message.parts.push(textPart);\n              write();\n              break;\n            }\n            case \"text-delta\": {\n              const textPart = state.activeTextParts[chunk.id];\n              textPart.text += chunk.delta;\n              textPart.providerMetadata = (_a17 = chunk.providerMetadata) != null ? _a17 : textPart.providerMetadata;\n              write();\n              break;\n            }\n            case \"text-end\": {\n              const textPart = state.activeTextParts[chunk.id];\n              textPart.state = \"done\";\n              textPart.providerMetadata = (_b = chunk.providerMetadata) != null ? _b : textPart.providerMetadata;\n              delete state.activeTextParts[chunk.id];\n              write();\n              break;\n            }\n            case \"reasoning-start\": {\n              const reasoningPart = {\n                type: \"reasoning\",\n                text: \"\",\n                providerMetadata: chunk.providerMetadata,\n                state: \"streaming\"\n              };\n              state.activeReasoningParts[chunk.id] = reasoningPart;\n              state.message.parts.push(reasoningPart);\n              write();\n              break;\n            }\n            case \"reasoning-delta\": {\n              const reasoningPart = state.activeReasoningParts[chunk.id];\n              reasoningPart.text += chunk.delta;\n              reasoningPart.providerMetadata = (_c = chunk.providerMetadata) != null ? _c : reasoningPart.providerMetadata;\n              write();\n              break;\n            }\n            case \"reasoning-end\": {\n              const reasoningPart = state.activeReasoningParts[chunk.id];\n              reasoningPart.providerMetadata = (_d = chunk.providerMetadata) != null ? _d : reasoningPart.providerMetadata;\n              reasoningPart.state = \"done\";\n              delete state.activeReasoningParts[chunk.id];\n              write();\n              break;\n            }\n            case \"file\": {\n              state.message.parts.push({\n                type: \"file\",\n                mediaType: chunk.mediaType,\n                url: chunk.url\n              });\n              write();\n              break;\n            }\n            case \"source-url\": {\n              state.message.parts.push({\n                type: \"source-url\",\n                sourceId: chunk.sourceId,\n                url: chunk.url,\n                title: chunk.title,\n                providerMetadata: chunk.providerMetadata\n              });\n              write();\n              break;\n            }\n            case \"source-document\": {\n              state.message.parts.push({\n                type: \"source-document\",\n                sourceId: chunk.sourceId,\n                mediaType: chunk.mediaType,\n                title: chunk.title,\n                filename: chunk.filename,\n                providerMetadata: chunk.providerMetadata\n              });\n              write();\n              break;\n            }\n            case \"tool-input-start\": {\n              const toolInvocations = state.message.parts.filter(isToolUIPart);\n              state.partialToolCalls[chunk.toolCallId] = {\n                text: \"\",\n                toolName: chunk.toolName,\n                index: toolInvocations.length,\n                dynamic: chunk.dynamic\n              };\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"input-streaming\",\n                  input: void 0\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"input-streaming\",\n                  input: void 0,\n                  providerExecuted: chunk.providerExecuted\n                });\n              }\n              write();\n              break;\n            }\n            case \"tool-input-delta\": {\n              const partialToolCall = state.partialToolCalls[chunk.toolCallId];\n              partialToolCall.text += chunk.inputTextDelta;\n              const { value: partialArgs } = await parsePartialJson(\n                partialToolCall.text\n              );\n              if (partialToolCall.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: partialToolCall.toolName,\n                  state: \"input-streaming\",\n                  input: partialArgs\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: partialToolCall.toolName,\n                  state: \"input-streaming\",\n                  input: partialArgs\n                });\n              }\n              write();\n              break;\n            }\n            case \"tool-input-available\": {\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"input-available\",\n                  input: chunk.input,\n                  providerMetadata: chunk.providerMetadata\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"input-available\",\n                  input: chunk.input,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata\n                });\n              }\n              write();\n              if (onToolCall && !chunk.providerExecuted) {\n                await onToolCall({\n                  toolCall: chunk\n                });\n              }\n              break;\n            }\n            case \"tool-input-error\": {\n              if (chunk.dynamic) {\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"output-error\",\n                  input: chunk.input,\n                  errorText: chunk.errorText,\n                  providerMetadata: chunk.providerMetadata\n                });\n              } else {\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: chunk.toolName,\n                  state: \"output-error\",\n                  input: void 0,\n                  rawInput: chunk.input,\n                  errorText: chunk.errorText,\n                  providerExecuted: chunk.providerExecuted,\n                  providerMetadata: chunk.providerMetadata\n                });\n              }\n              write();\n              break;\n            }\n            case \"tool-output-available\": {\n              if (chunk.dynamic) {\n                const toolInvocation = getDynamicToolInvocation(\n                  chunk.toolCallId\n                );\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: toolInvocation.toolName,\n                  state: \"output-available\",\n                  input: toolInvocation.input,\n                  output: chunk.output,\n                  preliminary: chunk.preliminary\n                });\n              } else {\n                const toolInvocation = getToolInvocation(chunk.toolCallId);\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: getToolName(toolInvocation),\n                  state: \"output-available\",\n                  input: toolInvocation.input,\n                  output: chunk.output,\n                  providerExecuted: chunk.providerExecuted,\n                  preliminary: chunk.preliminary\n                });\n              }\n              write();\n              break;\n            }\n            case \"tool-output-error\": {\n              if (chunk.dynamic) {\n                const toolInvocation = getDynamicToolInvocation(\n                  chunk.toolCallId\n                );\n                updateDynamicToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: toolInvocation.toolName,\n                  state: \"output-error\",\n                  input: toolInvocation.input,\n                  errorText: chunk.errorText\n                });\n              } else {\n                const toolInvocation = getToolInvocation(chunk.toolCallId);\n                updateToolPart({\n                  toolCallId: chunk.toolCallId,\n                  toolName: getToolName(toolInvocation),\n                  state: \"output-error\",\n                  input: toolInvocation.input,\n                  rawInput: toolInvocation.rawInput,\n                  errorText: chunk.errorText\n                });\n              }\n              write();\n              break;\n            }\n            case \"start-step\": {\n              state.message.parts.push({ type: \"step-start\" });\n              break;\n            }\n            case \"finish-step\": {\n              state.activeTextParts = {};\n              state.activeReasoningParts = {};\n              break;\n            }\n            case \"start\": {\n              if (chunk.messageId != null) {\n                state.message.id = chunk.messageId;\n              }\n              await updateMessageMetadata(chunk.messageMetadata);\n              if (chunk.messageId != null || chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n            case \"finish\": {\n              await updateMessageMetadata(chunk.messageMetadata);\n              if (chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n            case \"message-metadata\": {\n              await updateMessageMetadata(chunk.messageMetadata);\n              if (chunk.messageMetadata != null) {\n                write();\n              }\n              break;\n            }\n            case \"error\": {\n              onError == null ? void 0 : onError(new Error(chunk.errorText));\n              break;\n            }\n            default: {\n              if (isDataUIMessageChunk(chunk)) {\n                if ((dataPartSchemas == null ? void 0 : dataPartSchemas[chunk.type]) != null) {\n                  await validateTypes({\n                    value: chunk.data,\n                    schema: dataPartSchemas[chunk.type]\n                  });\n                }\n                const dataChunk = chunk;\n                if (dataChunk.transient) {\n                  onData == null ? void 0 : onData(dataChunk);\n                  break;\n                }\n                const existingUIPart = dataChunk.id != null ? state.message.parts.find(\n                  (chunkArg) => dataChunk.type === chunkArg.type && dataChunk.id === chunkArg.id\n                ) : void 0;\n                if (existingUIPart != null) {\n                  existingUIPart.data = dataChunk.data;\n                } else {\n                  state.message.parts.push(dataChunk);\n                }\n                onData == null ? void 0 : onData(dataChunk);\n                write();\n              }\n            }\n          }\n          controller.enqueue(chunk);\n        });\n      }\n    })\n  );\n}\n\n// src/ui-message-stream/handle-ui-message-stream-finish.ts\nfunction handleUIMessageStreamFinish({\n  messageId,\n  originalMessages = [],\n  onFinish,\n  onError,\n  stream\n}) {\n  let lastMessage = originalMessages == null ? void 0 : originalMessages[originalMessages.length - 1];\n  if ((lastMessage == null ? void 0 : lastMessage.role) !== \"assistant\") {\n    lastMessage = void 0;\n  } else {\n    messageId = lastMessage.id;\n  }\n  let isAborted = false;\n  const idInjectedStream = stream.pipeThrough(\n    new TransformStream({\n      transform(chunk, controller) {\n        if (chunk.type === \"start\") {\n          const startChunk = chunk;\n          if (startChunk.messageId == null && messageId != null) {\n            startChunk.messageId = messageId;\n          }\n        }\n        if (chunk.type === \"abort\") {\n          isAborted = true;\n        }\n        controller.enqueue(chunk);\n      }\n    })\n  );\n  if (onFinish == null) {\n    return idInjectedStream;\n  }\n  const state = createStreamingUIMessageState({\n    lastMessage: lastMessage ? structuredClone(lastMessage) : void 0,\n    messageId: messageId != null ? messageId : \"\"\n    // will be overridden by the stream\n  });\n  const runUpdateMessageJob = async (job) => {\n    await job({ state, write: () => {\n    } });\n  };\n  let finishCalled = false;\n  const callOnFinish = async () => {\n    if (finishCalled || !onFinish) {\n      return;\n    }\n    finishCalled = true;\n    const isContinuation = state.message.id === (lastMessage == null ? void 0 : lastMessage.id);\n    await onFinish({\n      isAborted,\n      isContinuation,\n      responseMessage: state.message,\n      messages: [\n        ...isContinuation ? originalMessages.slice(0, -1) : originalMessages,\n        state.message\n      ]\n    });\n  };\n  return processUIMessageStream({\n    stream: idInjectedStream,\n    runUpdateMessageJob,\n    onError\n  }).pipeThrough(\n    new TransformStream({\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n      },\n      // @ts-expect-error cancel is still new and missing from types https://developer.mozilla.org/en-US/docs/Web/API/TransformStream#browser_compatibility\n      async cancel() {\n        await callOnFinish();\n      },\n      async flush() {\n        await callOnFinish();\n      }\n    })\n  );\n}\n\n// src/ui-message-stream/pipe-ui-message-stream-to-response.ts\nfunction pipeUIMessageStreamToResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  stream,\n  consumeSseStream\n}) {\n  let sseStream = stream.pipeThrough(new JsonToSseTransformStream());\n  if (consumeSseStream) {\n    const [stream1, stream2] = sseStream.tee();\n    sseStream = stream1;\n    consumeSseStream({ stream: stream2 });\n  }\n  writeToServerResponse({\n    response,\n    status,\n    statusText,\n    headers: Object.fromEntries(\n      prepareHeaders(headers, UI_MESSAGE_STREAM_HEADERS).entries()\n    ),\n    stream: sseStream.pipeThrough(new TextEncoderStream())\n  });\n}\n\n// src/util/async-iterable-stream.ts\nfunction createAsyncIterableStream(source) {\n  const stream = source.pipeThrough(new TransformStream());\n  stream[Symbol.asyncIterator] = function() {\n    const reader = this.getReader();\n    let finished = false;\n    async function cleanup(cancelStream) {\n      var _a17;\n      finished = true;\n      try {\n        if (cancelStream) {\n          await ((_a17 = reader.cancel) == null ? void 0 : _a17.call(reader));\n        }\n      } finally {\n        try {\n          reader.releaseLock();\n        } catch (e) {\n        }\n      }\n    }\n    return {\n      /**\n       * Reads the next chunk from the stream.\n       * @returns A promise resolving to the next IteratorResult.\n       */\n      async next() {\n        if (finished) {\n          return { done: true, value: void 0 };\n        }\n        const { done, value } = await reader.read();\n        if (done) {\n          await cleanup(true);\n          return { done: true, value: void 0 };\n        }\n        return { done: false, value };\n      },\n      /**\n       * Called on early exit (e.g., break from for-await).\n       * Ensures the stream is cancelled and resources are released.\n       * @returns A promise resolving to a completed IteratorResult.\n       */\n      async return() {\n        await cleanup(true);\n        return { done: true, value: void 0 };\n      },\n      /**\n       * Called on early exit with error.\n       * Ensures the stream is cancelled and resources are released, then rethrows the error.\n       * @param err The error to throw.\n       * @returns A promise that rejects with the provided error.\n       */\n      async throw(err) {\n        await cleanup(true);\n        throw err;\n      }\n    };\n  };\n  return stream;\n}\n\n// src/util/consume-stream.ts\nasync function consumeStream({\n  stream,\n  onError\n}) {\n  const reader = stream.getReader();\n  try {\n    while (true) {\n      const { done } = await reader.read();\n      if (done)\n        break;\n    }\n  } catch (error) {\n    onError == null ? void 0 : onError(error);\n  } finally {\n    reader.releaseLock();\n  }\n}\n\n// src/util/create-resolvable-promise.ts\nfunction createResolvablePromise() {\n  let resolve2;\n  let reject;\n  const promise = new Promise((res, rej) => {\n    resolve2 = res;\n    reject = rej;\n  });\n  return {\n    promise,\n    resolve: resolve2,\n    reject\n  };\n}\n\n// src/util/create-stitchable-stream.ts\nfunction createStitchableStream() {\n  let innerStreamReaders = [];\n  let controller = null;\n  let isClosed = false;\n  let waitForNewStream = createResolvablePromise();\n  const terminate = () => {\n    isClosed = true;\n    waitForNewStream.resolve();\n    innerStreamReaders.forEach((reader) => reader.cancel());\n    innerStreamReaders = [];\n    controller == null ? void 0 : controller.close();\n  };\n  const processPull = async () => {\n    if (isClosed && innerStreamReaders.length === 0) {\n      controller == null ? void 0 : controller.close();\n      return;\n    }\n    if (innerStreamReaders.length === 0) {\n      waitForNewStream = createResolvablePromise();\n      await waitForNewStream.promise;\n      return processPull();\n    }\n    try {\n      const { value, done } = await innerStreamReaders[0].read();\n      if (done) {\n        innerStreamReaders.shift();\n        if (innerStreamReaders.length > 0) {\n          await processPull();\n        } else if (isClosed) {\n          controller == null ? void 0 : controller.close();\n        }\n      } else {\n        controller == null ? void 0 : controller.enqueue(value);\n      }\n    } catch (error) {\n      controller == null ? void 0 : controller.error(error);\n      innerStreamReaders.shift();\n      terminate();\n    }\n  };\n  return {\n    stream: new ReadableStream({\n      start(controllerParam) {\n        controller = controllerParam;\n      },\n      pull: processPull,\n      async cancel() {\n        for (const reader of innerStreamReaders) {\n          await reader.cancel();\n        }\n        innerStreamReaders = [];\n        isClosed = true;\n      }\n    }),\n    addStream: (innerStream) => {\n      if (isClosed) {\n        throw new Error(\"Cannot add inner stream: outer stream is closed\");\n      }\n      innerStreamReaders.push(innerStream.getReader());\n      waitForNewStream.resolve();\n    },\n    /**\n     * Gracefully close the outer stream. This will let the inner streams\n     * finish processing and then close the outer stream.\n     */\n    close: () => {\n      isClosed = true;\n      waitForNewStream.resolve();\n      if (innerStreamReaders.length === 0) {\n        controller == null ? void 0 : controller.close();\n      }\n    },\n    /**\n     * Immediately close the outer stream. This will cancel all inner streams\n     * and close the outer stream.\n     */\n    terminate\n  };\n}\n\n// src/util/delayed-promise.ts\nvar DelayedPromise = class {\n  constructor() {\n    this.status = { type: \"pending\" };\n    this._resolve = void 0;\n    this._reject = void 0;\n  }\n  get promise() {\n    if (this._promise) {\n      return this._promise;\n    }\n    this._promise = new Promise((resolve2, reject) => {\n      if (this.status.type === \"resolved\") {\n        resolve2(this.status.value);\n      } else if (this.status.type === \"rejected\") {\n        reject(this.status.error);\n      }\n      this._resolve = resolve2;\n      this._reject = reject;\n    });\n    return this._promise;\n  }\n  resolve(value) {\n    var _a17;\n    this.status = { type: \"resolved\", value };\n    if (this._promise) {\n      (_a17 = this._resolve) == null ? void 0 : _a17.call(this, value);\n    }\n  }\n  reject(error) {\n    var _a17;\n    this.status = { type: \"rejected\", error };\n    if (this._promise) {\n      (_a17 = this._reject) == null ? void 0 : _a17.call(this, error);\n    }\n  }\n};\n\n// src/util/now.ts\nfunction now() {\n  var _a17, _b;\n  return (_b = (_a17 = globalThis == null ? void 0 : globalThis.performance) == null ? void 0 : _a17.now()) != null ? _b : Date.now();\n}\n\n// src/generate-text/run-tools-transformation.ts\nimport {\n  executeTool as executeTool2,\n  generateId,\n  getErrorMessage as getErrorMessage6\n} from \"@ai-sdk/provider-utils\";\nfunction runToolsTransformation({\n  tools,\n  generatorStream,\n  tracer,\n  telemetry,\n  system,\n  messages,\n  abortSignal,\n  repairToolCall,\n  experimental_context\n}) {\n  let toolResultsStreamController = null;\n  const toolResultsStream = new ReadableStream({\n    start(controller) {\n      toolResultsStreamController = controller;\n    }\n  });\n  const outstandingToolResults = /* @__PURE__ */ new Set();\n  const toolInputs = /* @__PURE__ */ new Map();\n  let canClose = false;\n  let finishChunk = void 0;\n  function attemptClose() {\n    if (canClose && outstandingToolResults.size === 0) {\n      if (finishChunk != null) {\n        toolResultsStreamController.enqueue(finishChunk);\n      }\n      toolResultsStreamController.close();\n    }\n  }\n  const forwardStream = new TransformStream({\n    async transform(chunk, controller) {\n      const chunkType = chunk.type;\n      switch (chunkType) {\n        case \"stream-start\":\n        case \"text-start\":\n        case \"text-delta\":\n        case \"text-end\":\n        case \"reasoning-start\":\n        case \"reasoning-delta\":\n        case \"reasoning-end\":\n        case \"tool-input-start\":\n        case \"tool-input-delta\":\n        case \"tool-input-end\":\n        case \"source\":\n        case \"response-metadata\":\n        case \"error\":\n        case \"raw\": {\n          controller.enqueue(chunk);\n          break;\n        }\n        case \"file\": {\n          controller.enqueue({\n            type: \"file\",\n            file: new DefaultGeneratedFileWithType({\n              data: chunk.data,\n              mediaType: chunk.mediaType\n            })\n          });\n          break;\n        }\n        case \"finish\": {\n          finishChunk = {\n            type: \"finish\",\n            finishReason: chunk.finishReason,\n            usage: chunk.usage,\n            providerMetadata: chunk.providerMetadata\n          };\n          break;\n        }\n        case \"tool-call\": {\n          try {\n            const toolCall = await parseToolCall({\n              toolCall: chunk,\n              tools,\n              repairToolCall,\n              system,\n              messages\n            });\n            controller.enqueue(toolCall);\n            if (toolCall.invalid) {\n              toolResultsStreamController.enqueue({\n                type: \"tool-error\",\n                toolCallId: toolCall.toolCallId,\n                toolName: toolCall.toolName,\n                input: toolCall.input,\n                error: getErrorMessage6(toolCall.error),\n                dynamic: true\n              });\n              break;\n            }\n            const tool3 = tools[toolCall.toolName];\n            toolInputs.set(toolCall.toolCallId, toolCall.input);\n            if (tool3.onInputAvailable != null) {\n              await tool3.onInputAvailable({\n                input: toolCall.input,\n                toolCallId: toolCall.toolCallId,\n                messages,\n                abortSignal,\n                experimental_context\n              });\n            }\n            if (tool3.execute != null && toolCall.providerExecuted !== true) {\n              const toolExecutionId = generateId();\n              outstandingToolResults.add(toolExecutionId);\n              recordSpan({\n                name: \"ai.toolCall\",\n                attributes: selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    ...assembleOperationName({\n                      operationId: \"ai.toolCall\",\n                      telemetry\n                    }),\n                    \"ai.toolCall.name\": toolCall.toolName,\n                    \"ai.toolCall.id\": toolCall.toolCallId,\n                    \"ai.toolCall.args\": {\n                      output: () => JSON.stringify(toolCall.input)\n                    }\n                  }\n                }),\n                tracer,\n                fn: async (span) => {\n                  let output;\n                  try {\n                    const stream = executeTool2({\n                      execute: tool3.execute.bind(tool3),\n                      input: toolCall.input,\n                      options: {\n                        toolCallId: toolCall.toolCallId,\n                        messages,\n                        abortSignal,\n                        experimental_context\n                      }\n                    });\n                    for await (const part of stream) {\n                      toolResultsStreamController.enqueue({\n                        ...toolCall,\n                        type: \"tool-result\",\n                        output: part.output,\n                        ...part.type === \"preliminary\" && {\n                          preliminary: true\n                        }\n                      });\n                      if (part.type === \"final\") {\n                        output = part.output;\n                      }\n                    }\n                  } catch (error) {\n                    recordErrorOnSpan(span, error);\n                    toolResultsStreamController.enqueue({\n                      ...toolCall,\n                      type: \"tool-error\",\n                      error\n                    });\n                    outstandingToolResults.delete(toolExecutionId);\n                    attemptClose();\n                    return;\n                  }\n                  outstandingToolResults.delete(toolExecutionId);\n                  attemptClose();\n                  try {\n                    span.setAttributes(\n                      selectTelemetryAttributes({\n                        telemetry,\n                        attributes: {\n                          \"ai.toolCall.result\": {\n                            output: () => JSON.stringify(output)\n                          }\n                        }\n                      })\n                    );\n                  } catch (ignored) {\n                  }\n                }\n              });\n            }\n          } catch (error) {\n            toolResultsStreamController.enqueue({ type: \"error\", error });\n          }\n          break;\n        }\n        case \"tool-result\": {\n          const toolName = chunk.toolName;\n          if (chunk.isError) {\n            toolResultsStreamController.enqueue({\n              type: \"tool-error\",\n              toolCallId: chunk.toolCallId,\n              toolName,\n              input: toolInputs.get(chunk.toolCallId),\n              providerExecuted: chunk.providerExecuted,\n              error: chunk.result\n            });\n          } else {\n            controller.enqueue({\n              type: \"tool-result\",\n              toolCallId: chunk.toolCallId,\n              toolName,\n              input: toolInputs.get(chunk.toolCallId),\n              output: chunk.result,\n              providerExecuted: chunk.providerExecuted\n            });\n          }\n          break;\n        }\n        default: {\n          const _exhaustiveCheck = chunkType;\n          throw new Error(`Unhandled chunk type: ${_exhaustiveCheck}`);\n        }\n      }\n    },\n    flush() {\n      canClose = true;\n      attemptClose();\n    }\n  });\n  return new ReadableStream({\n    async start(controller) {\n      return Promise.all([\n        generatorStream.pipeThrough(forwardStream).pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n            }\n          })\n        ),\n        toolResultsStream.pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              controller.close();\n            }\n          })\n        )\n      ]);\n    }\n  });\n}\n\n// src/generate-text/stream-text.ts\nvar originalGenerateId2 = createIdGenerator2({\n  prefix: \"aitxt\",\n  size: 24\n});\nfunction streamText({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  stopWhen = stepCountIs(1),\n  experimental_output: output,\n  experimental_telemetry: telemetry,\n  prepareStep,\n  providerOptions,\n  experimental_activeTools,\n  activeTools = experimental_activeTools,\n  experimental_repairToolCall: repairToolCall,\n  experimental_transform: transform,\n  experimental_download: download2,\n  includeRawChunks = false,\n  onChunk,\n  onError = ({ error }) => {\n    console.error(error);\n  },\n  onFinish,\n  onAbort,\n  onStepFinish,\n  experimental_context,\n  _internal: {\n    now: now2 = now,\n    generateId: generateId3 = originalGenerateId2,\n    currentDate = () => /* @__PURE__ */ new Date()\n  } = {},\n  ...settings\n}) {\n  return new DefaultStreamTextResult({\n    model: resolveLanguageModel(model),\n    telemetry,\n    headers,\n    settings,\n    maxRetries,\n    abortSignal,\n    system,\n    prompt,\n    messages,\n    tools,\n    toolChoice,\n    transforms: asArray(transform),\n    activeTools,\n    repairToolCall,\n    stopConditions: asArray(stopWhen),\n    output,\n    providerOptions,\n    prepareStep,\n    includeRawChunks,\n    onChunk,\n    onError,\n    onFinish,\n    onAbort,\n    onStepFinish,\n    now: now2,\n    currentDate,\n    generateId: generateId3,\n    experimental_context,\n    download: download2\n  });\n}\nfunction createOutputTransformStream(output) {\n  if (!output) {\n    return new TransformStream({\n      transform(chunk, controller) {\n        controller.enqueue({ part: chunk, partialOutput: void 0 });\n      }\n    });\n  }\n  let firstTextChunkId = void 0;\n  let text2 = \"\";\n  let textChunk = \"\";\n  let lastPublishedJson = \"\";\n  function publishTextChunk({\n    controller,\n    partialOutput = void 0\n  }) {\n    controller.enqueue({\n      part: {\n        type: \"text-delta\",\n        id: firstTextChunkId,\n        text: textChunk\n      },\n      partialOutput\n    });\n    textChunk = \"\";\n  }\n  return new TransformStream({\n    async transform(chunk, controller) {\n      if (chunk.type === \"finish-step\" && textChunk.length > 0) {\n        publishTextChunk({ controller });\n      }\n      if (chunk.type !== \"text-delta\" && chunk.type !== \"text-start\" && chunk.type !== \"text-end\") {\n        controller.enqueue({ part: chunk, partialOutput: void 0 });\n        return;\n      }\n      if (firstTextChunkId == null) {\n        firstTextChunkId = chunk.id;\n      } else if (chunk.id !== firstTextChunkId) {\n        controller.enqueue({ part: chunk, partialOutput: void 0 });\n        return;\n      }\n      if (chunk.type === \"text-start\") {\n        controller.enqueue({ part: chunk, partialOutput: void 0 });\n        return;\n      }\n      if (chunk.type === \"text-end\") {\n        if (textChunk.length > 0) {\n          publishTextChunk({ controller });\n        }\n        controller.enqueue({ part: chunk, partialOutput: void 0 });\n        return;\n      }\n      text2 += chunk.text;\n      textChunk += chunk.text;\n      const result = await output.parsePartial({ text: text2 });\n      if (result != null) {\n        const currentJson = JSON.stringify(result.partial);\n        if (currentJson !== lastPublishedJson) {\n          publishTextChunk({ controller, partialOutput: result.partial });\n          lastPublishedJson = currentJson;\n        }\n      }\n    }\n  });\n}\nvar DefaultStreamTextResult = class {\n  constructor({\n    model,\n    telemetry,\n    headers,\n    settings,\n    maxRetries: maxRetriesArg,\n    abortSignal,\n    system,\n    prompt,\n    messages,\n    tools,\n    toolChoice,\n    transforms,\n    activeTools,\n    repairToolCall,\n    stopConditions,\n    output,\n    providerOptions,\n    prepareStep,\n    includeRawChunks,\n    now: now2,\n    currentDate,\n    generateId: generateId3,\n    onChunk,\n    onError,\n    onFinish,\n    onAbort,\n    onStepFinish,\n    experimental_context,\n    download: download2\n  }) {\n    this._totalUsage = new DelayedPromise();\n    this._finishReason = new DelayedPromise();\n    this._steps = new DelayedPromise();\n    this.output = output;\n    this.includeRawChunks = includeRawChunks;\n    this.tools = tools;\n    let stepFinish;\n    let recordedContent = [];\n    const recordedResponseMessages = [];\n    let recordedFinishReason = void 0;\n    let recordedTotalUsage = void 0;\n    let recordedRequest = {};\n    let recordedWarnings = [];\n    const recordedSteps = [];\n    let rootSpan;\n    let activeTextContent = {};\n    let activeReasoningContent = {};\n    const eventProcessor = new TransformStream({\n      async transform(chunk, controller) {\n        var _a17, _b, _c;\n        controller.enqueue(chunk);\n        const { part } = chunk;\n        if (part.type === \"text-delta\" || part.type === \"reasoning-delta\" || part.type === \"source\" || part.type === \"tool-call\" || part.type === \"tool-result\" || part.type === \"tool-input-start\" || part.type === \"tool-input-delta\" || part.type === \"raw\") {\n          await (onChunk == null ? void 0 : onChunk({ chunk: part }));\n        }\n        if (part.type === \"error\") {\n          await onError({ error: wrapGatewayError(part.error) });\n        }\n        if (part.type === \"text-start\") {\n          activeTextContent[part.id] = {\n            type: \"text\",\n            text: \"\",\n            providerMetadata: part.providerMetadata\n          };\n          recordedContent.push(activeTextContent[part.id]);\n        }\n        if (part.type === \"text-delta\") {\n          const activeText = activeTextContent[part.id];\n          if (activeText == null) {\n            controller.enqueue({\n              part: {\n                type: \"error\",\n                error: `text part ${part.id} not found`\n              },\n              partialOutput: void 0\n            });\n            return;\n          }\n          activeText.text += part.text;\n          activeText.providerMetadata = (_a17 = part.providerMetadata) != null ? _a17 : activeText.providerMetadata;\n        }\n        if (part.type === \"text-end\") {\n          delete activeTextContent[part.id];\n        }\n        if (part.type === \"reasoning-start\") {\n          activeReasoningContent[part.id] = {\n            type: \"reasoning\",\n            text: \"\",\n            providerMetadata: part.providerMetadata\n          };\n          recordedContent.push(activeReasoningContent[part.id]);\n        }\n        if (part.type === \"reasoning-delta\") {\n          const activeReasoning = activeReasoningContent[part.id];\n          if (activeReasoning == null) {\n            controller.enqueue({\n              part: {\n                type: \"error\",\n                error: `reasoning part ${part.id} not found`\n              },\n              partialOutput: void 0\n            });\n            return;\n          }\n          activeReasoning.text += part.text;\n          activeReasoning.providerMetadata = (_b = part.providerMetadata) != null ? _b : activeReasoning.providerMetadata;\n        }\n        if (part.type === \"reasoning-end\") {\n          const activeReasoning = activeReasoningContent[part.id];\n          if (activeReasoning == null) {\n            controller.enqueue({\n              part: {\n                type: \"error\",\n                error: `reasoning part ${part.id} not found`\n              },\n              partialOutput: void 0\n            });\n            return;\n          }\n          activeReasoning.providerMetadata = (_c = part.providerMetadata) != null ? _c : activeReasoning.providerMetadata;\n          delete activeReasoningContent[part.id];\n        }\n        if (part.type === \"file\") {\n          recordedContent.push({ type: \"file\", file: part.file });\n        }\n        if (part.type === \"source\") {\n          recordedContent.push(part);\n        }\n        if (part.type === \"tool-call\") {\n          recordedContent.push(part);\n        }\n        if (part.type === \"tool-result\" && !part.preliminary) {\n          recordedContent.push(part);\n        }\n        if (part.type === \"tool-error\") {\n          recordedContent.push(part);\n        }\n        if (part.type === \"start-step\") {\n          recordedRequest = part.request;\n          recordedWarnings = part.warnings;\n        }\n        if (part.type === \"finish-step\") {\n          const stepMessages = toResponseMessages({\n            content: recordedContent,\n            tools\n          });\n          const currentStepResult = new DefaultStepResult({\n            content: recordedContent,\n            finishReason: part.finishReason,\n            usage: part.usage,\n            warnings: recordedWarnings,\n            request: recordedRequest,\n            response: {\n              ...part.response,\n              messages: [...recordedResponseMessages, ...stepMessages]\n            },\n            providerMetadata: part.providerMetadata\n          });\n          await (onStepFinish == null ? void 0 : onStepFinish(currentStepResult));\n          logWarnings(recordedWarnings);\n          recordedSteps.push(currentStepResult);\n          recordedContent = [];\n          activeReasoningContent = {};\n          activeTextContent = {};\n          recordedResponseMessages.push(...stepMessages);\n          stepFinish.resolve();\n        }\n        if (part.type === \"finish\") {\n          recordedTotalUsage = part.totalUsage;\n          recordedFinishReason = part.finishReason;\n        }\n      },\n      async flush(controller) {\n        try {\n          if (recordedSteps.length === 0) {\n            const error = new NoOutputGeneratedError({\n              message: \"No output generated. Check the stream for errors.\"\n            });\n            self._finishReason.reject(error);\n            self._totalUsage.reject(error);\n            self._steps.reject(error);\n            return;\n          }\n          const finishReason = recordedFinishReason != null ? recordedFinishReason : \"unknown\";\n          const totalUsage = recordedTotalUsage != null ? recordedTotalUsage : {\n            inputTokens: void 0,\n            outputTokens: void 0,\n            totalTokens: void 0\n          };\n          self._finishReason.resolve(finishReason);\n          self._totalUsage.resolve(totalUsage);\n          self._steps.resolve(recordedSteps);\n          const finalStep = recordedSteps[recordedSteps.length - 1];\n          await (onFinish == null ? void 0 : onFinish({\n            finishReason,\n            totalUsage,\n            usage: finalStep.usage,\n            content: finalStep.content,\n            text: finalStep.text,\n            reasoningText: finalStep.reasoningText,\n            reasoning: finalStep.reasoning,\n            files: finalStep.files,\n            sources: finalStep.sources,\n            toolCalls: finalStep.toolCalls,\n            staticToolCalls: finalStep.staticToolCalls,\n            dynamicToolCalls: finalStep.dynamicToolCalls,\n            toolResults: finalStep.toolResults,\n            staticToolResults: finalStep.staticToolResults,\n            dynamicToolResults: finalStep.dynamicToolResults,\n            request: finalStep.request,\n            response: finalStep.response,\n            warnings: finalStep.warnings,\n            providerMetadata: finalStep.providerMetadata,\n            steps: recordedSteps\n          }));\n          rootSpan.setAttributes(\n            selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                \"ai.response.finishReason\": finishReason,\n                \"ai.response.text\": { output: () => finalStep.text },\n                \"ai.response.toolCalls\": {\n                  output: () => {\n                    var _a17;\n                    return ((_a17 = finalStep.toolCalls) == null ? void 0 : _a17.length) ? JSON.stringify(finalStep.toolCalls) : void 0;\n                  }\n                },\n                \"ai.response.providerMetadata\": JSON.stringify(\n                  finalStep.providerMetadata\n                ),\n                \"ai.usage.inputTokens\": totalUsage.inputTokens,\n                \"ai.usage.outputTokens\": totalUsage.outputTokens,\n                \"ai.usage.totalTokens\": totalUsage.totalTokens,\n                \"ai.usage.reasoningTokens\": totalUsage.reasoningTokens,\n                \"ai.usage.cachedInputTokens\": totalUsage.cachedInputTokens\n              }\n            })\n          );\n        } catch (error) {\n          controller.error(error);\n        } finally {\n          rootSpan.end();\n        }\n      }\n    });\n    const stitchableStream = createStitchableStream();\n    this.addStream = stitchableStream.addStream;\n    this.closeStream = stitchableStream.close;\n    const reader = stitchableStream.stream.getReader();\n    let stream = new ReadableStream({\n      async start(controller) {\n        controller.enqueue({ type: \"start\" });\n      },\n      async pull(controller) {\n        function abort() {\n          onAbort == null ? void 0 : onAbort({ steps: recordedSteps });\n          controller.enqueue({ type: \"abort\" });\n          controller.close();\n        }\n        try {\n          const { done, value } = await reader.read();\n          if (done) {\n            controller.close();\n            return;\n          }\n          if (abortSignal == null ? void 0 : abortSignal.aborted) {\n            abort();\n            return;\n          }\n          controller.enqueue(value);\n        } catch (error) {\n          if (isAbortError2(error) && (abortSignal == null ? void 0 : abortSignal.aborted)) {\n            abort();\n          } else {\n            controller.error(error);\n          }\n        }\n      },\n      cancel(reason) {\n        return stitchableStream.stream.cancel(reason);\n      }\n    });\n    for (const transform of transforms) {\n      stream = stream.pipeThrough(\n        transform({\n          tools,\n          stopStream() {\n            stitchableStream.terminate();\n          }\n        })\n      );\n    }\n    this.baseStream = stream.pipeThrough(createOutputTransformStream(output)).pipeThrough(eventProcessor);\n    const { maxRetries, retry } = prepareRetries({\n      maxRetries: maxRetriesArg,\n      abortSignal\n    });\n    const tracer = getTracer(telemetry);\n    const callSettings = prepareCallSettings(settings);\n    const baseTelemetryAttributes = getBaseTelemetryAttributes({\n      model,\n      telemetry,\n      headers,\n      settings: { ...callSettings, maxRetries }\n    });\n    const self = this;\n    recordSpan({\n      name: \"ai.streamText\",\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({ operationId: \"ai.streamText\", telemetry }),\n          ...baseTelemetryAttributes,\n          // specific settings that only make sense on the outer level:\n          \"ai.prompt\": {\n            input: () => JSON.stringify({ system, prompt, messages })\n          }\n        }\n      }),\n      tracer,\n      endWhenDone: false,\n      fn: async (rootSpanArg) => {\n        rootSpan = rootSpanArg;\n        async function streamStep({\n          currentStep,\n          responseMessages,\n          usage\n        }) {\n          var _a17, _b, _c, _d, _e;\n          const includeRawChunks2 = self.includeRawChunks;\n          stepFinish = new DelayedPromise();\n          const initialPrompt = await standardizePrompt({\n            system,\n            prompt,\n            messages\n          });\n          const stepInputMessages = [\n            ...initialPrompt.messages,\n            ...responseMessages\n          ];\n          const prepareStepResult = await (prepareStep == null ? void 0 : prepareStep({\n            model,\n            steps: recordedSteps,\n            stepNumber: recordedSteps.length,\n            messages: stepInputMessages\n          }));\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: {\n              system: (_a17 = prepareStepResult == null ? void 0 : prepareStepResult.system) != null ? _a17 : initialPrompt.system,\n              messages: (_b = prepareStepResult == null ? void 0 : prepareStepResult.messages) != null ? _b : stepInputMessages\n            },\n            supportedUrls: await model.supportedUrls,\n            download: download2\n          });\n          const stepModel = resolveLanguageModel(\n            (_c = prepareStepResult == null ? void 0 : prepareStepResult.model) != null ? _c : model\n          );\n          const { toolChoice: stepToolChoice, tools: stepTools } = prepareToolsAndToolChoice({\n            tools,\n            toolChoice: (_d = prepareStepResult == null ? void 0 : prepareStepResult.toolChoice) != null ? _d : toolChoice,\n            activeTools: (_e = prepareStepResult == null ? void 0 : prepareStepResult.activeTools) != null ? _e : activeTools\n          });\n          const {\n            result: { stream: stream2, response, request },\n            doStreamSpan,\n            startTimestampMs\n          } = await retry(\n            () => recordSpan({\n              name: \"ai.streamText.doStream\",\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: \"ai.streamText.doStream\",\n                    telemetry\n                  }),\n                  ...baseTelemetryAttributes,\n                  // model:\n                  \"ai.model.provider\": stepModel.provider,\n                  \"ai.model.id\": stepModel.modelId,\n                  // prompt:\n                  \"ai.prompt.messages\": {\n                    input: () => stringifyForTelemetry(promptMessages)\n                  },\n                  \"ai.prompt.tools\": {\n                    // convert the language model level tools:\n                    input: () => stepTools == null ? void 0 : stepTools.map((tool3) => JSON.stringify(tool3))\n                  },\n                  \"ai.prompt.toolChoice\": {\n                    input: () => stepToolChoice != null ? JSON.stringify(stepToolChoice) : void 0\n                  },\n                  // standardized gen-ai llm span attributes:\n                  \"gen_ai.system\": stepModel.provider,\n                  \"gen_ai.request.model\": stepModel.modelId,\n                  \"gen_ai.request.frequency_penalty\": callSettings.frequencyPenalty,\n                  \"gen_ai.request.max_tokens\": callSettings.maxOutputTokens,\n                  \"gen_ai.request.presence_penalty\": callSettings.presencePenalty,\n                  \"gen_ai.request.stop_sequences\": callSettings.stopSequences,\n                  \"gen_ai.request.temperature\": callSettings.temperature,\n                  \"gen_ai.request.top_k\": callSettings.topK,\n                  \"gen_ai.request.top_p\": callSettings.topP\n                }\n              }),\n              tracer,\n              endWhenDone: false,\n              fn: async (doStreamSpan2) => {\n                return {\n                  startTimestampMs: now2(),\n                  // get before the call\n                  doStreamSpan: doStreamSpan2,\n                  result: await stepModel.doStream({\n                    ...callSettings,\n                    tools: stepTools,\n                    toolChoice: stepToolChoice,\n                    responseFormat: output == null ? void 0 : output.responseFormat,\n                    prompt: promptMessages,\n                    providerOptions,\n                    abortSignal,\n                    headers,\n                    includeRawChunks: includeRawChunks2\n                  })\n                };\n              }\n            })\n          );\n          const streamWithToolResults = runToolsTransformation({\n            tools,\n            generatorStream: stream2,\n            tracer,\n            telemetry,\n            system,\n            messages: stepInputMessages,\n            repairToolCall,\n            abortSignal,\n            experimental_context\n          });\n          const stepRequest = request != null ? request : {};\n          const stepToolCalls = [];\n          const stepToolOutputs = [];\n          let warnings;\n          const activeToolCallToolNames = {};\n          let stepFinishReason = \"unknown\";\n          let stepUsage = {\n            inputTokens: void 0,\n            outputTokens: void 0,\n            totalTokens: void 0\n          };\n          let stepProviderMetadata;\n          let stepFirstChunk = true;\n          let stepResponse = {\n            id: generateId3(),\n            timestamp: currentDate(),\n            modelId: model.modelId\n          };\n          let activeText = \"\";\n          self.addStream(\n            streamWithToolResults.pipeThrough(\n              new TransformStream({\n                async transform(chunk, controller) {\n                  var _a18, _b2, _c2, _d2;\n                  if (chunk.type === \"stream-start\") {\n                    warnings = chunk.warnings;\n                    return;\n                  }\n                  if (stepFirstChunk) {\n                    const msToFirstChunk = now2() - startTimestampMs;\n                    stepFirstChunk = false;\n                    doStreamSpan.addEvent(\"ai.stream.firstChunk\", {\n                      \"ai.response.msToFirstChunk\": msToFirstChunk\n                    });\n                    doStreamSpan.setAttributes({\n                      \"ai.response.msToFirstChunk\": msToFirstChunk\n                    });\n                    controller.enqueue({\n                      type: \"start-step\",\n                      request: stepRequest,\n                      warnings: warnings != null ? warnings : []\n                    });\n                  }\n                  const chunkType = chunk.type;\n                  switch (chunkType) {\n                    case \"text-start\":\n                    case \"text-end\": {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"text-delta\": {\n                      if (chunk.delta.length > 0) {\n                        controller.enqueue({\n                          type: \"text-delta\",\n                          id: chunk.id,\n                          text: chunk.delta,\n                          providerMetadata: chunk.providerMetadata\n                        });\n                        activeText += chunk.delta;\n                      }\n                      break;\n                    }\n                    case \"reasoning-start\":\n                    case \"reasoning-end\": {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"reasoning-delta\": {\n                      controller.enqueue({\n                        type: \"reasoning-delta\",\n                        id: chunk.id,\n                        text: chunk.delta,\n                        providerMetadata: chunk.providerMetadata\n                      });\n                      break;\n                    }\n                    case \"tool-call\": {\n                      controller.enqueue(chunk);\n                      stepToolCalls.push(chunk);\n                      break;\n                    }\n                    case \"tool-result\": {\n                      controller.enqueue(chunk);\n                      if (!chunk.preliminary) {\n                        stepToolOutputs.push(chunk);\n                      }\n                      break;\n                    }\n                    case \"tool-error\": {\n                      controller.enqueue(chunk);\n                      stepToolOutputs.push(chunk);\n                      break;\n                    }\n                    case \"response-metadata\": {\n                      stepResponse = {\n                        id: (_a18 = chunk.id) != null ? _a18 : stepResponse.id,\n                        timestamp: (_b2 = chunk.timestamp) != null ? _b2 : stepResponse.timestamp,\n                        modelId: (_c2 = chunk.modelId) != null ? _c2 : stepResponse.modelId\n                      };\n                      break;\n                    }\n                    case \"finish\": {\n                      stepUsage = chunk.usage;\n                      stepFinishReason = chunk.finishReason;\n                      stepProviderMetadata = chunk.providerMetadata;\n                      const msToFinish = now2() - startTimestampMs;\n                      doStreamSpan.addEvent(\"ai.stream.finish\");\n                      doStreamSpan.setAttributes({\n                        \"ai.response.msToFinish\": msToFinish,\n                        \"ai.response.avgOutputTokensPerSecond\": 1e3 * ((_d2 = stepUsage.outputTokens) != null ? _d2 : 0) / msToFinish\n                      });\n                      break;\n                    }\n                    case \"file\": {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"source\": {\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"tool-input-start\": {\n                      activeToolCallToolNames[chunk.id] = chunk.toolName;\n                      const tool3 = tools == null ? void 0 : tools[chunk.toolName];\n                      if ((tool3 == null ? void 0 : tool3.onInputStart) != null) {\n                        await tool3.onInputStart({\n                          toolCallId: chunk.id,\n                          messages: stepInputMessages,\n                          abortSignal,\n                          experimental_context\n                        });\n                      }\n                      controller.enqueue({\n                        ...chunk,\n                        dynamic: (tool3 == null ? void 0 : tool3.type) === \"dynamic\"\n                      });\n                      break;\n                    }\n                    case \"tool-input-end\": {\n                      delete activeToolCallToolNames[chunk.id];\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"tool-input-delta\": {\n                      const toolName = activeToolCallToolNames[chunk.id];\n                      const tool3 = tools == null ? void 0 : tools[toolName];\n                      if ((tool3 == null ? void 0 : tool3.onInputDelta) != null) {\n                        await tool3.onInputDelta({\n                          inputTextDelta: chunk.delta,\n                          toolCallId: chunk.id,\n                          messages: stepInputMessages,\n                          abortSignal,\n                          experimental_context\n                        });\n                      }\n                      controller.enqueue(chunk);\n                      break;\n                    }\n                    case \"error\": {\n                      controller.enqueue(chunk);\n                      stepFinishReason = \"error\";\n                      break;\n                    }\n                    case \"raw\": {\n                      if (includeRawChunks2) {\n                        controller.enqueue(chunk);\n                      }\n                      break;\n                    }\n                    default: {\n                      const exhaustiveCheck = chunkType;\n                      throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n                    }\n                  }\n                },\n                // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n                async flush(controller) {\n                  const stepToolCallsJson = stepToolCalls.length > 0 ? JSON.stringify(stepToolCalls) : void 0;\n                  try {\n                    doStreamSpan.setAttributes(\n                      selectTelemetryAttributes({\n                        telemetry,\n                        attributes: {\n                          \"ai.response.finishReason\": stepFinishReason,\n                          \"ai.response.text\": {\n                            output: () => activeText\n                          },\n                          \"ai.response.toolCalls\": {\n                            output: () => stepToolCallsJson\n                          },\n                          \"ai.response.id\": stepResponse.id,\n                          \"ai.response.model\": stepResponse.modelId,\n                          \"ai.response.timestamp\": stepResponse.timestamp.toISOString(),\n                          \"ai.response.providerMetadata\": JSON.stringify(stepProviderMetadata),\n                          \"ai.usage.inputTokens\": stepUsage.inputTokens,\n                          \"ai.usage.outputTokens\": stepUsage.outputTokens,\n                          \"ai.usage.totalTokens\": stepUsage.totalTokens,\n                          \"ai.usage.reasoningTokens\": stepUsage.reasoningTokens,\n                          \"ai.usage.cachedInputTokens\": stepUsage.cachedInputTokens,\n                          // standardized gen-ai llm span attributes:\n                          \"gen_ai.response.finish_reasons\": [stepFinishReason],\n                          \"gen_ai.response.id\": stepResponse.id,\n                          \"gen_ai.response.model\": stepResponse.modelId,\n                          \"gen_ai.usage.input_tokens\": stepUsage.inputTokens,\n                          \"gen_ai.usage.output_tokens\": stepUsage.outputTokens\n                        }\n                      })\n                    );\n                  } catch (error) {\n                  } finally {\n                    doStreamSpan.end();\n                  }\n                  controller.enqueue({\n                    type: \"finish-step\",\n                    finishReason: stepFinishReason,\n                    usage: stepUsage,\n                    providerMetadata: stepProviderMetadata,\n                    response: {\n                      ...stepResponse,\n                      headers: response == null ? void 0 : response.headers\n                    }\n                  });\n                  const combinedUsage = addLanguageModelUsage(usage, stepUsage);\n                  await stepFinish.promise;\n                  const clientToolCalls = stepToolCalls.filter(\n                    (toolCall) => toolCall.providerExecuted !== true\n                  );\n                  const clientToolOutputs = stepToolOutputs.filter(\n                    (toolOutput) => toolOutput.providerExecuted !== true\n                  );\n                  if (clientToolCalls.length > 0 && // all current tool calls have outputs (incl. execution errors):\n                  clientToolOutputs.length === clientToolCalls.length && // continue until a stop condition is met:\n                  !await isStopConditionMet({\n                    stopConditions,\n                    steps: recordedSteps\n                  })) {\n                    responseMessages.push(\n                      ...toResponseMessages({\n                        content: (\n                          // use transformed content to create the messages for the next step:\n                          recordedSteps[recordedSteps.length - 1].content\n                        ),\n                        tools\n                      })\n                    );\n                    try {\n                      await streamStep({\n                        currentStep: currentStep + 1,\n                        responseMessages,\n                        usage: combinedUsage\n                      });\n                    } catch (error) {\n                      controller.enqueue({\n                        type: \"error\",\n                        error\n                      });\n                      self.closeStream();\n                    }\n                  } else {\n                    controller.enqueue({\n                      type: \"finish\",\n                      finishReason: stepFinishReason,\n                      totalUsage: combinedUsage\n                    });\n                    self.closeStream();\n                  }\n                }\n              })\n            )\n          );\n        }\n        await streamStep({\n          currentStep: 0,\n          responseMessages: [],\n          usage: {\n            inputTokens: void 0,\n            outputTokens: void 0,\n            totalTokens: void 0\n          }\n        });\n      }\n    }).catch((error) => {\n      self.addStream(\n        new ReadableStream({\n          start(controller) {\n            controller.enqueue({ type: \"error\", error });\n            controller.close();\n          }\n        })\n      );\n      self.closeStream();\n    });\n  }\n  get steps() {\n    this.consumeStream();\n    return this._steps.promise;\n  }\n  get finalStep() {\n    return this.steps.then((steps) => steps[steps.length - 1]);\n  }\n  get content() {\n    return this.finalStep.then((step) => step.content);\n  }\n  get warnings() {\n    return this.finalStep.then((step) => step.warnings);\n  }\n  get providerMetadata() {\n    return this.finalStep.then((step) => step.providerMetadata);\n  }\n  get text() {\n    return this.finalStep.then((step) => step.text);\n  }\n  get reasoningText() {\n    return this.finalStep.then((step) => step.reasoningText);\n  }\n  get reasoning() {\n    return this.finalStep.then((step) => step.reasoning);\n  }\n  get sources() {\n    return this.finalStep.then((step) => step.sources);\n  }\n  get files() {\n    return this.finalStep.then((step) => step.files);\n  }\n  get toolCalls() {\n    return this.finalStep.then((step) => step.toolCalls);\n  }\n  get staticToolCalls() {\n    return this.finalStep.then((step) => step.staticToolCalls);\n  }\n  get dynamicToolCalls() {\n    return this.finalStep.then((step) => step.dynamicToolCalls);\n  }\n  get toolResults() {\n    return this.finalStep.then((step) => step.toolResults);\n  }\n  get staticToolResults() {\n    return this.finalStep.then((step) => step.staticToolResults);\n  }\n  get dynamicToolResults() {\n    return this.finalStep.then((step) => step.dynamicToolResults);\n  }\n  get usage() {\n    return this.finalStep.then((step) => step.usage);\n  }\n  get request() {\n    return this.finalStep.then((step) => step.request);\n  }\n  get response() {\n    return this.finalStep.then((step) => step.response);\n  }\n  get totalUsage() {\n    this.consumeStream();\n    return this._totalUsage.promise;\n  }\n  get finishReason() {\n    this.consumeStream();\n    return this._finishReason.promise;\n  }\n  /**\n  Split out a new stream from the original stream.\n  The original stream is replaced to allow for further splitting,\n  since we do not know how many times the stream will be split.\n  \n  Note: this leads to buffering the stream content on the server.\n  However, the LLM results are expected to be small enough to not cause issues.\n     */\n  teeStream() {\n    const [stream1, stream2] = this.baseStream.tee();\n    this.baseStream = stream2;\n    return stream1;\n  }\n  get textStream() {\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream({\n          transform({ part }, controller) {\n            if (part.type === \"text-delta\") {\n              controller.enqueue(part.text);\n            }\n          }\n        })\n      )\n    );\n  }\n  get fullStream() {\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream({\n          transform({ part }, controller) {\n            controller.enqueue(part);\n          }\n        })\n      )\n    );\n  }\n  async consumeStream(options) {\n    var _a17;\n    try {\n      await consumeStream({\n        stream: this.fullStream,\n        onError: options == null ? void 0 : options.onError\n      });\n    } catch (error) {\n      (_a17 = options == null ? void 0 : options.onError) == null ? void 0 : _a17.call(options, error);\n    }\n  }\n  get experimental_partialOutputStream() {\n    if (this.output == null) {\n      throw new NoOutputSpecifiedError();\n    }\n    return createAsyncIterableStream(\n      this.teeStream().pipeThrough(\n        new TransformStream({\n          transform({ partialOutput }, controller) {\n            if (partialOutput != null) {\n              controller.enqueue(partialOutput);\n            }\n          }\n        })\n      )\n    );\n  }\n  toUIMessageStream({\n    originalMessages,\n    generateMessageId,\n    onFinish,\n    messageMetadata,\n    sendReasoning = true,\n    sendSources = false,\n    sendStart = true,\n    sendFinish = true,\n    onError = getErrorMessage7\n  } = {}) {\n    const responseMessageId = generateMessageId != null ? getResponseUIMessageId({\n      originalMessages,\n      responseMessageId: generateMessageId\n    }) : void 0;\n    const toolNamesByCallId = {};\n    const isDynamic = (toolCallId) => {\n      var _a17, _b;\n      const toolName = toolNamesByCallId[toolCallId];\n      const dynamic = ((_b = (_a17 = this.tools) == null ? void 0 : _a17[toolName]) == null ? void 0 : _b.type) === \"dynamic\";\n      return dynamic ? true : void 0;\n    };\n    const baseStream = this.fullStream.pipeThrough(\n      new TransformStream({\n        transform: async (part, controller) => {\n          const messageMetadataValue = messageMetadata == null ? void 0 : messageMetadata({ part });\n          const partType = part.type;\n          switch (partType) {\n            case \"text-start\": {\n              controller.enqueue({\n                type: \"text-start\",\n                id: part.id,\n                ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n              });\n              break;\n            }\n            case \"text-delta\": {\n              controller.enqueue({\n                type: \"text-delta\",\n                id: part.id,\n                delta: part.text,\n                ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n              });\n              break;\n            }\n            case \"text-end\": {\n              controller.enqueue({\n                type: \"text-end\",\n                id: part.id,\n                ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n              });\n              break;\n            }\n            case \"reasoning-start\": {\n              controller.enqueue({\n                type: \"reasoning-start\",\n                id: part.id,\n                ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n              });\n              break;\n            }\n            case \"reasoning-delta\": {\n              if (sendReasoning) {\n                controller.enqueue({\n                  type: \"reasoning-delta\",\n                  id: part.id,\n                  delta: part.text,\n                  ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n                });\n              }\n              break;\n            }\n            case \"reasoning-end\": {\n              controller.enqueue({\n                type: \"reasoning-end\",\n                id: part.id,\n                ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n              });\n              break;\n            }\n            case \"file\": {\n              controller.enqueue({\n                type: \"file\",\n                mediaType: part.file.mediaType,\n                url: `data:${part.file.mediaType};base64,${part.file.base64}`\n              });\n              break;\n            }\n            case \"source\": {\n              if (sendSources && part.sourceType === \"url\") {\n                controller.enqueue({\n                  type: \"source-url\",\n                  sourceId: part.id,\n                  url: part.url,\n                  title: part.title,\n                  ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n                });\n              }\n              if (sendSources && part.sourceType === \"document\") {\n                controller.enqueue({\n                  type: \"source-document\",\n                  sourceId: part.id,\n                  mediaType: part.mediaType,\n                  title: part.title,\n                  filename: part.filename,\n                  ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {}\n                });\n              }\n              break;\n            }\n            case \"tool-input-start\": {\n              toolNamesByCallId[part.id] = part.toolName;\n              const dynamic = isDynamic(part.id);\n              controller.enqueue({\n                type: \"tool-input-start\",\n                toolCallId: part.id,\n                toolName: part.toolName,\n                ...part.providerExecuted != null ? { providerExecuted: part.providerExecuted } : {},\n                ...dynamic != null ? { dynamic } : {}\n              });\n              break;\n            }\n            case \"tool-input-delta\": {\n              controller.enqueue({\n                type: \"tool-input-delta\",\n                toolCallId: part.id,\n                inputTextDelta: part.delta\n              });\n              break;\n            }\n            case \"tool-call\": {\n              toolNamesByCallId[part.toolCallId] = part.toolName;\n              const dynamic = isDynamic(part.toolCallId);\n              if (part.invalid) {\n                controller.enqueue({\n                  type: \"tool-input-error\",\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  input: part.input,\n                  ...part.providerExecuted != null ? { providerExecuted: part.providerExecuted } : {},\n                  ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {},\n                  ...dynamic != null ? { dynamic } : {},\n                  errorText: onError(part.error)\n                });\n              } else {\n                controller.enqueue({\n                  type: \"tool-input-available\",\n                  toolCallId: part.toolCallId,\n                  toolName: part.toolName,\n                  input: part.input,\n                  ...part.providerExecuted != null ? { providerExecuted: part.providerExecuted } : {},\n                  ...part.providerMetadata != null ? { providerMetadata: part.providerMetadata } : {},\n                  ...dynamic != null ? { dynamic } : {}\n                });\n              }\n              break;\n            }\n            case \"tool-result\": {\n              const dynamic = isDynamic(part.toolCallId);\n              controller.enqueue({\n                type: \"tool-output-available\",\n                toolCallId: part.toolCallId,\n                output: part.output,\n                ...part.providerExecuted != null ? { providerExecuted: part.providerExecuted } : {},\n                ...part.preliminary != null ? { preliminary: part.preliminary } : {},\n                ...dynamic != null ? { dynamic } : {}\n              });\n              break;\n            }\n            case \"tool-error\": {\n              const dynamic = isDynamic(part.toolCallId);\n              controller.enqueue({\n                type: \"tool-output-error\",\n                toolCallId: part.toolCallId,\n                errorText: onError(part.error),\n                ...part.providerExecuted != null ? { providerExecuted: part.providerExecuted } : {},\n                ...dynamic != null ? { dynamic } : {}\n              });\n              break;\n            }\n            case \"error\": {\n              controller.enqueue({\n                type: \"error\",\n                errorText: onError(part.error)\n              });\n              break;\n            }\n            case \"start-step\": {\n              controller.enqueue({ type: \"start-step\" });\n              break;\n            }\n            case \"finish-step\": {\n              controller.enqueue({ type: \"finish-step\" });\n              break;\n            }\n            case \"start\": {\n              if (sendStart) {\n                controller.enqueue({\n                  type: \"start\",\n                  ...messageMetadataValue != null ? { messageMetadata: messageMetadataValue } : {},\n                  ...responseMessageId != null ? { messageId: responseMessageId } : {}\n                });\n              }\n              break;\n            }\n            case \"finish\": {\n              if (sendFinish) {\n                controller.enqueue({\n                  type: \"finish\",\n                  ...messageMetadataValue != null ? { messageMetadata: messageMetadataValue } : {}\n                });\n              }\n              break;\n            }\n            case \"abort\": {\n              controller.enqueue(part);\n              break;\n            }\n            case \"tool-input-end\": {\n              break;\n            }\n            case \"raw\": {\n              break;\n            }\n            default: {\n              const exhaustiveCheck = partType;\n              throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n            }\n          }\n          if (messageMetadataValue != null && partType !== \"start\" && partType !== \"finish\") {\n            controller.enqueue({\n              type: \"message-metadata\",\n              messageMetadata: messageMetadataValue\n            });\n          }\n        }\n      })\n    );\n    return createAsyncIterableStream(\n      handleUIMessageStreamFinish({\n        stream: baseStream,\n        messageId: responseMessageId != null ? responseMessageId : generateMessageId == null ? void 0 : generateMessageId(),\n        originalMessages,\n        onFinish,\n        onError\n      })\n    );\n  }\n  pipeUIMessageStreamToResponse(response, {\n    originalMessages,\n    generateMessageId,\n    onFinish,\n    messageMetadata,\n    sendReasoning,\n    sendSources,\n    sendFinish,\n    sendStart,\n    onError,\n    ...init\n  } = {}) {\n    pipeUIMessageStreamToResponse({\n      response,\n      stream: this.toUIMessageStream({\n        originalMessages,\n        generateMessageId,\n        onFinish,\n        messageMetadata,\n        sendReasoning,\n        sendSources,\n        sendFinish,\n        sendStart,\n        onError\n      }),\n      ...init\n    });\n  }\n  pipeTextStreamToResponse(response, init) {\n    pipeTextStreamToResponse({\n      response,\n      textStream: this.textStream,\n      ...init\n    });\n  }\n  toUIMessageStreamResponse({\n    originalMessages,\n    generateMessageId,\n    onFinish,\n    messageMetadata,\n    sendReasoning,\n    sendSources,\n    sendFinish,\n    sendStart,\n    onError,\n    ...init\n  } = {}) {\n    return createUIMessageStreamResponse({\n      stream: this.toUIMessageStream({\n        originalMessages,\n        generateMessageId,\n        onFinish,\n        messageMetadata,\n        sendReasoning,\n        sendSources,\n        sendFinish,\n        sendStart,\n        onError\n      }),\n      ...init\n    });\n  }\n  toTextStreamResponse(init) {\n    return createTextStreamResponse({\n      textStream: this.textStream,\n      ...init\n    });\n  }\n};\n\n// src/ui/convert-to-model-messages.ts\nfunction convertToModelMessages(messages, options) {\n  const modelMessages = [];\n  if (options == null ? void 0 : options.ignoreIncompleteToolCalls) {\n    messages = messages.map((message) => ({\n      ...message,\n      parts: message.parts.filter(\n        (part) => !isToolOrDynamicToolUIPart(part) || part.state !== \"input-streaming\" && part.state !== \"input-available\"\n      )\n    }));\n  }\n  for (const message of messages) {\n    switch (message.role) {\n      case \"system\": {\n        const textParts = message.parts.filter((part) => part.type === \"text\");\n        const providerMetadata = textParts.reduce((acc, part) => {\n          if (part.providerMetadata != null) {\n            return { ...acc, ...part.providerMetadata };\n          }\n          return acc;\n        }, {});\n        modelMessages.push({\n          role: \"system\",\n          content: textParts.map((part) => part.text).join(\"\"),\n          ...Object.keys(providerMetadata).length > 0 ? { providerOptions: providerMetadata } : {}\n        });\n        break;\n      }\n      case \"user\": {\n        modelMessages.push({\n          role: \"user\",\n          content: message.parts.filter(\n            (part) => part.type === \"text\" || part.type === \"file\"\n          ).map((part) => {\n            switch (part.type) {\n              case \"text\":\n                return {\n                  type: \"text\",\n                  text: part.text,\n                  ...part.providerMetadata != null ? { providerOptions: part.providerMetadata } : {}\n                };\n              case \"file\":\n                return {\n                  type: \"file\",\n                  mediaType: part.mediaType,\n                  filename: part.filename,\n                  data: part.url,\n                  ...part.providerMetadata != null ? { providerOptions: part.providerMetadata } : {}\n                };\n              default:\n                return part;\n            }\n          })\n        });\n        break;\n      }\n      case \"assistant\": {\n        if (message.parts != null) {\n          let processBlock2 = function() {\n            var _a17, _b;\n            if (block.length === 0) {\n              return;\n            }\n            const content = [];\n            for (const part of block) {\n              if (part.type === \"text\") {\n                content.push({\n                  type: \"text\",\n                  text: part.text,\n                  ...part.providerMetadata != null ? { providerOptions: part.providerMetadata } : {}\n                });\n              } else if (part.type === \"file\") {\n                content.push({\n                  type: \"file\",\n                  mediaType: part.mediaType,\n                  filename: part.filename,\n                  data: part.url\n                });\n              } else if (part.type === \"reasoning\") {\n                content.push({\n                  type: \"reasoning\",\n                  text: part.text,\n                  providerOptions: part.providerMetadata\n                });\n              } else if (part.type === \"dynamic-tool\") {\n                const toolName = part.toolName;\n                if (part.state !== \"input-streaming\") {\n                  content.push({\n                    type: \"tool-call\",\n                    toolCallId: part.toolCallId,\n                    toolName,\n                    input: part.input,\n                    ...part.callProviderMetadata != null ? { providerOptions: part.callProviderMetadata } : {}\n                  });\n                }\n              } else if (isToolUIPart(part)) {\n                const toolName = getToolName(part);\n                if (part.state !== \"input-streaming\") {\n                  content.push({\n                    type: \"tool-call\",\n                    toolCallId: part.toolCallId,\n                    toolName,\n                    input: part.state === \"output-error\" ? (_a17 = part.input) != null ? _a17 : part.rawInput : part.input,\n                    providerExecuted: part.providerExecuted,\n                    ...part.callProviderMetadata != null ? { providerOptions: part.callProviderMetadata } : {}\n                  });\n                  if (part.providerExecuted === true && (part.state === \"output-available\" || part.state === \"output-error\")) {\n                    content.push({\n                      type: \"tool-result\",\n                      toolCallId: part.toolCallId,\n                      toolName,\n                      output: createToolModelOutput({\n                        output: part.state === \"output-error\" ? part.errorText : part.output,\n                        tool: (_b = options == null ? void 0 : options.tools) == null ? void 0 : _b[toolName],\n                        errorMode: part.state === \"output-error\" ? \"json\" : \"none\"\n                      })\n                    });\n                  }\n                }\n              } else {\n                const _exhaustiveCheck = part;\n                throw new Error(`Unsupported part: ${_exhaustiveCheck}`);\n              }\n            }\n            modelMessages.push({\n              role: \"assistant\",\n              content\n            });\n            const toolParts = block.filter(\n              (part) => isToolUIPart(part) && part.providerExecuted !== true || part.type === \"dynamic-tool\"\n            );\n            if (toolParts.length > 0) {\n              modelMessages.push({\n                role: \"tool\",\n                content: toolParts.map((toolPart) => {\n                  var _a18;\n                  switch (toolPart.state) {\n                    case \"output-error\":\n                    case \"output-available\": {\n                      const toolName = toolPart.type === \"dynamic-tool\" ? toolPart.toolName : getToolName(toolPart);\n                      return {\n                        type: \"tool-result\",\n                        toolCallId: toolPart.toolCallId,\n                        toolName,\n                        output: createToolModelOutput({\n                          output: toolPart.state === \"output-error\" ? toolPart.errorText : toolPart.output,\n                          tool: (_a18 = options == null ? void 0 : options.tools) == null ? void 0 : _a18[toolName],\n                          errorMode: toolPart.state === \"output-error\" ? \"text\" : \"none\"\n                        })\n                      };\n                    }\n                    default: {\n                      return null;\n                    }\n                  }\n                }).filter(\n                  (output) => output != null\n                )\n              });\n            }\n            block = [];\n          };\n          var processBlock = processBlock2;\n          let block = [];\n          for (const part of message.parts) {\n            if (part.type === \"text\" || part.type === \"reasoning\" || part.type === \"file\" || part.type === \"dynamic-tool\" || isToolUIPart(part)) {\n              block.push(part);\n            } else if (part.type === \"step-start\") {\n              processBlock2();\n            }\n          }\n          processBlock2();\n          break;\n        }\n        break;\n      }\n      default: {\n        const _exhaustiveCheck = message.role;\n        throw new MessageConversionError({\n          originalMessage: message,\n          message: `Unsupported role: ${_exhaustiveCheck}`\n        });\n      }\n    }\n  }\n  return modelMessages;\n}\nvar convertToCoreMessages = convertToModelMessages;\n\n// src/agent/agent.ts\nvar Agent = class {\n  constructor(settings) {\n    this.settings = settings;\n  }\n  get tools() {\n    return this.settings.tools;\n  }\n  async generate(options) {\n    return generateText({ ...this.settings, ...options });\n  }\n  stream(options) {\n    return streamText({ ...this.settings, ...options });\n  }\n  /**\n   * Creates a response object that streams UI messages to the client.\n   */\n  respond(options) {\n    return this.stream({\n      prompt: convertToModelMessages(options.messages)\n    }).toUIMessageStreamResponse();\n  }\n};\n\n// src/embed/embed.ts\nasync function embed({\n  model: modelArg,\n  value,\n  providerOptions,\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry\n}) {\n  const model = resolveEmbeddingModel(modelArg);\n  const { maxRetries, retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { maxRetries }\n  });\n  const tracer = getTracer(telemetry);\n  return recordSpan({\n    name: \"ai.embed\",\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({ operationId: \"ai.embed\", telemetry }),\n        ...baseTelemetryAttributes,\n        \"ai.value\": { input: () => JSON.stringify(value) }\n      }\n    }),\n    tracer,\n    fn: async (span) => {\n      const { embedding, usage, response, providerMetadata } = await retry(\n        () => (\n          // nested spans to align with the embedMany telemetry data:\n          recordSpan({\n            name: \"ai.embed.doEmbed\",\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: \"ai.embed.doEmbed\",\n                  telemetry\n                }),\n                ...baseTelemetryAttributes,\n                // specific settings that only make sense on the outer level:\n                \"ai.values\": { input: () => [JSON.stringify(value)] }\n              }\n            }),\n            tracer,\n            fn: async (doEmbedSpan) => {\n              var _a17;\n              const modelResponse = await model.doEmbed({\n                values: [value],\n                abortSignal,\n                headers,\n                providerOptions\n              });\n              const embedding2 = modelResponse.embeddings[0];\n              const usage2 = (_a17 = modelResponse.usage) != null ? _a17 : { tokens: NaN };\n              doEmbedSpan.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    \"ai.embeddings\": {\n                      output: () => modelResponse.embeddings.map(\n                        (embedding3) => JSON.stringify(embedding3)\n                      )\n                    },\n                    \"ai.usage.tokens\": usage2.tokens\n                  }\n                })\n              );\n              return {\n                embedding: embedding2,\n                usage: usage2,\n                providerMetadata: modelResponse.providerMetadata,\n                response: modelResponse.response\n              };\n            }\n          })\n        )\n      );\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            \"ai.embedding\": { output: () => JSON.stringify(embedding) },\n            \"ai.usage.tokens\": usage.tokens\n          }\n        })\n      );\n      return new DefaultEmbedResult({\n        value,\n        embedding,\n        usage,\n        providerMetadata,\n        response\n      });\n    }\n  });\n}\nvar DefaultEmbedResult = class {\n  constructor(options) {\n    this.value = options.value;\n    this.embedding = options.embedding;\n    this.usage = options.usage;\n    this.providerMetadata = options.providerMetadata;\n    this.response = options.response;\n  }\n};\n\n// src/util/split-array.ts\nfunction splitArray(array, chunkSize) {\n  if (chunkSize <= 0) {\n    throw new Error(\"chunkSize must be greater than 0\");\n  }\n  const result = [];\n  for (let i = 0; i < array.length; i += chunkSize) {\n    result.push(array.slice(i, i + chunkSize));\n  }\n  return result;\n}\n\n// src/embed/embed-many.ts\nasync function embedMany({\n  model: modelArg,\n  values,\n  maxParallelCalls = Infinity,\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers,\n  providerOptions,\n  experimental_telemetry: telemetry\n}) {\n  const model = resolveEmbeddingModel(modelArg);\n  const { maxRetries, retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { maxRetries }\n  });\n  const tracer = getTracer(telemetry);\n  return recordSpan({\n    name: \"ai.embedMany\",\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({ operationId: \"ai.embedMany\", telemetry }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        \"ai.values\": {\n          input: () => values.map((value) => JSON.stringify(value))\n        }\n      }\n    }),\n    tracer,\n    fn: async (span) => {\n      var _a17;\n      const [maxEmbeddingsPerCall, supportsParallelCalls] = await Promise.all([\n        model.maxEmbeddingsPerCall,\n        model.supportsParallelCalls\n      ]);\n      if (maxEmbeddingsPerCall == null || maxEmbeddingsPerCall === Infinity) {\n        const { embeddings: embeddings2, usage, response, providerMetadata: providerMetadata2 } = await retry(\n          () => {\n            return recordSpan({\n              name: \"ai.embedMany.doEmbed\",\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: \"ai.embedMany.doEmbed\",\n                    telemetry\n                  }),\n                  ...baseTelemetryAttributes,\n                  // specific settings that only make sense on the outer level:\n                  \"ai.values\": {\n                    input: () => values.map((value) => JSON.stringify(value))\n                  }\n                }\n              }),\n              tracer,\n              fn: async (doEmbedSpan) => {\n                var _a18;\n                const modelResponse = await model.doEmbed({\n                  values,\n                  abortSignal,\n                  headers,\n                  providerOptions\n                });\n                const embeddings3 = modelResponse.embeddings;\n                const usage2 = (_a18 = modelResponse.usage) != null ? _a18 : { tokens: NaN };\n                doEmbedSpan.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      \"ai.embeddings\": {\n                        output: () => embeddings3.map(\n                          (embedding) => JSON.stringify(embedding)\n                        )\n                      },\n                      \"ai.usage.tokens\": usage2.tokens\n                    }\n                  })\n                );\n                return {\n                  embeddings: embeddings3,\n                  usage: usage2,\n                  providerMetadata: modelResponse.providerMetadata,\n                  response: modelResponse.response\n                };\n              }\n            });\n          }\n        );\n        span.setAttributes(\n          selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              \"ai.embeddings\": {\n                output: () => embeddings2.map((embedding) => JSON.stringify(embedding))\n              },\n              \"ai.usage.tokens\": usage.tokens\n            }\n          })\n        );\n        return new DefaultEmbedManyResult({\n          values,\n          embeddings: embeddings2,\n          usage,\n          providerMetadata: providerMetadata2,\n          responses: [response]\n        });\n      }\n      const valueChunks = splitArray(values, maxEmbeddingsPerCall);\n      const embeddings = [];\n      const responses = [];\n      let tokens = 0;\n      let providerMetadata;\n      const parallelChunks = splitArray(\n        valueChunks,\n        supportsParallelCalls ? maxParallelCalls : 1\n      );\n      for (const parallelChunk of parallelChunks) {\n        const results = await Promise.all(\n          parallelChunk.map((chunk) => {\n            return retry(() => {\n              return recordSpan({\n                name: \"ai.embedMany.doEmbed\",\n                attributes: selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    ...assembleOperationName({\n                      operationId: \"ai.embedMany.doEmbed\",\n                      telemetry\n                    }),\n                    ...baseTelemetryAttributes,\n                    // specific settings that only make sense on the outer level:\n                    \"ai.values\": {\n                      input: () => chunk.map((value) => JSON.stringify(value))\n                    }\n                  }\n                }),\n                tracer,\n                fn: async (doEmbedSpan) => {\n                  var _a18;\n                  const modelResponse = await model.doEmbed({\n                    values: chunk,\n                    abortSignal,\n                    headers,\n                    providerOptions\n                  });\n                  const embeddings2 = modelResponse.embeddings;\n                  const usage = (_a18 = modelResponse.usage) != null ? _a18 : { tokens: NaN };\n                  doEmbedSpan.setAttributes(\n                    selectTelemetryAttributes({\n                      telemetry,\n                      attributes: {\n                        \"ai.embeddings\": {\n                          output: () => embeddings2.map(\n                            (embedding) => JSON.stringify(embedding)\n                          )\n                        },\n                        \"ai.usage.tokens\": usage.tokens\n                      }\n                    })\n                  );\n                  return {\n                    embeddings: embeddings2,\n                    usage,\n                    providerMetadata: modelResponse.providerMetadata,\n                    response: modelResponse.response\n                  };\n                }\n              });\n            });\n          })\n        );\n        for (const result of results) {\n          embeddings.push(...result.embeddings);\n          responses.push(result.response);\n          tokens += result.usage.tokens;\n          if (result.providerMetadata) {\n            if (!providerMetadata) {\n              providerMetadata = { ...result.providerMetadata };\n            } else {\n              for (const [providerName, metadata] of Object.entries(\n                result.providerMetadata\n              )) {\n                providerMetadata[providerName] = {\n                  ...(_a17 = providerMetadata[providerName]) != null ? _a17 : {},\n                  ...metadata\n                };\n              }\n            }\n          }\n        }\n      }\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            \"ai.embeddings\": {\n              output: () => embeddings.map((embedding) => JSON.stringify(embedding))\n            },\n            \"ai.usage.tokens\": tokens\n          }\n        })\n      );\n      return new DefaultEmbedManyResult({\n        values,\n        embeddings,\n        usage: { tokens },\n        providerMetadata,\n        responses\n      });\n    }\n  });\n}\nvar DefaultEmbedManyResult = class {\n  constructor(options) {\n    this.values = options.values;\n    this.embeddings = options.embeddings;\n    this.usage = options.usage;\n    this.providerMetadata = options.providerMetadata;\n    this.responses = options.responses;\n  }\n};\n\n// src/generate-image/generate-image.ts\nasync function generateImage({\n  model,\n  prompt,\n  n = 1,\n  maxImagesPerCall,\n  size,\n  aspectRatio,\n  seed,\n  providerOptions,\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers\n}) {\n  var _a17, _b;\n  if (model.specificationVersion !== \"v2\") {\n    throw new UnsupportedModelVersionError({\n      version: model.specificationVersion,\n      provider: model.provider,\n      modelId: model.modelId\n    });\n  }\n  const { retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const maxImagesPerCallWithDefault = (_a17 = maxImagesPerCall != null ? maxImagesPerCall : await invokeModelMaxImagesPerCall(model)) != null ? _a17 : 1;\n  const callCount = Math.ceil(n / maxImagesPerCallWithDefault);\n  const callImageCounts = Array.from({ length: callCount }, (_, i) => {\n    if (i < callCount - 1) {\n      return maxImagesPerCallWithDefault;\n    }\n    const remainder = n % maxImagesPerCallWithDefault;\n    return remainder === 0 ? maxImagesPerCallWithDefault : remainder;\n  });\n  const results = await Promise.all(\n    callImageCounts.map(\n      async (callImageCount) => retry(\n        () => model.doGenerate({\n          prompt,\n          n: callImageCount,\n          abortSignal,\n          headers,\n          size,\n          aspectRatio,\n          seed,\n          providerOptions: providerOptions != null ? providerOptions : {}\n        })\n      )\n    )\n  );\n  const images = [];\n  const warnings = [];\n  const responses = [];\n  const providerMetadata = {};\n  for (const result of results) {\n    images.push(\n      ...result.images.map(\n        (image) => {\n          var _a18;\n          return new DefaultGeneratedFile({\n            data: image,\n            mediaType: (_a18 = detectMediaType({\n              data: image,\n              signatures: imageMediaTypeSignatures\n            })) != null ? _a18 : \"image/png\"\n          });\n        }\n      )\n    );\n    warnings.push(...result.warnings);\n    if (result.providerMetadata) {\n      for (const [providerName, metadata] of Object.entries(result.providerMetadata)) {\n        (_b = providerMetadata[providerName]) != null ? _b : providerMetadata[providerName] = { images: [] };\n        providerMetadata[providerName].images.push(\n          ...result.providerMetadata[providerName].images\n        );\n      }\n    }\n    responses.push(result.response);\n  }\n  logWarnings(warnings);\n  if (!images.length) {\n    throw new NoImageGeneratedError({ responses });\n  }\n  return new DefaultGenerateImageResult({\n    images,\n    warnings,\n    responses,\n    providerMetadata\n  });\n}\nvar DefaultGenerateImageResult = class {\n  constructor(options) {\n    this.images = options.images;\n    this.warnings = options.warnings;\n    this.responses = options.responses;\n    this.providerMetadata = options.providerMetadata;\n  }\n  get image() {\n    return this.images[0];\n  }\n};\nasync function invokeModelMaxImagesPerCall(model) {\n  const isFunction = model.maxImagesPerCall instanceof Function;\n  if (!isFunction) {\n    return model.maxImagesPerCall;\n  }\n  return model.maxImagesPerCall({\n    modelId: model.modelId\n  });\n}\n\n// src/generate-object/generate-object.ts\nimport {\n  createIdGenerator as createIdGenerator3\n} from \"@ai-sdk/provider-utils\";\n\n// src/generate-text/extract-reasoning-content.ts\nfunction extractReasoningContent(content) {\n  const parts = content.filter(\n    (content2) => content2.type === \"reasoning\"\n  );\n  return parts.length === 0 ? void 0 : parts.map((content2) => content2.text).join(\"\\n\");\n}\n\n// src/generate-object/output-strategy.ts\nimport {\n  isJSONArray,\n  isJSONObject,\n  TypeValidationError as TypeValidationError2,\n  UnsupportedFunctionalityError as UnsupportedFunctionalityError2\n} from \"@ai-sdk/provider\";\nimport {\n  asSchema as asSchema3,\n  safeValidateTypes as safeValidateTypes3\n} from \"@ai-sdk/provider-utils\";\nvar noSchemaOutputStrategy = {\n  type: \"no-schema\",\n  jsonSchema: void 0,\n  async validatePartialResult({ value, textDelta }) {\n    return { success: true, value: { partial: value, textDelta } };\n  },\n  async validateFinalResult(value, context) {\n    return value === void 0 ? {\n      success: false,\n      error: new NoObjectGeneratedError({\n        message: \"No object generated: response did not match schema.\",\n        text: context.text,\n        response: context.response,\n        usage: context.usage,\n        finishReason: context.finishReason\n      })\n    } : { success: true, value };\n  },\n  createElementStream() {\n    throw new UnsupportedFunctionalityError2({\n      functionality: \"element streams in no-schema mode\"\n    });\n  }\n};\nvar objectOutputStrategy = (schema) => ({\n  type: \"object\",\n  jsonSchema: schema.jsonSchema,\n  async validatePartialResult({ value, textDelta }) {\n    return {\n      success: true,\n      value: {\n        // Note: currently no validation of partial results:\n        partial: value,\n        textDelta\n      }\n    };\n  },\n  async validateFinalResult(value) {\n    return safeValidateTypes3({ value, schema });\n  },\n  createElementStream() {\n    throw new UnsupportedFunctionalityError2({\n      functionality: \"element streams in object mode\"\n    });\n  }\n});\nvar arrayOutputStrategy = (schema) => {\n  const { $schema, ...itemSchema } = schema.jsonSchema;\n  return {\n    type: \"enum\",\n    // wrap in object that contains array of elements, since most LLMs will not\n    // be able to generate an array directly:\n    // possible future optimization: use arrays directly when model supports grammar-guided generation\n    jsonSchema: {\n      $schema: \"http://json-schema.org/draft-07/schema#\",\n      type: \"object\",\n      properties: {\n        elements: { type: \"array\", items: itemSchema }\n      },\n      required: [\"elements\"],\n      additionalProperties: false\n    },\n    async validatePartialResult({\n      value,\n      latestObject,\n      isFirstDelta,\n      isFinalDelta\n    }) {\n      var _a17;\n      if (!isJSONObject(value) || !isJSONArray(value.elements)) {\n        return {\n          success: false,\n          error: new TypeValidationError2({\n            value,\n            cause: \"value must be an object that contains an array of elements\"\n          })\n        };\n      }\n      const inputArray = value.elements;\n      const resultArray = [];\n      for (let i = 0; i < inputArray.length; i++) {\n        const element = inputArray[i];\n        const result = await safeValidateTypes3({ value: element, schema });\n        if (i === inputArray.length - 1 && !isFinalDelta) {\n          continue;\n        }\n        if (!result.success) {\n          return result;\n        }\n        resultArray.push(result.value);\n      }\n      const publishedElementCount = (_a17 = latestObject == null ? void 0 : latestObject.length) != null ? _a17 : 0;\n      let textDelta = \"\";\n      if (isFirstDelta) {\n        textDelta += \"[\";\n      }\n      if (publishedElementCount > 0) {\n        textDelta += \",\";\n      }\n      textDelta += resultArray.slice(publishedElementCount).map((element) => JSON.stringify(element)).join(\",\");\n      if (isFinalDelta) {\n        textDelta += \"]\";\n      }\n      return {\n        success: true,\n        value: {\n          partial: resultArray,\n          textDelta\n        }\n      };\n    },\n    async validateFinalResult(value) {\n      if (!isJSONObject(value) || !isJSONArray(value.elements)) {\n        return {\n          success: false,\n          error: new TypeValidationError2({\n            value,\n            cause: \"value must be an object that contains an array of elements\"\n          })\n        };\n      }\n      const inputArray = value.elements;\n      for (const element of inputArray) {\n        const result = await safeValidateTypes3({ value: element, schema });\n        if (!result.success) {\n          return result;\n        }\n      }\n      return { success: true, value: inputArray };\n    },\n    createElementStream(originalStream) {\n      let publishedElements = 0;\n      return createAsyncIterableStream(\n        originalStream.pipeThrough(\n          new TransformStream({\n            transform(chunk, controller) {\n              switch (chunk.type) {\n                case \"object\": {\n                  const array = chunk.object;\n                  for (; publishedElements < array.length; publishedElements++) {\n                    controller.enqueue(array[publishedElements]);\n                  }\n                  break;\n                }\n                case \"text-delta\":\n                case \"finish\":\n                case \"error\":\n                  break;\n                default: {\n                  const _exhaustiveCheck = chunk;\n                  throw new Error(\n                    `Unsupported chunk type: ${_exhaustiveCheck}`\n                  );\n                }\n              }\n            }\n          })\n        )\n      );\n    }\n  };\n};\nvar enumOutputStrategy = (enumValues) => {\n  return {\n    type: \"enum\",\n    // wrap in object that contains result, since most LLMs will not\n    // be able to generate an enum value directly:\n    // possible future optimization: use enums directly when model supports top-level enums\n    jsonSchema: {\n      $schema: \"http://json-schema.org/draft-07/schema#\",\n      type: \"object\",\n      properties: {\n        result: { type: \"string\", enum: enumValues }\n      },\n      required: [\"result\"],\n      additionalProperties: false\n    },\n    async validateFinalResult(value) {\n      if (!isJSONObject(value) || typeof value.result !== \"string\") {\n        return {\n          success: false,\n          error: new TypeValidationError2({\n            value,\n            cause: 'value must be an object that contains a string in the \"result\" property.'\n          })\n        };\n      }\n      const result = value.result;\n      return enumValues.includes(result) ? { success: true, value: result } : {\n        success: false,\n        error: new TypeValidationError2({\n          value,\n          cause: \"value must be a string in the enum\"\n        })\n      };\n    },\n    async validatePartialResult({ value, textDelta }) {\n      if (!isJSONObject(value) || typeof value.result !== \"string\") {\n        return {\n          success: false,\n          error: new TypeValidationError2({\n            value,\n            cause: 'value must be an object that contains a string in the \"result\" property.'\n          })\n        };\n      }\n      const result = value.result;\n      const possibleEnumValues = enumValues.filter(\n        (enumValue) => enumValue.startsWith(result)\n      );\n      if (value.result.length === 0 || possibleEnumValues.length === 0) {\n        return {\n          success: false,\n          error: new TypeValidationError2({\n            value,\n            cause: \"value must be a string in the enum\"\n          })\n        };\n      }\n      return {\n        success: true,\n        value: {\n          partial: possibleEnumValues.length > 1 ? result : possibleEnumValues[0],\n          textDelta\n        }\n      };\n    },\n    createElementStream() {\n      throw new UnsupportedFunctionalityError2({\n        functionality: \"element streams in enum mode\"\n      });\n    }\n  };\n};\nfunction getOutputStrategy({\n  output,\n  schema,\n  enumValues\n}) {\n  switch (output) {\n    case \"object\":\n      return objectOutputStrategy(asSchema3(schema));\n    case \"array\":\n      return arrayOutputStrategy(asSchema3(schema));\n    case \"enum\":\n      return enumOutputStrategy(enumValues);\n    case \"no-schema\":\n      return noSchemaOutputStrategy;\n    default: {\n      const _exhaustiveCheck = output;\n      throw new Error(`Unsupported output: ${_exhaustiveCheck}`);\n    }\n  }\n}\n\n// src/generate-object/parse-and-validate-object-result.ts\nimport { JSONParseError as JSONParseError2, TypeValidationError as TypeValidationError3 } from \"@ai-sdk/provider\";\nimport { safeParseJSON as safeParseJSON3 } from \"@ai-sdk/provider-utils\";\nasync function parseAndValidateObjectResult(result, outputStrategy, context) {\n  const parseResult = await safeParseJSON3({ text: result });\n  if (!parseResult.success) {\n    throw new NoObjectGeneratedError({\n      message: \"No object generated: could not parse the response.\",\n      cause: parseResult.error,\n      text: result,\n      response: context.response,\n      usage: context.usage,\n      finishReason: context.finishReason\n    });\n  }\n  const validationResult = await outputStrategy.validateFinalResult(\n    parseResult.value,\n    {\n      text: result,\n      response: context.response,\n      usage: context.usage\n    }\n  );\n  if (!validationResult.success) {\n    throw new NoObjectGeneratedError({\n      message: \"No object generated: response did not match schema.\",\n      cause: validationResult.error,\n      text: result,\n      response: context.response,\n      usage: context.usage,\n      finishReason: context.finishReason\n    });\n  }\n  return validationResult.value;\n}\nasync function parseAndValidateObjectResultWithRepair(result, outputStrategy, repairText, context) {\n  try {\n    return await parseAndValidateObjectResult(result, outputStrategy, context);\n  } catch (error) {\n    if (repairText != null && NoObjectGeneratedError.isInstance(error) && (JSONParseError2.isInstance(error.cause) || TypeValidationError3.isInstance(error.cause))) {\n      const repairedText = await repairText({\n        text: result,\n        error: error.cause\n      });\n      if (repairedText === null) {\n        throw error;\n      }\n      return await parseAndValidateObjectResult(\n        repairedText,\n        outputStrategy,\n        context\n      );\n    }\n    throw error;\n  }\n}\n\n// src/generate-object/validate-object-generation-input.ts\nfunction validateObjectGenerationInput({\n  output,\n  schema,\n  schemaName,\n  schemaDescription,\n  enumValues\n}) {\n  if (output != null && output !== \"object\" && output !== \"array\" && output !== \"enum\" && output !== \"no-schema\") {\n    throw new InvalidArgumentError({\n      parameter: \"output\",\n      value: output,\n      message: \"Invalid output type.\"\n    });\n  }\n  if (output === \"no-schema\") {\n    if (schema != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schema\",\n        value: schema,\n        message: \"Schema is not supported for no-schema output.\"\n      });\n    }\n    if (schemaDescription != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schemaDescription\",\n        value: schemaDescription,\n        message: \"Schema description is not supported for no-schema output.\"\n      });\n    }\n    if (schemaName != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schemaName\",\n        value: schemaName,\n        message: \"Schema name is not supported for no-schema output.\"\n      });\n    }\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: \"enumValues\",\n        value: enumValues,\n        message: \"Enum values are not supported for no-schema output.\"\n      });\n    }\n  }\n  if (output === \"object\") {\n    if (schema == null) {\n      throw new InvalidArgumentError({\n        parameter: \"schema\",\n        value: schema,\n        message: \"Schema is required for object output.\"\n      });\n    }\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: \"enumValues\",\n        value: enumValues,\n        message: \"Enum values are not supported for object output.\"\n      });\n    }\n  }\n  if (output === \"array\") {\n    if (schema == null) {\n      throw new InvalidArgumentError({\n        parameter: \"schema\",\n        value: schema,\n        message: \"Element schema is required for array output.\"\n      });\n    }\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: \"enumValues\",\n        value: enumValues,\n        message: \"Enum values are not supported for array output.\"\n      });\n    }\n  }\n  if (output === \"enum\") {\n    if (schema != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schema\",\n        value: schema,\n        message: \"Schema is not supported for enum output.\"\n      });\n    }\n    if (schemaDescription != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schemaDescription\",\n        value: schemaDescription,\n        message: \"Schema description is not supported for enum output.\"\n      });\n    }\n    if (schemaName != null) {\n      throw new InvalidArgumentError({\n        parameter: \"schemaName\",\n        value: schemaName,\n        message: \"Schema name is not supported for enum output.\"\n      });\n    }\n    if (enumValues == null) {\n      throw new InvalidArgumentError({\n        parameter: \"enumValues\",\n        value: enumValues,\n        message: \"Enum values are required for enum output.\"\n      });\n    }\n    for (const value of enumValues) {\n      if (typeof value !== \"string\") {\n        throw new InvalidArgumentError({\n          parameter: \"enumValues\",\n          value,\n          message: \"Enum values must be strings.\"\n        });\n      }\n    }\n  }\n}\n\n// src/generate-object/generate-object.ts\nvar originalGenerateId3 = createIdGenerator3({ prefix: \"aiobj\", size: 24 });\nasync function generateObject(options) {\n  const {\n    model: modelArg,\n    output = \"object\",\n    system,\n    prompt,\n    messages,\n    maxRetries: maxRetriesArg,\n    abortSignal,\n    headers,\n    experimental_repairText: repairText,\n    experimental_telemetry: telemetry,\n    experimental_download: download2,\n    providerOptions,\n    _internal: {\n      generateId: generateId3 = originalGenerateId3,\n      currentDate = () => /* @__PURE__ */ new Date()\n    } = {},\n    ...settings\n  } = options;\n  const model = resolveLanguageModel(modelArg);\n  const enumValues = \"enum\" in options ? options.enum : void 0;\n  const {\n    schema: inputSchema,\n    schemaDescription,\n    schemaName\n  } = \"schema\" in options ? options : {};\n  validateObjectGenerationInput({\n    output,\n    schema: inputSchema,\n    schemaName,\n    schemaDescription,\n    enumValues\n  });\n  const { maxRetries, retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const outputStrategy = getOutputStrategy({\n    output,\n    schema: inputSchema,\n    enumValues\n  });\n  const callSettings = prepareCallSettings(settings);\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...callSettings, maxRetries }\n  });\n  const tracer = getTracer(telemetry);\n  try {\n    return await recordSpan({\n      name: \"ai.generateObject\",\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({\n            operationId: \"ai.generateObject\",\n            telemetry\n          }),\n          ...baseTelemetryAttributes,\n          // specific settings that only make sense on the outer level:\n          \"ai.prompt\": {\n            input: () => JSON.stringify({ system, prompt, messages })\n          },\n          \"ai.schema\": outputStrategy.jsonSchema != null ? { input: () => JSON.stringify(outputStrategy.jsonSchema) } : void 0,\n          \"ai.schema.name\": schemaName,\n          \"ai.schema.description\": schemaDescription,\n          \"ai.settings.output\": outputStrategy.type\n        }\n      }),\n      tracer,\n      fn: async (span) => {\n        var _a17;\n        let result;\n        let finishReason;\n        let usage;\n        let warnings;\n        let response;\n        let request;\n        let resultProviderMetadata;\n        let reasoning;\n        const standardizedPrompt = await standardizePrompt({\n          system,\n          prompt,\n          messages\n        });\n        const promptMessages = await convertToLanguageModelPrompt({\n          prompt: standardizedPrompt,\n          supportedUrls: await model.supportedUrls,\n          download: download2\n        });\n        const generateResult = await retry(\n          () => recordSpan({\n            name: \"ai.generateObject.doGenerate\",\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: \"ai.generateObject.doGenerate\",\n                  telemetry\n                }),\n                ...baseTelemetryAttributes,\n                \"ai.prompt.messages\": {\n                  input: () => stringifyForTelemetry(promptMessages)\n                },\n                // standardized gen-ai llm span attributes:\n                \"gen_ai.system\": model.provider,\n                \"gen_ai.request.model\": model.modelId,\n                \"gen_ai.request.frequency_penalty\": callSettings.frequencyPenalty,\n                \"gen_ai.request.max_tokens\": callSettings.maxOutputTokens,\n                \"gen_ai.request.presence_penalty\": callSettings.presencePenalty,\n                \"gen_ai.request.temperature\": callSettings.temperature,\n                \"gen_ai.request.top_k\": callSettings.topK,\n                \"gen_ai.request.top_p\": callSettings.topP\n              }\n            }),\n            tracer,\n            fn: async (span2) => {\n              var _a18, _b, _c, _d, _e, _f, _g, _h;\n              const result2 = await model.doGenerate({\n                responseFormat: {\n                  type: \"json\",\n                  schema: outputStrategy.jsonSchema,\n                  name: schemaName,\n                  description: schemaDescription\n                },\n                ...prepareCallSettings(settings),\n                prompt: promptMessages,\n                providerOptions,\n                abortSignal,\n                headers\n              });\n              const responseData = {\n                id: (_b = (_a18 = result2.response) == null ? void 0 : _a18.id) != null ? _b : generateId3(),\n                timestamp: (_d = (_c = result2.response) == null ? void 0 : _c.timestamp) != null ? _d : currentDate(),\n                modelId: (_f = (_e = result2.response) == null ? void 0 : _e.modelId) != null ? _f : model.modelId,\n                headers: (_g = result2.response) == null ? void 0 : _g.headers,\n                body: (_h = result2.response) == null ? void 0 : _h.body\n              };\n              const text2 = extractTextContent(result2.content);\n              const reasoning2 = extractReasoningContent(result2.content);\n              if (text2 === void 0) {\n                throw new NoObjectGeneratedError({\n                  message: \"No object generated: the model did not return a response.\",\n                  response: responseData,\n                  usage: result2.usage,\n                  finishReason: result2.finishReason\n                });\n              }\n              span2.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    \"ai.response.finishReason\": result2.finishReason,\n                    \"ai.response.object\": { output: () => text2 },\n                    \"ai.response.id\": responseData.id,\n                    \"ai.response.model\": responseData.modelId,\n                    \"ai.response.timestamp\": responseData.timestamp.toISOString(),\n                    \"ai.response.providerMetadata\": JSON.stringify(\n                      result2.providerMetadata\n                    ),\n                    // TODO rename telemetry attributes to inputTokens and outputTokens\n                    \"ai.usage.promptTokens\": result2.usage.inputTokens,\n                    \"ai.usage.completionTokens\": result2.usage.outputTokens,\n                    // standardized gen-ai llm span attributes:\n                    \"gen_ai.response.finish_reasons\": [result2.finishReason],\n                    \"gen_ai.response.id\": responseData.id,\n                    \"gen_ai.response.model\": responseData.modelId,\n                    \"gen_ai.usage.input_tokens\": result2.usage.inputTokens,\n                    \"gen_ai.usage.output_tokens\": result2.usage.outputTokens\n                  }\n                })\n              );\n              return {\n                ...result2,\n                objectText: text2,\n                reasoning: reasoning2,\n                responseData\n              };\n            }\n          })\n        );\n        result = generateResult.objectText;\n        finishReason = generateResult.finishReason;\n        usage = generateResult.usage;\n        warnings = generateResult.warnings;\n        resultProviderMetadata = generateResult.providerMetadata;\n        request = (_a17 = generateResult.request) != null ? _a17 : {};\n        response = generateResult.responseData;\n        reasoning = generateResult.reasoning;\n        logWarnings(warnings);\n        const object2 = await parseAndValidateObjectResultWithRepair(\n          result,\n          outputStrategy,\n          repairText,\n          {\n            response,\n            usage,\n            finishReason\n          }\n        );\n        span.setAttributes(\n          selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              \"ai.response.finishReason\": finishReason,\n              \"ai.response.object\": {\n                output: () => JSON.stringify(object2)\n              },\n              \"ai.response.providerMetadata\": JSON.stringify(\n                resultProviderMetadata\n              ),\n              // TODO rename telemetry attributes to inputTokens and outputTokens\n              \"ai.usage.promptTokens\": usage.inputTokens,\n              \"ai.usage.completionTokens\": usage.outputTokens\n            }\n          })\n        );\n        return new DefaultGenerateObjectResult({\n          object: object2,\n          reasoning,\n          finishReason,\n          usage,\n          warnings,\n          request,\n          response,\n          providerMetadata: resultProviderMetadata\n        });\n      }\n    });\n  } catch (error) {\n    throw wrapGatewayError(error);\n  }\n}\nvar DefaultGenerateObjectResult = class {\n  constructor(options) {\n    this.object = options.object;\n    this.finishReason = options.finishReason;\n    this.usage = options.usage;\n    this.warnings = options.warnings;\n    this.providerMetadata = options.providerMetadata;\n    this.response = options.response;\n    this.request = options.request;\n    this.reasoning = options.reasoning;\n  }\n  toJsonResponse(init) {\n    var _a17;\n    return new Response(JSON.stringify(this.object), {\n      status: (_a17 = init == null ? void 0 : init.status) != null ? _a17 : 200,\n      headers: prepareHeaders(init == null ? void 0 : init.headers, {\n        \"content-type\": \"application/json; charset=utf-8\"\n      })\n    });\n  }\n};\n\n// src/generate-object/stream-object.ts\nimport {\n  createIdGenerator as createIdGenerator4\n} from \"@ai-sdk/provider-utils\";\n\n// src/util/cosine-similarity.ts\nfunction cosineSimilarity(vector1, vector2) {\n  if (vector1.length !== vector2.length) {\n    throw new InvalidArgumentError({\n      parameter: \"vector1,vector2\",\n      value: { vector1Length: vector1.length, vector2Length: vector2.length },\n      message: `Vectors must have the same length`\n    });\n  }\n  const n = vector1.length;\n  if (n === 0) {\n    return 0;\n  }\n  let magnitudeSquared1 = 0;\n  let magnitudeSquared2 = 0;\n  let dotProduct = 0;\n  for (let i = 0; i < n; i++) {\n    const value1 = vector1[i];\n    const value2 = vector2[i];\n    magnitudeSquared1 += value1 * value1;\n    magnitudeSquared2 += value2 * value2;\n    dotProduct += value1 * value2;\n  }\n  return magnitudeSquared1 === 0 || magnitudeSquared2 === 0 ? 0 : dotProduct / (Math.sqrt(magnitudeSquared1) * Math.sqrt(magnitudeSquared2));\n}\n\n// src/util/data-url.ts\nfunction getTextFromDataUrl(dataUrl) {\n  const [header, base64Content] = dataUrl.split(\",\");\n  const mediaType = header.split(\";\")[0].split(\":\")[1];\n  if (mediaType == null || base64Content == null) {\n    throw new Error(\"Invalid data URL format\");\n  }\n  try {\n    return window.atob(base64Content);\n  } catch (error) {\n    throw new Error(`Error decoding data URL`);\n  }\n}\n\n// src/util/is-deep-equal-data.ts\nfunction isDeepEqualData(obj1, obj2) {\n  if (obj1 === obj2)\n    return true;\n  if (obj1 == null || obj2 == null)\n    return false;\n  if (typeof obj1 !== \"object\" && typeof obj2 !== \"object\")\n    return obj1 === obj2;\n  if (obj1.constructor !== obj2.constructor)\n    return false;\n  if (obj1 instanceof Date && obj2 instanceof Date) {\n    return obj1.getTime() === obj2.getTime();\n  }\n  if (Array.isArray(obj1)) {\n    if (obj1.length !== obj2.length)\n      return false;\n    for (let i = 0; i < obj1.length; i++) {\n      if (!isDeepEqualData(obj1[i], obj2[i]))\n        return false;\n    }\n    return true;\n  }\n  const keys1 = Object.keys(obj1);\n  const keys2 = Object.keys(obj2);\n  if (keys1.length !== keys2.length)\n    return false;\n  for (const key of keys1) {\n    if (!keys2.includes(key))\n      return false;\n    if (!isDeepEqualData(obj1[key], obj2[key]))\n      return false;\n  }\n  return true;\n}\n\n// src/util/serial-job-executor.ts\nvar SerialJobExecutor = class {\n  constructor() {\n    this.queue = [];\n    this.isProcessing = false;\n  }\n  async processQueue() {\n    if (this.isProcessing) {\n      return;\n    }\n    this.isProcessing = true;\n    while (this.queue.length > 0) {\n      await this.queue[0]();\n      this.queue.shift();\n    }\n    this.isProcessing = false;\n  }\n  async run(job) {\n    return new Promise((resolve2, reject) => {\n      this.queue.push(async () => {\n        try {\n          await job();\n          resolve2();\n        } catch (error) {\n          reject(error);\n        }\n      });\n      void this.processQueue();\n    });\n  }\n};\n\n// src/util/simulate-readable-stream.ts\nimport { delay as delayFunction } from \"@ai-sdk/provider-utils\";\nfunction simulateReadableStream({\n  chunks,\n  initialDelayInMs = 0,\n  chunkDelayInMs = 0,\n  _internal\n}) {\n  var _a17;\n  const delay2 = (_a17 = _internal == null ? void 0 : _internal.delay) != null ? _a17 : delayFunction;\n  let index = 0;\n  return new ReadableStream({\n    async pull(controller) {\n      if (index < chunks.length) {\n        await delay2(index === 0 ? initialDelayInMs : chunkDelayInMs);\n        controller.enqueue(chunks[index++]);\n      } else {\n        controller.close();\n      }\n    }\n  });\n}\n\n// src/generate-object/stream-object.ts\nvar originalGenerateId4 = createIdGenerator4({ prefix: \"aiobj\", size: 24 });\nfunction streamObject(options) {\n  const {\n    model,\n    output = \"object\",\n    system,\n    prompt,\n    messages,\n    maxRetries,\n    abortSignal,\n    headers,\n    experimental_repairText: repairText,\n    experimental_telemetry: telemetry,\n    experimental_download: download2,\n    providerOptions,\n    onError = ({ error }) => {\n      console.error(error);\n    },\n    onFinish,\n    _internal: {\n      generateId: generateId3 = originalGenerateId4,\n      currentDate = () => /* @__PURE__ */ new Date(),\n      now: now2 = now\n    } = {},\n    ...settings\n  } = options;\n  const enumValues = \"enum\" in options && options.enum ? options.enum : void 0;\n  const {\n    schema: inputSchema,\n    schemaDescription,\n    schemaName\n  } = \"schema\" in options ? options : {};\n  validateObjectGenerationInput({\n    output,\n    schema: inputSchema,\n    schemaName,\n    schemaDescription,\n    enumValues\n  });\n  const outputStrategy = getOutputStrategy({\n    output,\n    schema: inputSchema,\n    enumValues\n  });\n  return new DefaultStreamObjectResult({\n    model,\n    telemetry,\n    headers,\n    settings,\n    maxRetries,\n    abortSignal,\n    outputStrategy,\n    system,\n    prompt,\n    messages,\n    schemaName,\n    schemaDescription,\n    providerOptions,\n    repairText,\n    onError,\n    onFinish,\n    download: download2,\n    generateId: generateId3,\n    currentDate,\n    now: now2\n  });\n}\nvar DefaultStreamObjectResult = class {\n  constructor({\n    model: modelArg,\n    headers,\n    telemetry,\n    settings,\n    maxRetries: maxRetriesArg,\n    abortSignal,\n    outputStrategy,\n    system,\n    prompt,\n    messages,\n    schemaName,\n    schemaDescription,\n    providerOptions,\n    repairText,\n    onError,\n    onFinish,\n    download: download2,\n    generateId: generateId3,\n    currentDate,\n    now: now2\n  }) {\n    this._object = new DelayedPromise();\n    this._usage = new DelayedPromise();\n    this._providerMetadata = new DelayedPromise();\n    this._warnings = new DelayedPromise();\n    this._request = new DelayedPromise();\n    this._response = new DelayedPromise();\n    this._finishReason = new DelayedPromise();\n    const model = resolveLanguageModel(modelArg);\n    const { maxRetries, retry } = prepareRetries({\n      maxRetries: maxRetriesArg,\n      abortSignal\n    });\n    const callSettings = prepareCallSettings(settings);\n    const baseTelemetryAttributes = getBaseTelemetryAttributes({\n      model,\n      telemetry,\n      headers,\n      settings: { ...callSettings, maxRetries }\n    });\n    const tracer = getTracer(telemetry);\n    const self = this;\n    const stitchableStream = createStitchableStream();\n    const eventProcessor = new TransformStream({\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n        if (chunk.type === \"error\") {\n          onError({ error: wrapGatewayError(chunk.error) });\n        }\n      }\n    });\n    this.baseStream = stitchableStream.stream.pipeThrough(eventProcessor);\n    recordSpan({\n      name: \"ai.streamObject\",\n      attributes: selectTelemetryAttributes({\n        telemetry,\n        attributes: {\n          ...assembleOperationName({\n            operationId: \"ai.streamObject\",\n            telemetry\n          }),\n          ...baseTelemetryAttributes,\n          // specific settings that only make sense on the outer level:\n          \"ai.prompt\": {\n            input: () => JSON.stringify({ system, prompt, messages })\n          },\n          \"ai.schema\": outputStrategy.jsonSchema != null ? { input: () => JSON.stringify(outputStrategy.jsonSchema) } : void 0,\n          \"ai.schema.name\": schemaName,\n          \"ai.schema.description\": schemaDescription,\n          \"ai.settings.output\": outputStrategy.type\n        }\n      }),\n      tracer,\n      endWhenDone: false,\n      fn: async (rootSpan) => {\n        const standardizedPrompt = await standardizePrompt({\n          system,\n          prompt,\n          messages\n        });\n        const callOptions = {\n          responseFormat: {\n            type: \"json\",\n            schema: outputStrategy.jsonSchema,\n            name: schemaName,\n            description: schemaDescription\n          },\n          ...prepareCallSettings(settings),\n          prompt: await convertToLanguageModelPrompt({\n            prompt: standardizedPrompt,\n            supportedUrls: await model.supportedUrls,\n            download: download2\n          }),\n          providerOptions,\n          abortSignal,\n          headers,\n          includeRawChunks: false\n        };\n        const transformer = {\n          transform: (chunk, controller) => {\n            switch (chunk.type) {\n              case \"text-delta\":\n                controller.enqueue(chunk.delta);\n                break;\n              case \"response-metadata\":\n              case \"finish\":\n              case \"error\":\n              case \"stream-start\":\n                controller.enqueue(chunk);\n                break;\n            }\n          }\n        };\n        const {\n          result: { stream, response, request },\n          doStreamSpan,\n          startTimestampMs\n        } = await retry(\n          () => recordSpan({\n            name: \"ai.streamObject.doStream\",\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: \"ai.streamObject.doStream\",\n                  telemetry\n                }),\n                ...baseTelemetryAttributes,\n                \"ai.prompt.messages\": {\n                  input: () => stringifyForTelemetry(callOptions.prompt)\n                },\n                // standardized gen-ai llm span attributes:\n                \"gen_ai.system\": model.provider,\n                \"gen_ai.request.model\": model.modelId,\n                \"gen_ai.request.frequency_penalty\": callSettings.frequencyPenalty,\n                \"gen_ai.request.max_tokens\": callSettings.maxOutputTokens,\n                \"gen_ai.request.presence_penalty\": callSettings.presencePenalty,\n                \"gen_ai.request.temperature\": callSettings.temperature,\n                \"gen_ai.request.top_k\": callSettings.topK,\n                \"gen_ai.request.top_p\": callSettings.topP\n              }\n            }),\n            tracer,\n            endWhenDone: false,\n            fn: async (doStreamSpan2) => ({\n              startTimestampMs: now2(),\n              doStreamSpan: doStreamSpan2,\n              result: await model.doStream(callOptions)\n            })\n          })\n        );\n        self._request.resolve(request != null ? request : {});\n        let warnings;\n        let usage = {\n          inputTokens: void 0,\n          outputTokens: void 0,\n          totalTokens: void 0\n        };\n        let finishReason;\n        let providerMetadata;\n        let object2;\n        let error;\n        let accumulatedText = \"\";\n        let textDelta = \"\";\n        let fullResponse = {\n          id: generateId3(),\n          timestamp: currentDate(),\n          modelId: model.modelId\n        };\n        let latestObjectJson = void 0;\n        let latestObject = void 0;\n        let isFirstChunk = true;\n        let isFirstDelta = true;\n        const transformedStream = stream.pipeThrough(new TransformStream(transformer)).pipeThrough(\n          new TransformStream({\n            async transform(chunk, controller) {\n              var _a17, _b, _c;\n              if (typeof chunk === \"object\" && chunk.type === \"stream-start\") {\n                warnings = chunk.warnings;\n                return;\n              }\n              if (isFirstChunk) {\n                const msToFirstChunk = now2() - startTimestampMs;\n                isFirstChunk = false;\n                doStreamSpan.addEvent(\"ai.stream.firstChunk\", {\n                  \"ai.stream.msToFirstChunk\": msToFirstChunk\n                });\n                doStreamSpan.setAttributes({\n                  \"ai.stream.msToFirstChunk\": msToFirstChunk\n                });\n              }\n              if (typeof chunk === \"string\") {\n                accumulatedText += chunk;\n                textDelta += chunk;\n                const { value: currentObjectJson, state: parseState } = await parsePartialJson(accumulatedText);\n                if (currentObjectJson !== void 0 && !isDeepEqualData(latestObjectJson, currentObjectJson)) {\n                  const validationResult = await outputStrategy.validatePartialResult({\n                    value: currentObjectJson,\n                    textDelta,\n                    latestObject,\n                    isFirstDelta,\n                    isFinalDelta: parseState === \"successful-parse\"\n                  });\n                  if (validationResult.success && !isDeepEqualData(\n                    latestObject,\n                    validationResult.value.partial\n                  )) {\n                    latestObjectJson = currentObjectJson;\n                    latestObject = validationResult.value.partial;\n                    controller.enqueue({\n                      type: \"object\",\n                      object: latestObject\n                    });\n                    controller.enqueue({\n                      type: \"text-delta\",\n                      textDelta: validationResult.value.textDelta\n                    });\n                    textDelta = \"\";\n                    isFirstDelta = false;\n                  }\n                }\n                return;\n              }\n              switch (chunk.type) {\n                case \"response-metadata\": {\n                  fullResponse = {\n                    id: (_a17 = chunk.id) != null ? _a17 : fullResponse.id,\n                    timestamp: (_b = chunk.timestamp) != null ? _b : fullResponse.timestamp,\n                    modelId: (_c = chunk.modelId) != null ? _c : fullResponse.modelId\n                  };\n                  break;\n                }\n                case \"finish\": {\n                  if (textDelta !== \"\") {\n                    controller.enqueue({ type: \"text-delta\", textDelta });\n                  }\n                  finishReason = chunk.finishReason;\n                  usage = chunk.usage;\n                  providerMetadata = chunk.providerMetadata;\n                  controller.enqueue({\n                    ...chunk,\n                    usage,\n                    response: fullResponse\n                  });\n                  logWarnings(warnings != null ? warnings : []);\n                  self._usage.resolve(usage);\n                  self._providerMetadata.resolve(providerMetadata);\n                  self._warnings.resolve(warnings);\n                  self._response.resolve({\n                    ...fullResponse,\n                    headers: response == null ? void 0 : response.headers\n                  });\n                  self._finishReason.resolve(finishReason != null ? finishReason : \"unknown\");\n                  try {\n                    object2 = await parseAndValidateObjectResultWithRepair(\n                      accumulatedText,\n                      outputStrategy,\n                      repairText,\n                      {\n                        response: fullResponse,\n                        usage,\n                        finishReason\n                      }\n                    );\n                    self._object.resolve(object2);\n                  } catch (e) {\n                    error = e;\n                    self._object.reject(e);\n                  }\n                  break;\n                }\n                default: {\n                  controller.enqueue(chunk);\n                  break;\n                }\n              }\n            },\n            // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n            async flush(controller) {\n              try {\n                const finalUsage = usage != null ? usage : {\n                  promptTokens: NaN,\n                  completionTokens: NaN,\n                  totalTokens: NaN\n                };\n                doStreamSpan.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      \"ai.response.finishReason\": finishReason,\n                      \"ai.response.object\": {\n                        output: () => JSON.stringify(object2)\n                      },\n                      \"ai.response.id\": fullResponse.id,\n                      \"ai.response.model\": fullResponse.modelId,\n                      \"ai.response.timestamp\": fullResponse.timestamp.toISOString(),\n                      \"ai.response.providerMetadata\": JSON.stringify(providerMetadata),\n                      \"ai.usage.inputTokens\": finalUsage.inputTokens,\n                      \"ai.usage.outputTokens\": finalUsage.outputTokens,\n                      \"ai.usage.totalTokens\": finalUsage.totalTokens,\n                      \"ai.usage.reasoningTokens\": finalUsage.reasoningTokens,\n                      \"ai.usage.cachedInputTokens\": finalUsage.cachedInputTokens,\n                      // standardized gen-ai llm span attributes:\n                      \"gen_ai.response.finish_reasons\": [finishReason],\n                      \"gen_ai.response.id\": fullResponse.id,\n                      \"gen_ai.response.model\": fullResponse.modelId,\n                      \"gen_ai.usage.input_tokens\": finalUsage.inputTokens,\n                      \"gen_ai.usage.output_tokens\": finalUsage.outputTokens\n                    }\n                  })\n                );\n                doStreamSpan.end();\n                rootSpan.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      \"ai.usage.inputTokens\": finalUsage.inputTokens,\n                      \"ai.usage.outputTokens\": finalUsage.outputTokens,\n                      \"ai.usage.totalTokens\": finalUsage.totalTokens,\n                      \"ai.usage.reasoningTokens\": finalUsage.reasoningTokens,\n                      \"ai.usage.cachedInputTokens\": finalUsage.cachedInputTokens,\n                      \"ai.response.object\": {\n                        output: () => JSON.stringify(object2)\n                      },\n                      \"ai.response.providerMetadata\": JSON.stringify(providerMetadata)\n                    }\n                  })\n                );\n                await (onFinish == null ? void 0 : onFinish({\n                  usage: finalUsage,\n                  object: object2,\n                  error,\n                  response: {\n                    ...fullResponse,\n                    headers: response == null ? void 0 : response.headers\n                  },\n                  warnings,\n                  providerMetadata\n                }));\n              } catch (error2) {\n                controller.enqueue({ type: \"error\", error: error2 });\n              } finally {\n                rootSpan.end();\n              }\n            }\n          })\n        );\n        stitchableStream.addStream(transformedStream);\n      }\n    }).catch((error) => {\n      stitchableStream.addStream(\n        new ReadableStream({\n          start(controller) {\n            controller.enqueue({ type: \"error\", error });\n            controller.close();\n          }\n        })\n      );\n    }).finally(() => {\n      stitchableStream.close();\n    });\n    this.outputStrategy = outputStrategy;\n  }\n  get object() {\n    return this._object.promise;\n  }\n  get usage() {\n    return this._usage.promise;\n  }\n  get providerMetadata() {\n    return this._providerMetadata.promise;\n  }\n  get warnings() {\n    return this._warnings.promise;\n  }\n  get request() {\n    return this._request.promise;\n  }\n  get response() {\n    return this._response.promise;\n  }\n  get finishReason() {\n    return this._finishReason.promise;\n  }\n  get partialObjectStream() {\n    return createAsyncIterableStream(\n      this.baseStream.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            switch (chunk.type) {\n              case \"object\":\n                controller.enqueue(chunk.object);\n                break;\n              case \"text-delta\":\n              case \"finish\":\n              case \"error\":\n                break;\n              default: {\n                const _exhaustiveCheck = chunk;\n                throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n              }\n            }\n          }\n        })\n      )\n    );\n  }\n  get elementStream() {\n    return this.outputStrategy.createElementStream(this.baseStream);\n  }\n  get textStream() {\n    return createAsyncIterableStream(\n      this.baseStream.pipeThrough(\n        new TransformStream({\n          transform(chunk, controller) {\n            switch (chunk.type) {\n              case \"text-delta\":\n                controller.enqueue(chunk.textDelta);\n                break;\n              case \"object\":\n              case \"finish\":\n              case \"error\":\n                break;\n              default: {\n                const _exhaustiveCheck = chunk;\n                throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n              }\n            }\n          }\n        })\n      )\n    );\n  }\n  get fullStream() {\n    return createAsyncIterableStream(this.baseStream);\n  }\n  pipeTextStreamToResponse(response, init) {\n    pipeTextStreamToResponse({\n      response,\n      textStream: this.textStream,\n      ...init\n    });\n  }\n  toTextStreamResponse(init) {\n    return createTextStreamResponse({\n      textStream: this.textStream,\n      ...init\n    });\n  }\n};\n\n// src/error/no-speech-generated-error.ts\nimport { AISDKError as AISDKError20 } from \"@ai-sdk/provider\";\nvar NoSpeechGeneratedError = class extends AISDKError20 {\n  constructor(options) {\n    super({\n      name: \"AI_NoSpeechGeneratedError\",\n      message: \"No speech audio generated.\"\n    });\n    this.responses = options.responses;\n  }\n};\n\n// src/generate-speech/generated-audio-file.ts\nvar DefaultGeneratedAudioFile = class extends DefaultGeneratedFile {\n  constructor({\n    data,\n    mediaType\n  }) {\n    super({ data, mediaType });\n    let format = \"mp3\";\n    if (mediaType) {\n      const mediaTypeParts = mediaType.split(\"/\");\n      if (mediaTypeParts.length === 2) {\n        if (mediaType !== \"audio/mpeg\") {\n          format = mediaTypeParts[1];\n        }\n      }\n    }\n    if (!format) {\n      throw new Error(\n        \"Audio format must be provided or determinable from media type\"\n      );\n    }\n    this.format = format;\n  }\n};\n\n// src/generate-speech/generate-speech.ts\nasync function generateSpeech({\n  model,\n  text: text2,\n  voice,\n  outputFormat,\n  instructions,\n  speed,\n  language,\n  providerOptions = {},\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers\n}) {\n  var _a17;\n  if (model.specificationVersion !== \"v2\") {\n    throw new UnsupportedModelVersionError({\n      version: model.specificationVersion,\n      provider: model.provider,\n      modelId: model.modelId\n    });\n  }\n  const { retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const result = await retry(\n    () => model.doGenerate({\n      text: text2,\n      voice,\n      outputFormat,\n      instructions,\n      speed,\n      language,\n      abortSignal,\n      headers,\n      providerOptions\n    })\n  );\n  if (!result.audio || result.audio.length === 0) {\n    throw new NoSpeechGeneratedError({ responses: [result.response] });\n  }\n  logWarnings(result.warnings);\n  return new DefaultSpeechResult({\n    audio: new DefaultGeneratedAudioFile({\n      data: result.audio,\n      mediaType: (_a17 = detectMediaType({\n        data: result.audio,\n        signatures: audioMediaTypeSignatures\n      })) != null ? _a17 : \"audio/mp3\"\n    }),\n    warnings: result.warnings,\n    responses: [result.response],\n    providerMetadata: result.providerMetadata\n  });\n}\nvar DefaultSpeechResult = class {\n  constructor(options) {\n    var _a17;\n    this.audio = options.audio;\n    this.warnings = options.warnings;\n    this.responses = options.responses;\n    this.providerMetadata = (_a17 = options.providerMetadata) != null ? _a17 : {};\n  }\n};\n\n// src/generate-text/output.ts\nvar output_exports = {};\n__export(output_exports, {\n  object: () => object,\n  text: () => text\n});\nimport {\n  asSchema as asSchema4,\n  safeParseJSON as safeParseJSON4,\n  safeValidateTypes as safeValidateTypes4\n} from \"@ai-sdk/provider-utils\";\nvar text = () => ({\n  type: \"text\",\n  responseFormat: { type: \"text\" },\n  async parsePartial({ text: text2 }) {\n    return { partial: text2 };\n  },\n  async parseOutput({ text: text2 }) {\n    return text2;\n  }\n});\nvar object = ({\n  schema: inputSchema\n}) => {\n  const schema = asSchema4(inputSchema);\n  return {\n    type: \"object\",\n    responseFormat: {\n      type: \"json\",\n      schema: schema.jsonSchema\n    },\n    async parsePartial({ text: text2 }) {\n      const result = await parsePartialJson(text2);\n      switch (result.state) {\n        case \"failed-parse\":\n        case \"undefined-input\":\n          return void 0;\n        case \"repaired-parse\":\n        case \"successful-parse\":\n          return {\n            // Note: currently no validation of partial results:\n            partial: result.value\n          };\n        default: {\n          const _exhaustiveCheck = result.state;\n          throw new Error(`Unsupported parse state: ${_exhaustiveCheck}`);\n        }\n      }\n    },\n    async parseOutput({ text: text2 }, context) {\n      const parseResult = await safeParseJSON4({ text: text2 });\n      if (!parseResult.success) {\n        throw new NoObjectGeneratedError({\n          message: \"No object generated: could not parse the response.\",\n          cause: parseResult.error,\n          text: text2,\n          response: context.response,\n          usage: context.usage,\n          finishReason: context.finishReason\n        });\n      }\n      const validationResult = await safeValidateTypes4({\n        value: parseResult.value,\n        schema\n      });\n      if (!validationResult.success) {\n        throw new NoObjectGeneratedError({\n          message: \"No object generated: response did not match schema.\",\n          cause: validationResult.error,\n          text: text2,\n          response: context.response,\n          usage: context.usage,\n          finishReason: context.finishReason\n        });\n      }\n      return validationResult.value;\n    }\n  };\n};\n\n// src/generate-text/smooth-stream.ts\nimport { delay as originalDelay } from \"@ai-sdk/provider-utils\";\nimport { InvalidArgumentError as InvalidArgumentError2 } from \"@ai-sdk/provider\";\nvar CHUNKING_REGEXPS = {\n  word: /\\S+\\s+/m,\n  line: /\\n+/m\n};\nfunction smoothStream({\n  delayInMs = 10,\n  chunking = \"word\",\n  _internal: { delay: delay2 = originalDelay } = {}\n} = {}) {\n  let detectChunk;\n  if (typeof chunking === \"function\") {\n    detectChunk = (buffer) => {\n      const match = chunking(buffer);\n      if (match == null) {\n        return null;\n      }\n      if (!match.length) {\n        throw new Error(`Chunking function must return a non-empty string.`);\n      }\n      if (!buffer.startsWith(match)) {\n        throw new Error(\n          `Chunking function must return a match that is a prefix of the buffer. Received: \"${match}\" expected to start with \"${buffer}\"`\n        );\n      }\n      return match;\n    };\n  } else {\n    const chunkingRegex = typeof chunking === \"string\" ? CHUNKING_REGEXPS[chunking] : chunking;\n    if (chunkingRegex == null) {\n      throw new InvalidArgumentError2({\n        argument: \"chunking\",\n        message: `Chunking must be \"word\" or \"line\" or a RegExp. Received: ${chunking}`\n      });\n    }\n    detectChunk = (buffer) => {\n      const match = chunkingRegex.exec(buffer);\n      if (!match) {\n        return null;\n      }\n      return buffer.slice(0, match.index) + (match == null ? void 0 : match[0]);\n    };\n  }\n  return () => {\n    let buffer = \"\";\n    let id = \"\";\n    return new TransformStream({\n      async transform(chunk, controller) {\n        if (chunk.type !== \"text-delta\") {\n          if (buffer.length > 0) {\n            controller.enqueue({ type: \"text-delta\", text: buffer, id });\n            buffer = \"\";\n          }\n          controller.enqueue(chunk);\n          return;\n        }\n        if (chunk.id !== id && buffer.length > 0) {\n          controller.enqueue({ type: \"text-delta\", text: buffer, id });\n          buffer = \"\";\n        }\n        buffer += chunk.text;\n        id = chunk.id;\n        let match;\n        while ((match = detectChunk(buffer)) != null) {\n          controller.enqueue({ type: \"text-delta\", text: match, id });\n          buffer = buffer.slice(match.length);\n          await delay2(delayInMs);\n        }\n      }\n    });\n  };\n}\n\n// src/middleware/default-settings-middleware.ts\nfunction defaultSettingsMiddleware({\n  settings\n}) {\n  return {\n    middlewareVersion: \"v2\",\n    transformParams: async ({ params }) => {\n      return mergeObjects(settings, params);\n    }\n  };\n}\n\n// src/util/get-potential-start-index.ts\nfunction getPotentialStartIndex(text2, searchedText) {\n  if (searchedText.length === 0) {\n    return null;\n  }\n  const directIndex = text2.indexOf(searchedText);\n  if (directIndex !== -1) {\n    return directIndex;\n  }\n  for (let i = text2.length - 1; i >= 0; i--) {\n    const suffix = text2.substring(i);\n    if (searchedText.startsWith(suffix)) {\n      return i;\n    }\n  }\n  return null;\n}\n\n// src/middleware/extract-reasoning-middleware.ts\nfunction extractReasoningMiddleware({\n  tagName,\n  separator = \"\\n\",\n  startWithReasoning = false\n}) {\n  const openingTag = `<${tagName}>`;\n  const closingTag = `</${tagName}>`;\n  return {\n    middlewareVersion: \"v2\",\n    wrapGenerate: async ({ doGenerate }) => {\n      const { content, ...rest } = await doGenerate();\n      const transformedContent = [];\n      for (const part of content) {\n        if (part.type !== \"text\") {\n          transformedContent.push(part);\n          continue;\n        }\n        const text2 = startWithReasoning ? openingTag + part.text : part.text;\n        const regexp = new RegExp(`${openingTag}(.*?)${closingTag}`, \"gs\");\n        const matches = Array.from(text2.matchAll(regexp));\n        if (!matches.length) {\n          transformedContent.push(part);\n          continue;\n        }\n        const reasoningText = matches.map((match) => match[1]).join(separator);\n        let textWithoutReasoning = text2;\n        for (let i = matches.length - 1; i >= 0; i--) {\n          const match = matches[i];\n          const beforeMatch = textWithoutReasoning.slice(0, match.index);\n          const afterMatch = textWithoutReasoning.slice(\n            match.index + match[0].length\n          );\n          textWithoutReasoning = beforeMatch + (beforeMatch.length > 0 && afterMatch.length > 0 ? separator : \"\") + afterMatch;\n        }\n        transformedContent.push({\n          type: \"reasoning\",\n          text: reasoningText\n        });\n        transformedContent.push({\n          type: \"text\",\n          text: textWithoutReasoning\n        });\n      }\n      return { content: transformedContent, ...rest };\n    },\n    wrapStream: async ({ doStream }) => {\n      const { stream, ...rest } = await doStream();\n      const reasoningExtractions = {};\n      let delayedTextStart;\n      return {\n        stream: stream.pipeThrough(\n          new TransformStream({\n            transform: (chunk, controller) => {\n              if (chunk.type === \"text-start\") {\n                delayedTextStart = chunk;\n                return;\n              }\n              if (chunk.type === \"text-end\" && delayedTextStart) {\n                controller.enqueue(delayedTextStart);\n                delayedTextStart = void 0;\n              }\n              if (chunk.type !== \"text-delta\") {\n                controller.enqueue(chunk);\n                return;\n              }\n              if (reasoningExtractions[chunk.id] == null) {\n                reasoningExtractions[chunk.id] = {\n                  isFirstReasoning: true,\n                  isFirstText: true,\n                  afterSwitch: false,\n                  isReasoning: startWithReasoning,\n                  buffer: \"\",\n                  idCounter: 0,\n                  textId: chunk.id\n                };\n              }\n              const activeExtraction = reasoningExtractions[chunk.id];\n              activeExtraction.buffer += chunk.delta;\n              function publish(text2) {\n                if (text2.length > 0) {\n                  const prefix = activeExtraction.afterSwitch && (activeExtraction.isReasoning ? !activeExtraction.isFirstReasoning : !activeExtraction.isFirstText) ? separator : \"\";\n                  if (activeExtraction.isReasoning && (activeExtraction.afterSwitch || activeExtraction.isFirstReasoning)) {\n                    controller.enqueue({\n                      type: \"reasoning-start\",\n                      id: `reasoning-${activeExtraction.idCounter}`\n                    });\n                  }\n                  if (activeExtraction.isReasoning) {\n                    controller.enqueue({\n                      type: \"reasoning-delta\",\n                      delta: prefix + text2,\n                      id: `reasoning-${activeExtraction.idCounter}`\n                    });\n                  } else {\n                    if (delayedTextStart) {\n                      controller.enqueue(delayedTextStart);\n                      delayedTextStart = void 0;\n                    }\n                    controller.enqueue({\n                      type: \"text-delta\",\n                      delta: prefix + text2,\n                      id: activeExtraction.textId\n                    });\n                  }\n                  activeExtraction.afterSwitch = false;\n                  if (activeExtraction.isReasoning) {\n                    activeExtraction.isFirstReasoning = false;\n                  } else {\n                    activeExtraction.isFirstText = false;\n                  }\n                }\n              }\n              do {\n                const nextTag = activeExtraction.isReasoning ? closingTag : openingTag;\n                const startIndex = getPotentialStartIndex(\n                  activeExtraction.buffer,\n                  nextTag\n                );\n                if (startIndex == null) {\n                  publish(activeExtraction.buffer);\n                  activeExtraction.buffer = \"\";\n                  break;\n                }\n                publish(activeExtraction.buffer.slice(0, startIndex));\n                const foundFullMatch = startIndex + nextTag.length <= activeExtraction.buffer.length;\n                if (foundFullMatch) {\n                  activeExtraction.buffer = activeExtraction.buffer.slice(\n                    startIndex + nextTag.length\n                  );\n                  if (activeExtraction.isReasoning) {\n                    controller.enqueue({\n                      type: \"reasoning-end\",\n                      id: `reasoning-${activeExtraction.idCounter++}`\n                    });\n                  }\n                  activeExtraction.isReasoning = !activeExtraction.isReasoning;\n                  activeExtraction.afterSwitch = true;\n                } else {\n                  activeExtraction.buffer = activeExtraction.buffer.slice(startIndex);\n                  break;\n                }\n              } while (true);\n            }\n          })\n        ),\n        ...rest\n      };\n    }\n  };\n}\n\n// src/middleware/simulate-streaming-middleware.ts\nfunction simulateStreamingMiddleware() {\n  return {\n    middlewareVersion: \"v2\",\n    wrapStream: async ({ doGenerate }) => {\n      const result = await doGenerate();\n      let id = 0;\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({\n            type: \"stream-start\",\n            warnings: result.warnings\n          });\n          controller.enqueue({ type: \"response-metadata\", ...result.response });\n          for (const part of result.content) {\n            switch (part.type) {\n              case \"text\": {\n                if (part.text.length > 0) {\n                  controller.enqueue({ type: \"text-start\", id: String(id) });\n                  controller.enqueue({\n                    type: \"text-delta\",\n                    id: String(id),\n                    delta: part.text\n                  });\n                  controller.enqueue({ type: \"text-end\", id: String(id) });\n                  id++;\n                }\n                break;\n              }\n              case \"reasoning\": {\n                controller.enqueue({\n                  type: \"reasoning-start\",\n                  id: String(id),\n                  providerMetadata: part.providerMetadata\n                });\n                controller.enqueue({\n                  type: \"reasoning-delta\",\n                  id: String(id),\n                  delta: part.text\n                });\n                controller.enqueue({ type: \"reasoning-end\", id: String(id) });\n                id++;\n                break;\n              }\n              default: {\n                controller.enqueue(part);\n                break;\n              }\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        request: result.request,\n        response: result.response\n      };\n    }\n  };\n}\n\n// src/middleware/wrap-language-model.ts\nvar wrapLanguageModel = ({\n  model,\n  middleware: middlewareArg,\n  modelId,\n  providerId\n}) => {\n  return asArray(middlewareArg).reverse().reduce((wrappedModel, middleware) => {\n    return doWrap({ model: wrappedModel, middleware, modelId, providerId });\n  }, model);\n};\nvar doWrap = ({\n  model,\n  middleware: {\n    transformParams,\n    wrapGenerate,\n    wrapStream,\n    overrideProvider,\n    overrideModelId,\n    overrideSupportedUrls\n  },\n  modelId,\n  providerId\n}) => {\n  var _a17, _b, _c;\n  async function doTransform({\n    params,\n    type\n  }) {\n    return transformParams ? await transformParams({ params, type, model }) : params;\n  }\n  return {\n    specificationVersion: \"v2\",\n    provider: (_a17 = providerId != null ? providerId : overrideProvider == null ? void 0 : overrideProvider({ model })) != null ? _a17 : model.provider,\n    modelId: (_b = modelId != null ? modelId : overrideModelId == null ? void 0 : overrideModelId({ model })) != null ? _b : model.modelId,\n    supportedUrls: (_c = overrideSupportedUrls == null ? void 0 : overrideSupportedUrls({ model })) != null ? _c : model.supportedUrls,\n    async doGenerate(params) {\n      const transformedParams = await doTransform({ params, type: \"generate\" });\n      const doGenerate = async () => model.doGenerate(transformedParams);\n      const doStream = async () => model.doStream(transformedParams);\n      return wrapGenerate ? wrapGenerate({\n        doGenerate,\n        doStream,\n        params: transformedParams,\n        model\n      }) : doGenerate();\n    },\n    async doStream(params) {\n      const transformedParams = await doTransform({ params, type: \"stream\" });\n      const doGenerate = async () => model.doGenerate(transformedParams);\n      const doStream = async () => model.doStream(transformedParams);\n      return wrapStream ? wrapStream({ doGenerate, doStream, params: transformedParams, model }) : doStream();\n    }\n  };\n};\n\n// src/middleware/wrap-provider.ts\nfunction wrapProvider({\n  provider,\n  languageModelMiddleware\n}) {\n  const wrappedProvider = {\n    languageModel(modelId) {\n      let model = provider.languageModel(modelId);\n      model = wrapLanguageModel({\n        model,\n        middleware: languageModelMiddleware\n      });\n      return model;\n    },\n    textEmbeddingModel: provider.textEmbeddingModel,\n    imageModel: provider.imageModel,\n    transcriptionModel: provider.transcriptionModel,\n    speechModel: provider.speechModel\n  };\n  return wrappedProvider;\n}\n\n// src/registry/custom-provider.ts\nimport {\n  NoSuchModelError as NoSuchModelError2\n} from \"@ai-sdk/provider\";\nfunction customProvider({\n  languageModels,\n  textEmbeddingModels,\n  imageModels,\n  transcriptionModels,\n  speechModels,\n  fallbackProvider\n}) {\n  return {\n    languageModel(modelId) {\n      if (languageModels != null && modelId in languageModels) {\n        return languageModels[modelId];\n      }\n      if (fallbackProvider) {\n        return fallbackProvider.languageModel(modelId);\n      }\n      throw new NoSuchModelError2({ modelId, modelType: \"languageModel\" });\n    },\n    textEmbeddingModel(modelId) {\n      if (textEmbeddingModels != null && modelId in textEmbeddingModels) {\n        return textEmbeddingModels[modelId];\n      }\n      if (fallbackProvider) {\n        return fallbackProvider.textEmbeddingModel(modelId);\n      }\n      throw new NoSuchModelError2({ modelId, modelType: \"textEmbeddingModel\" });\n    },\n    imageModel(modelId) {\n      if (imageModels != null && modelId in imageModels) {\n        return imageModels[modelId];\n      }\n      if (fallbackProvider == null ? void 0 : fallbackProvider.imageModel) {\n        return fallbackProvider.imageModel(modelId);\n      }\n      throw new NoSuchModelError2({ modelId, modelType: \"imageModel\" });\n    },\n    transcriptionModel(modelId) {\n      if (transcriptionModels != null && modelId in transcriptionModels) {\n        return transcriptionModels[modelId];\n      }\n      if (fallbackProvider == null ? void 0 : fallbackProvider.transcriptionModel) {\n        return fallbackProvider.transcriptionModel(modelId);\n      }\n      throw new NoSuchModelError2({ modelId, modelType: \"transcriptionModel\" });\n    },\n    speechModel(modelId) {\n      if (speechModels != null && modelId in speechModels) {\n        return speechModels[modelId];\n      }\n      if (fallbackProvider == null ? void 0 : fallbackProvider.speechModel) {\n        return fallbackProvider.speechModel(modelId);\n      }\n      throw new NoSuchModelError2({ modelId, modelType: \"speechModel\" });\n    }\n  };\n}\nvar experimental_customProvider = customProvider;\n\n// src/registry/no-such-provider-error.ts\nimport { AISDKError as AISDKError21, NoSuchModelError as NoSuchModelError3 } from \"@ai-sdk/provider\";\nvar name16 = \"AI_NoSuchProviderError\";\nvar marker16 = `vercel.ai.error.${name16}`;\nvar symbol16 = Symbol.for(marker16);\nvar _a16;\nvar NoSuchProviderError = class extends NoSuchModelError3 {\n  constructor({\n    modelId,\n    modelType,\n    providerId,\n    availableProviders,\n    message = `No such provider: ${providerId} (available providers: ${availableProviders.join()})`\n  }) {\n    super({ errorName: name16, modelId, modelType, message });\n    this[_a16] = true;\n    this.providerId = providerId;\n    this.availableProviders = availableProviders;\n  }\n  static isInstance(error) {\n    return AISDKError21.hasMarker(error, marker16);\n  }\n};\n_a16 = symbol16;\n\n// src/registry/provider-registry.ts\nimport {\n  NoSuchModelError as NoSuchModelError4\n} from \"@ai-sdk/provider\";\nfunction createProviderRegistry(providers, {\n  separator = \":\",\n  languageModelMiddleware\n} = {}) {\n  const registry = new DefaultProviderRegistry({\n    separator,\n    languageModelMiddleware\n  });\n  for (const [id, provider] of Object.entries(providers)) {\n    registry.registerProvider({ id, provider });\n  }\n  return registry;\n}\nvar experimental_createProviderRegistry = createProviderRegistry;\nvar DefaultProviderRegistry = class {\n  constructor({\n    separator,\n    languageModelMiddleware\n  }) {\n    this.providers = {};\n    this.separator = separator;\n    this.languageModelMiddleware = languageModelMiddleware;\n  }\n  registerProvider({\n    id,\n    provider\n  }) {\n    this.providers[id] = provider;\n  }\n  getProvider(id, modelType) {\n    const provider = this.providers[id];\n    if (provider == null) {\n      throw new NoSuchProviderError({\n        modelId: id,\n        modelType,\n        providerId: id,\n        availableProviders: Object.keys(this.providers)\n      });\n    }\n    return provider;\n  }\n  splitId(id, modelType) {\n    const index = id.indexOf(this.separator);\n    if (index === -1) {\n      throw new NoSuchModelError4({\n        modelId: id,\n        modelType,\n        message: `Invalid ${modelType} id for registry: ${id} (must be in the format \"providerId${this.separator}modelId\")`\n      });\n    }\n    return [id.slice(0, index), id.slice(index + this.separator.length)];\n  }\n  languageModel(id) {\n    var _a17, _b;\n    const [providerId, modelId] = this.splitId(id, \"languageModel\");\n    let model = (_b = (_a17 = this.getProvider(providerId, \"languageModel\")).languageModel) == null ? void 0 : _b.call(\n      _a17,\n      modelId\n    );\n    if (model == null) {\n      throw new NoSuchModelError4({ modelId: id, modelType: \"languageModel\" });\n    }\n    if (this.languageModelMiddleware != null) {\n      model = wrapLanguageModel({\n        model,\n        middleware: this.languageModelMiddleware\n      });\n    }\n    return model;\n  }\n  textEmbeddingModel(id) {\n    var _a17;\n    const [providerId, modelId] = this.splitId(id, \"textEmbeddingModel\");\n    const provider = this.getProvider(providerId, \"textEmbeddingModel\");\n    const model = (_a17 = provider.textEmbeddingModel) == null ? void 0 : _a17.call(provider, modelId);\n    if (model == null) {\n      throw new NoSuchModelError4({\n        modelId: id,\n        modelType: \"textEmbeddingModel\"\n      });\n    }\n    return model;\n  }\n  imageModel(id) {\n    var _a17;\n    const [providerId, modelId] = this.splitId(id, \"imageModel\");\n    const provider = this.getProvider(providerId, \"imageModel\");\n    const model = (_a17 = provider.imageModel) == null ? void 0 : _a17.call(provider, modelId);\n    if (model == null) {\n      throw new NoSuchModelError4({ modelId: id, modelType: \"imageModel\" });\n    }\n    return model;\n  }\n  transcriptionModel(id) {\n    var _a17;\n    const [providerId, modelId] = this.splitId(id, \"transcriptionModel\");\n    const provider = this.getProvider(providerId, \"transcriptionModel\");\n    const model = (_a17 = provider.transcriptionModel) == null ? void 0 : _a17.call(provider, modelId);\n    if (model == null) {\n      throw new NoSuchModelError4({\n        modelId: id,\n        modelType: \"transcriptionModel\"\n      });\n    }\n    return model;\n  }\n  speechModel(id) {\n    var _a17;\n    const [providerId, modelId] = this.splitId(id, \"speechModel\");\n    const provider = this.getProvider(providerId, \"speechModel\");\n    const model = (_a17 = provider.speechModel) == null ? void 0 : _a17.call(provider, modelId);\n    if (model == null) {\n      throw new NoSuchModelError4({ modelId: id, modelType: \"speechModel\" });\n    }\n    return model;\n  }\n};\n\n// src/tool/mcp/mcp-client.ts\nimport {\n  dynamicTool,\n  jsonSchema,\n  tool\n} from \"@ai-sdk/provider-utils\";\n\n// src/tool/mcp/mcp-sse-transport.ts\nimport { EventSourceParserStream } from \"@ai-sdk/provider-utils\";\n\n// src/tool/mcp/json-rpc-message.ts\nimport { z as z9 } from \"zod/v4\";\n\n// src/tool/mcp/types.ts\nimport { z as z8 } from \"zod/v4\";\nvar LATEST_PROTOCOL_VERSION = \"2025-06-18\";\nvar SUPPORTED_PROTOCOL_VERSIONS = [\n  LATEST_PROTOCOL_VERSION,\n  \"2025-03-26\",\n  \"2024-11-05\"\n];\nvar ClientOrServerImplementationSchema = z8.looseObject({\n  name: z8.string(),\n  version: z8.string()\n});\nvar BaseParamsSchema = z8.looseObject({\n  _meta: z8.optional(z8.object({}).loose())\n});\nvar ResultSchema = BaseParamsSchema;\nvar RequestSchema = z8.object({\n  method: z8.string(),\n  params: z8.optional(BaseParamsSchema)\n});\nvar ServerCapabilitiesSchema = z8.looseObject({\n  experimental: z8.optional(z8.object({}).loose()),\n  logging: z8.optional(z8.object({}).loose()),\n  prompts: z8.optional(\n    z8.looseObject({\n      listChanged: z8.optional(z8.boolean())\n    })\n  ),\n  resources: z8.optional(\n    z8.looseObject({\n      subscribe: z8.optional(z8.boolean()),\n      listChanged: z8.optional(z8.boolean())\n    })\n  ),\n  tools: z8.optional(\n    z8.looseObject({\n      listChanged: z8.optional(z8.boolean())\n    })\n  )\n});\nvar InitializeResultSchema = ResultSchema.extend({\n  protocolVersion: z8.string(),\n  capabilities: ServerCapabilitiesSchema,\n  serverInfo: ClientOrServerImplementationSchema,\n  instructions: z8.optional(z8.string())\n});\nvar PaginatedResultSchema = ResultSchema.extend({\n  nextCursor: z8.optional(z8.string())\n});\nvar ToolSchema = z8.object({\n  name: z8.string(),\n  description: z8.optional(z8.string()),\n  inputSchema: z8.object({\n    type: z8.literal(\"object\"),\n    properties: z8.optional(z8.object({}).loose())\n  }).loose()\n}).loose();\nvar ListToolsResultSchema = PaginatedResultSchema.extend({\n  tools: z8.array(ToolSchema)\n});\nvar TextContentSchema = z8.object({\n  type: z8.literal(\"text\"),\n  text: z8.string()\n}).loose();\nvar ImageContentSchema = z8.object({\n  type: z8.literal(\"image\"),\n  data: z8.base64(),\n  mimeType: z8.string()\n}).loose();\nvar ResourceContentsSchema = z8.object({\n  /**\n   * The URI of this resource.\n   */\n  uri: z8.string(),\n  /**\n   * The MIME type of this resource, if known.\n   */\n  mimeType: z8.optional(z8.string())\n}).loose();\nvar TextResourceContentsSchema = ResourceContentsSchema.extend({\n  text: z8.string()\n});\nvar BlobResourceContentsSchema = ResourceContentsSchema.extend({\n  blob: z8.base64()\n});\nvar EmbeddedResourceSchema = z8.object({\n  type: z8.literal(\"resource\"),\n  resource: z8.union([TextResourceContentsSchema, BlobResourceContentsSchema])\n}).loose();\nvar CallToolResultSchema = ResultSchema.extend({\n  content: z8.array(\n    z8.union([TextContentSchema, ImageContentSchema, EmbeddedResourceSchema])\n  ),\n  isError: z8.boolean().default(false).optional()\n}).or(\n  ResultSchema.extend({\n    toolResult: z8.unknown()\n  })\n);\n\n// src/tool/mcp/json-rpc-message.ts\nvar JSONRPC_VERSION = \"2.0\";\nvar JSONRPCRequestSchema = z9.object({\n  jsonrpc: z9.literal(JSONRPC_VERSION),\n  id: z9.union([z9.string(), z9.number().int()])\n}).merge(RequestSchema).strict();\nvar JSONRPCResponseSchema = z9.object({\n  jsonrpc: z9.literal(JSONRPC_VERSION),\n  id: z9.union([z9.string(), z9.number().int()]),\n  result: ResultSchema\n}).strict();\nvar JSONRPCErrorSchema = z9.object({\n  jsonrpc: z9.literal(JSONRPC_VERSION),\n  id: z9.union([z9.string(), z9.number().int()]),\n  error: z9.object({\n    code: z9.number().int(),\n    message: z9.string(),\n    data: z9.optional(z9.unknown())\n  })\n}).strict();\nvar JSONRPCNotificationSchema = z9.object({\n  jsonrpc: z9.literal(JSONRPC_VERSION)\n}).merge(\n  z9.object({\n    method: z9.string(),\n    params: z9.optional(BaseParamsSchema)\n  })\n).strict();\nvar JSONRPCMessageSchema = z9.union([\n  JSONRPCRequestSchema,\n  JSONRPCNotificationSchema,\n  JSONRPCResponseSchema,\n  JSONRPCErrorSchema\n]);\n\n// src/tool/mcp/mcp-sse-transport.ts\nvar SseMCPTransport = class {\n  constructor({\n    url,\n    headers\n  }) {\n    this.connected = false;\n    this.url = new URL(url);\n    this.headers = headers;\n  }\n  async start() {\n    return new Promise((resolve2, reject) => {\n      if (this.connected) {\n        return resolve2();\n      }\n      this.abortController = new AbortController();\n      const establishConnection = async () => {\n        var _a17, _b, _c;\n        try {\n          const headers = new Headers(this.headers);\n          headers.set(\"Accept\", \"text/event-stream\");\n          const response = await fetch(this.url.href, {\n            headers,\n            signal: (_a17 = this.abortController) == null ? void 0 : _a17.signal\n          });\n          if (!response.ok || !response.body) {\n            const error = new MCPClientError({\n              message: `MCP SSE Transport Error: ${response.status} ${response.statusText}`\n            });\n            (_b = this.onerror) == null ? void 0 : _b.call(this, error);\n            return reject(error);\n          }\n          const stream = response.body.pipeThrough(new TextDecoderStream()).pipeThrough(new EventSourceParserStream());\n          const reader = stream.getReader();\n          const processEvents = async () => {\n            var _a18, _b2, _c2;\n            try {\n              while (true) {\n                const { done, value } = await reader.read();\n                if (done) {\n                  if (this.connected) {\n                    this.connected = false;\n                    throw new MCPClientError({\n                      message: \"MCP SSE Transport Error: Connection closed unexpectedly\"\n                    });\n                  }\n                  return;\n                }\n                const { event, data } = value;\n                if (event === \"endpoint\") {\n                  this.endpoint = new URL(data, this.url);\n                  if (this.endpoint.origin !== this.url.origin) {\n                    throw new MCPClientError({\n                      message: `MCP SSE Transport Error: Endpoint origin does not match connection origin: ${this.endpoint.origin}`\n                    });\n                  }\n                  this.connected = true;\n                  resolve2();\n                } else if (event === \"message\") {\n                  try {\n                    const message = JSONRPCMessageSchema.parse(\n                      JSON.parse(data)\n                    );\n                    (_a18 = this.onmessage) == null ? void 0 : _a18.call(this, message);\n                  } catch (error) {\n                    const e = new MCPClientError({\n                      message: \"MCP SSE Transport Error: Failed to parse message\",\n                      cause: error\n                    });\n                    (_b2 = this.onerror) == null ? void 0 : _b2.call(this, e);\n                  }\n                }\n              }\n            } catch (error) {\n              if (error instanceof Error && error.name === \"AbortError\") {\n                return;\n              }\n              (_c2 = this.onerror) == null ? void 0 : _c2.call(this, error);\n              reject(error);\n            }\n          };\n          this.sseConnection = {\n            close: () => reader.cancel()\n          };\n          processEvents();\n        } catch (error) {\n          if (error instanceof Error && error.name === \"AbortError\") {\n            return;\n          }\n          (_c = this.onerror) == null ? void 0 : _c.call(this, error);\n          reject(error);\n        }\n      };\n      establishConnection();\n    });\n  }\n  async close() {\n    var _a17, _b, _c;\n    this.connected = false;\n    (_a17 = this.sseConnection) == null ? void 0 : _a17.close();\n    (_b = this.abortController) == null ? void 0 : _b.abort();\n    (_c = this.onclose) == null ? void 0 : _c.call(this);\n  }\n  async send(message) {\n    var _a17, _b, _c;\n    if (!this.endpoint || !this.connected) {\n      throw new MCPClientError({\n        message: \"MCP SSE Transport Error: Not connected\"\n      });\n    }\n    try {\n      const headers = new Headers(this.headers);\n      headers.set(\"Content-Type\", \"application/json\");\n      const init = {\n        method: \"POST\",\n        headers,\n        body: JSON.stringify(message),\n        signal: (_a17 = this.abortController) == null ? void 0 : _a17.signal\n      };\n      const response = await fetch(this.endpoint, init);\n      if (!response.ok) {\n        const text2 = await response.text().catch(() => null);\n        const error = new MCPClientError({\n          message: `MCP SSE Transport Error: POSTing to endpoint (HTTP ${response.status}): ${text2}`\n        });\n        (_b = this.onerror) == null ? void 0 : _b.call(this, error);\n        return;\n      }\n    } catch (error) {\n      (_c = this.onerror) == null ? void 0 : _c.call(this, error);\n      return;\n    }\n  }\n};\n\n// src/tool/mcp/mcp-transport.ts\nfunction createMcpTransport(config) {\n  if (config.type !== \"sse\") {\n    throw new MCPClientError({\n      message: \"Unsupported or invalid transport configuration. If you are using a custom transport, make sure it implements the MCPTransport interface.\"\n    });\n  }\n  return new SseMCPTransport(config);\n}\nfunction isCustomMcpTransport(transport) {\n  return \"start\" in transport && typeof transport.start === \"function\" && \"send\" in transport && typeof transport.send === \"function\" && \"close\" in transport && typeof transport.close === \"function\";\n}\n\n// src/tool/mcp/mcp-client.ts\nvar CLIENT_VERSION = \"1.0.0\";\nasync function createMCPClient(config) {\n  const client = new DefaultMCPClient(config);\n  await client.init();\n  return client;\n}\nvar DefaultMCPClient = class {\n  constructor({\n    transport: transportConfig,\n    name: name17 = \"ai-sdk-mcp-client\",\n    onUncaughtError\n  }) {\n    this.requestMessageId = 0;\n    this.responseHandlers = /* @__PURE__ */ new Map();\n    this.serverCapabilities = {};\n    this.isClosed = true;\n    this.onUncaughtError = onUncaughtError;\n    if (isCustomMcpTransport(transportConfig)) {\n      this.transport = transportConfig;\n    } else {\n      this.transport = createMcpTransport(transportConfig);\n    }\n    this.transport.onclose = () => this.onClose();\n    this.transport.onerror = (error) => this.onError(error);\n    this.transport.onmessage = (message) => {\n      if (\"method\" in message) {\n        this.onError(\n          new MCPClientError({\n            message: \"Unsupported message type\"\n          })\n        );\n        return;\n      }\n      this.onResponse(message);\n    };\n    this.clientInfo = {\n      name: name17,\n      version: CLIENT_VERSION\n    };\n  }\n  async init() {\n    try {\n      await this.transport.start();\n      this.isClosed = false;\n      const result = await this.request({\n        request: {\n          method: \"initialize\",\n          params: {\n            protocolVersion: LATEST_PROTOCOL_VERSION,\n            capabilities: {},\n            clientInfo: this.clientInfo\n          }\n        },\n        resultSchema: InitializeResultSchema\n      });\n      if (result === void 0) {\n        throw new MCPClientError({\n          message: \"Server sent invalid initialize result\"\n        });\n      }\n      if (!SUPPORTED_PROTOCOL_VERSIONS.includes(result.protocolVersion)) {\n        throw new MCPClientError({\n          message: `Server's protocol version is not supported: ${result.protocolVersion}`\n        });\n      }\n      this.serverCapabilities = result.capabilities;\n      await this.notification({\n        method: \"notifications/initialized\"\n      });\n      return this;\n    } catch (error) {\n      await this.close();\n      throw error;\n    }\n  }\n  async close() {\n    var _a17;\n    if (this.isClosed)\n      return;\n    await ((_a17 = this.transport) == null ? void 0 : _a17.close());\n    this.onClose();\n  }\n  assertCapability(method) {\n    switch (method) {\n      case \"initialize\":\n        break;\n      case \"tools/list\":\n      case \"tools/call\":\n        if (!this.serverCapabilities.tools) {\n          throw new MCPClientError({\n            message: `Server does not support tools`\n          });\n        }\n        break;\n      default:\n        throw new MCPClientError({\n          message: `Unsupported method: ${method}`\n        });\n    }\n  }\n  async request({\n    request,\n    resultSchema,\n    options\n  }) {\n    return new Promise((resolve2, reject) => {\n      if (this.isClosed) {\n        return reject(\n          new MCPClientError({\n            message: \"Attempted to send a request from a closed client\"\n          })\n        );\n      }\n      this.assertCapability(request.method);\n      const signal = options == null ? void 0 : options.signal;\n      signal == null ? void 0 : signal.throwIfAborted();\n      const messageId = this.requestMessageId++;\n      const jsonrpcRequest = {\n        ...request,\n        jsonrpc: \"2.0\",\n        id: messageId\n      };\n      const cleanup = () => {\n        this.responseHandlers.delete(messageId);\n      };\n      this.responseHandlers.set(messageId, (response) => {\n        if (signal == null ? void 0 : signal.aborted) {\n          return reject(\n            new MCPClientError({\n              message: \"Request was aborted\",\n              cause: signal.reason\n            })\n          );\n        }\n        if (response instanceof Error) {\n          return reject(response);\n        }\n        try {\n          const result = resultSchema.parse(response.result);\n          resolve2(result);\n        } catch (error) {\n          const parseError = new MCPClientError({\n            message: \"Failed to parse server response\",\n            cause: error\n          });\n          reject(parseError);\n        }\n      });\n      this.transport.send(jsonrpcRequest).catch((error) => {\n        cleanup();\n        reject(error);\n      });\n    });\n  }\n  async listTools({\n    params,\n    options\n  } = {}) {\n    try {\n      return this.request({\n        request: { method: \"tools/list\", params },\n        resultSchema: ListToolsResultSchema,\n        options\n      });\n    } catch (error) {\n      throw error;\n    }\n  }\n  async callTool({\n    name: name17,\n    args,\n    options\n  }) {\n    try {\n      return this.request({\n        request: { method: \"tools/call\", params: { name: name17, arguments: args } },\n        resultSchema: CallToolResultSchema,\n        options: {\n          signal: options == null ? void 0 : options.abortSignal\n        }\n      });\n    } catch (error) {\n      throw error;\n    }\n  }\n  async notification(notification) {\n    const jsonrpcNotification = {\n      ...notification,\n      jsonrpc: \"2.0\"\n    };\n    await this.transport.send(jsonrpcNotification);\n  }\n  /**\n   * Returns a set of AI SDK tools from the MCP server\n   * @returns A record of tool names to their implementations\n   */\n  async tools({\n    schemas = \"automatic\"\n  } = {}) {\n    var _a17;\n    const tools = {};\n    try {\n      const listToolsResult = await this.listTools();\n      for (const { name: name17, description, inputSchema } of listToolsResult.tools) {\n        if (schemas !== \"automatic\" && !(name17 in schemas)) {\n          continue;\n        }\n        const self = this;\n        const execute = async (args, options) => {\n          var _a18;\n          (_a18 = options == null ? void 0 : options.abortSignal) == null ? void 0 : _a18.throwIfAborted();\n          return self.callTool({ name: name17, args, options });\n        };\n        const toolWithExecute = schemas === \"automatic\" ? dynamicTool({\n          description,\n          inputSchema: jsonSchema({\n            ...inputSchema,\n            properties: (_a17 = inputSchema.properties) != null ? _a17 : {},\n            additionalProperties: false\n          }),\n          execute\n        }) : tool({\n          description,\n          inputSchema: schemas[name17].inputSchema,\n          execute\n        });\n        tools[name17] = toolWithExecute;\n      }\n      return tools;\n    } catch (error) {\n      throw error;\n    }\n  }\n  onClose() {\n    if (this.isClosed)\n      return;\n    this.isClosed = true;\n    const error = new MCPClientError({\n      message: \"Connection closed\"\n    });\n    for (const handler of this.responseHandlers.values()) {\n      handler(error);\n    }\n    this.responseHandlers.clear();\n  }\n  onError(error) {\n    if (this.onUncaughtError) {\n      this.onUncaughtError(error);\n    }\n  }\n  onResponse(response) {\n    const messageId = Number(response.id);\n    const handler = this.responseHandlers.get(messageId);\n    if (handler === void 0) {\n      throw new MCPClientError({\n        message: `Protocol error: Received a response for an unknown message ID: ${JSON.stringify(\n          response\n        )}`\n      });\n    }\n    this.responseHandlers.delete(messageId);\n    handler(\n      \"result\" in response ? response : new MCPClientError({\n        message: response.error.message,\n        cause: response.error\n      })\n    );\n  }\n};\n\n// src/error/no-transcript-generated-error.ts\nimport { AISDKError as AISDKError22 } from \"@ai-sdk/provider\";\nvar NoTranscriptGeneratedError = class extends AISDKError22 {\n  constructor(options) {\n    super({\n      name: \"AI_NoTranscriptGeneratedError\",\n      message: \"No transcript generated.\"\n    });\n    this.responses = options.responses;\n  }\n};\n\n// src/transcribe/transcribe.ts\nasync function transcribe({\n  model,\n  audio,\n  providerOptions = {},\n  maxRetries: maxRetriesArg,\n  abortSignal,\n  headers\n}) {\n  if (model.specificationVersion !== \"v2\") {\n    throw new UnsupportedModelVersionError({\n      version: model.specificationVersion,\n      provider: model.provider,\n      modelId: model.modelId\n    });\n  }\n  const { retry } = prepareRetries({\n    maxRetries: maxRetriesArg,\n    abortSignal\n  });\n  const audioData = audio instanceof URL ? (await download({ url: audio })).data : convertDataContentToUint8Array(audio);\n  const result = await retry(\n    () => {\n      var _a17;\n      return model.doGenerate({\n        audio: audioData,\n        abortSignal,\n        headers,\n        providerOptions,\n        mediaType: (_a17 = detectMediaType({\n          data: audioData,\n          signatures: audioMediaTypeSignatures\n        })) != null ? _a17 : \"audio/wav\"\n      });\n    }\n  );\n  logWarnings(result.warnings);\n  if (!result.text) {\n    throw new NoTranscriptGeneratedError({ responses: [result.response] });\n  }\n  return new DefaultTranscriptionResult({\n    text: result.text,\n    segments: result.segments,\n    language: result.language,\n    durationInSeconds: result.durationInSeconds,\n    warnings: result.warnings,\n    responses: [result.response],\n    providerMetadata: result.providerMetadata\n  });\n}\nvar DefaultTranscriptionResult = class {\n  constructor(options) {\n    var _a17;\n    this.text = options.text;\n    this.segments = options.segments;\n    this.language = options.language;\n    this.durationInSeconds = options.durationInSeconds;\n    this.warnings = options.warnings;\n    this.responses = options.responses;\n    this.providerMetadata = (_a17 = options.providerMetadata) != null ? _a17 : {};\n  }\n};\n\n// src/ui/call-completion-api.ts\nimport { parseJsonEventStream } from \"@ai-sdk/provider-utils\";\n\n// src/ui/process-text-stream.ts\nasync function processTextStream({\n  stream,\n  onTextPart\n}) {\n  const reader = stream.pipeThrough(new TextDecoderStream()).getReader();\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) {\n      break;\n    }\n    await onTextPart(value);\n  }\n}\n\n// src/ui/call-completion-api.ts\nvar getOriginalFetch = () => fetch;\nasync function callCompletionApi({\n  api,\n  prompt,\n  credentials,\n  headers,\n  body,\n  streamProtocol = \"data\",\n  setCompletion,\n  setLoading,\n  setError,\n  setAbortController,\n  onFinish,\n  onError,\n  fetch: fetch2 = getOriginalFetch()\n}) {\n  var _a17;\n  try {\n    setLoading(true);\n    setError(void 0);\n    const abortController = new AbortController();\n    setAbortController(abortController);\n    setCompletion(\"\");\n    const response = await fetch2(api, {\n      method: \"POST\",\n      body: JSON.stringify({\n        prompt,\n        ...body\n      }),\n      credentials,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        ...headers\n      },\n      signal: abortController.signal\n    }).catch((err) => {\n      throw err;\n    });\n    if (!response.ok) {\n      throw new Error(\n        (_a17 = await response.text()) != null ? _a17 : \"Failed to fetch the chat response.\"\n      );\n    }\n    if (!response.body) {\n      throw new Error(\"The response body is empty.\");\n    }\n    let result = \"\";\n    switch (streamProtocol) {\n      case \"text\": {\n        await processTextStream({\n          stream: response.body,\n          onTextPart: (chunk) => {\n            result += chunk;\n            setCompletion(result);\n          }\n        });\n        break;\n      }\n      case \"data\": {\n        await consumeStream({\n          stream: parseJsonEventStream({\n            stream: response.body,\n            schema: uiMessageChunkSchema\n          }).pipeThrough(\n            new TransformStream({\n              async transform(part) {\n                if (!part.success) {\n                  throw part.error;\n                }\n                const streamPart = part.value;\n                if (streamPart.type === \"text-delta\") {\n                  result += streamPart.delta;\n                  setCompletion(result);\n                } else if (streamPart.type === \"error\") {\n                  throw new Error(streamPart.errorText);\n                }\n              }\n            })\n          ),\n          onError: (error) => {\n            throw error;\n          }\n        });\n        break;\n      }\n      default: {\n        const exhaustiveCheck = streamProtocol;\n        throw new Error(`Unknown stream protocol: ${exhaustiveCheck}`);\n      }\n    }\n    if (onFinish) {\n      onFinish(prompt, result);\n    }\n    setAbortController(null);\n    return result;\n  } catch (err) {\n    if (err.name === \"AbortError\") {\n      setAbortController(null);\n      return null;\n    }\n    if (err instanceof Error) {\n      if (onError) {\n        onError(err);\n      }\n    }\n    setError(err);\n  } finally {\n    setLoading(false);\n  }\n}\n\n// src/ui/chat.ts\nimport {\n  generateId as generateIdFunc\n} from \"@ai-sdk/provider-utils\";\n\n// src/ui/convert-file-list-to-file-ui-parts.ts\nasync function convertFileListToFileUIParts(files) {\n  if (files == null) {\n    return [];\n  }\n  if (!globalThis.FileList || !(files instanceof globalThis.FileList)) {\n    throw new Error(\"FileList is not supported in the current environment\");\n  }\n  return Promise.all(\n    Array.from(files).map(async (file) => {\n      const { name: name17, type } = file;\n      const dataUrl = await new Promise((resolve2, reject) => {\n        const reader = new FileReader();\n        reader.onload = (readerEvent) => {\n          var _a17;\n          resolve2((_a17 = readerEvent.target) == null ? void 0 : _a17.result);\n        };\n        reader.onerror = (error) => reject(error);\n        reader.readAsDataURL(file);\n      });\n      return {\n        type: \"file\",\n        mediaType: type,\n        filename: name17,\n        url: dataUrl\n      };\n    })\n  );\n}\n\n// src/ui/default-chat-transport.ts\nimport { parseJsonEventStream as parseJsonEventStream2 } from \"@ai-sdk/provider-utils\";\n\n// src/ui/http-chat-transport.ts\nimport { resolve } from \"@ai-sdk/provider-utils\";\nvar HttpChatTransport = class {\n  constructor({\n    api = \"/api/chat\",\n    credentials,\n    headers,\n    body,\n    fetch: fetch2,\n    prepareSendMessagesRequest,\n    prepareReconnectToStreamRequest\n  }) {\n    this.api = api;\n    this.credentials = credentials;\n    this.headers = headers;\n    this.body = body;\n    this.fetch = fetch2;\n    this.prepareSendMessagesRequest = prepareSendMessagesRequest;\n    this.prepareReconnectToStreamRequest = prepareReconnectToStreamRequest;\n  }\n  async sendMessages({\n    abortSignal,\n    ...options\n  }) {\n    var _a17, _b, _c, _d, _e;\n    const resolvedBody = await resolve(this.body);\n    const resolvedHeaders = await resolve(this.headers);\n    const resolvedCredentials = await resolve(this.credentials);\n    const preparedRequest = await ((_a17 = this.prepareSendMessagesRequest) == null ? void 0 : _a17.call(this, {\n      api: this.api,\n      id: options.chatId,\n      messages: options.messages,\n      body: { ...resolvedBody, ...options.body },\n      headers: { ...resolvedHeaders, ...options.headers },\n      credentials: resolvedCredentials,\n      requestMetadata: options.metadata,\n      trigger: options.trigger,\n      messageId: options.messageId\n    }));\n    const api = (_b = preparedRequest == null ? void 0 : preparedRequest.api) != null ? _b : this.api;\n    const headers = (preparedRequest == null ? void 0 : preparedRequest.headers) !== void 0 ? preparedRequest.headers : { ...resolvedHeaders, ...options.headers };\n    const body = (preparedRequest == null ? void 0 : preparedRequest.body) !== void 0 ? preparedRequest.body : {\n      ...resolvedBody,\n      ...options.body,\n      id: options.chatId,\n      messages: options.messages,\n      trigger: options.trigger,\n      messageId: options.messageId\n    };\n    const credentials = (_c = preparedRequest == null ? void 0 : preparedRequest.credentials) != null ? _c : resolvedCredentials;\n    const fetch2 = (_d = this.fetch) != null ? _d : globalThis.fetch;\n    const response = await fetch2(api, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        ...headers\n      },\n      body: JSON.stringify(body),\n      credentials,\n      signal: abortSignal\n    });\n    if (!response.ok) {\n      throw new Error(\n        (_e = await response.text()) != null ? _e : \"Failed to fetch the chat response.\"\n      );\n    }\n    if (!response.body) {\n      throw new Error(\"The response body is empty.\");\n    }\n    return this.processResponseStream(response.body);\n  }\n  async reconnectToStream(options) {\n    var _a17, _b, _c, _d, _e;\n    const resolvedBody = await resolve(this.body);\n    const resolvedHeaders = await resolve(this.headers);\n    const resolvedCredentials = await resolve(this.credentials);\n    const preparedRequest = await ((_a17 = this.prepareReconnectToStreamRequest) == null ? void 0 : _a17.call(this, {\n      api: this.api,\n      id: options.chatId,\n      body: { ...resolvedBody, ...options.body },\n      headers: { ...resolvedHeaders, ...options.headers },\n      credentials: resolvedCredentials,\n      requestMetadata: options.metadata\n    }));\n    const api = (_b = preparedRequest == null ? void 0 : preparedRequest.api) != null ? _b : `${this.api}/${options.chatId}/stream`;\n    const headers = (preparedRequest == null ? void 0 : preparedRequest.headers) !== void 0 ? preparedRequest.headers : { ...resolvedHeaders, ...options.headers };\n    const credentials = (_c = preparedRequest == null ? void 0 : preparedRequest.credentials) != null ? _c : resolvedCredentials;\n    const fetch2 = (_d = this.fetch) != null ? _d : globalThis.fetch;\n    const response = await fetch2(api, {\n      method: \"GET\",\n      headers,\n      credentials\n    });\n    if (response.status === 204) {\n      return null;\n    }\n    if (!response.ok) {\n      throw new Error(\n        (_e = await response.text()) != null ? _e : \"Failed to fetch the chat response.\"\n      );\n    }\n    if (!response.body) {\n      throw new Error(\"The response body is empty.\");\n    }\n    return this.processResponseStream(response.body);\n  }\n};\n\n// src/ui/default-chat-transport.ts\nvar DefaultChatTransport = class extends HttpChatTransport {\n  constructor(options = {}) {\n    super(options);\n  }\n  processResponseStream(stream) {\n    return parseJsonEventStream2({\n      stream,\n      schema: uiMessageChunkSchema\n    }).pipeThrough(\n      new TransformStream({\n        async transform(chunk, controller) {\n          if (!chunk.success) {\n            throw chunk.error;\n          }\n          controller.enqueue(chunk.value);\n        }\n      })\n    );\n  }\n};\n\n// src/ui/chat.ts\nvar AbstractChat = class {\n  constructor({\n    generateId: generateId3 = generateIdFunc,\n    id = generateId3(),\n    transport = new DefaultChatTransport(),\n    messageMetadataSchema,\n    dataPartSchemas,\n    state,\n    onError,\n    onToolCall,\n    onFinish,\n    onData,\n    sendAutomaticallyWhen\n  }) {\n    this.activeResponse = void 0;\n    this.jobExecutor = new SerialJobExecutor();\n    /**\n     * Appends or replaces a user message to the chat list. This triggers the API call to fetch\n     * the assistant's response.\n     *\n     * If a messageId is provided, the message will be replaced.\n     */\n    this.sendMessage = async (message, options) => {\n      var _a17, _b, _c, _d;\n      if (message == null) {\n        await this.makeRequest({\n          trigger: \"submit-message\",\n          messageId: (_a17 = this.lastMessage) == null ? void 0 : _a17.id,\n          ...options\n        });\n        return;\n      }\n      let uiMessage;\n      if (\"text\" in message || \"files\" in message) {\n        const fileParts = Array.isArray(message.files) ? message.files : await convertFileListToFileUIParts(message.files);\n        uiMessage = {\n          parts: [\n            ...fileParts,\n            ...\"text\" in message && message.text != null ? [{ type: \"text\", text: message.text }] : []\n          ]\n        };\n      } else {\n        uiMessage = message;\n      }\n      if (message.messageId != null) {\n        const messageIndex = this.state.messages.findIndex(\n          (m) => m.id === message.messageId\n        );\n        if (messageIndex === -1) {\n          throw new Error(`message with id ${message.messageId} not found`);\n        }\n        if (this.state.messages[messageIndex].role !== \"user\") {\n          throw new Error(\n            `message with id ${message.messageId} is not a user message`\n          );\n        }\n        this.state.messages = this.state.messages.slice(0, messageIndex + 1);\n        this.state.replaceMessage(messageIndex, {\n          ...uiMessage,\n          id: message.messageId,\n          role: (_b = uiMessage.role) != null ? _b : \"user\",\n          metadata: message.metadata\n        });\n      } else {\n        this.state.pushMessage({\n          ...uiMessage,\n          id: (_c = uiMessage.id) != null ? _c : this.generateId(),\n          role: (_d = uiMessage.role) != null ? _d : \"user\",\n          metadata: message.metadata\n        });\n      }\n      await this.makeRequest({\n        trigger: \"submit-message\",\n        messageId: message.messageId,\n        ...options\n      });\n    };\n    /**\n     * Regenerate the assistant message with the provided message id.\n     * If no message id is provided, the last assistant message will be regenerated.\n     */\n    this.regenerate = async ({\n      messageId,\n      ...options\n    } = {}) => {\n      const messageIndex = messageId == null ? this.state.messages.length - 1 : this.state.messages.findIndex((message) => message.id === messageId);\n      if (messageIndex === -1) {\n        throw new Error(`message ${messageId} not found`);\n      }\n      this.state.messages = this.state.messages.slice(\n        0,\n        // if the message is a user message, we need to include it in the request:\n        this.messages[messageIndex].role === \"assistant\" ? messageIndex : messageIndex + 1\n      );\n      await this.makeRequest({\n        trigger: \"regenerate-message\",\n        messageId,\n        ...options\n      });\n    };\n    /**\n     * Attempt to resume an ongoing streaming response.\n     */\n    this.resumeStream = async (options = {}) => {\n      await this.makeRequest({ trigger: \"resume-stream\", ...options });\n    };\n    /**\n     * Clear the error state and set the status to ready if the chat is in an error state.\n     */\n    this.clearError = () => {\n      if (this.status === \"error\") {\n        this.state.error = void 0;\n        this.setStatus({ status: \"ready\" });\n      }\n    };\n    this.addToolResult = async ({\n      tool: tool3,\n      toolCallId,\n      output\n    }) => this.jobExecutor.run(async () => {\n      var _a17, _b;\n      const messages = this.state.messages;\n      const lastMessage = messages[messages.length - 1];\n      this.state.replaceMessage(messages.length - 1, {\n        ...lastMessage,\n        parts: lastMessage.parts.map(\n          (part) => isToolOrDynamicToolUIPart(part) && part.toolCallId === toolCallId ? { ...part, state: \"output-available\", output } : part\n        )\n      });\n      if (this.activeResponse) {\n        this.activeResponse.state.message.parts = this.activeResponse.state.message.parts.map(\n          (part) => isToolOrDynamicToolUIPart(part) && part.toolCallId === toolCallId ? {\n            ...part,\n            state: \"output-available\",\n            output,\n            errorText: void 0\n          } : part\n        );\n      }\n      if (this.status !== \"streaming\" && this.status !== \"submitted\" && ((_a17 = this.sendAutomaticallyWhen) == null ? void 0 : _a17.call(this, { messages: this.state.messages }))) {\n        this.makeRequest({\n          trigger: \"submit-message\",\n          messageId: (_b = this.lastMessage) == null ? void 0 : _b.id\n        });\n      }\n    });\n    /**\n     * Abort the current request immediately, keep the generated tokens if any.\n     */\n    this.stop = async () => {\n      var _a17;\n      if (this.status !== \"streaming\" && this.status !== \"submitted\")\n        return;\n      if ((_a17 = this.activeResponse) == null ? void 0 : _a17.abortController) {\n        this.activeResponse.abortController.abort();\n      }\n    };\n    this.id = id;\n    this.transport = transport;\n    this.generateId = generateId3;\n    this.messageMetadataSchema = messageMetadataSchema;\n    this.dataPartSchemas = dataPartSchemas;\n    this.state = state;\n    this.onError = onError;\n    this.onToolCall = onToolCall;\n    this.onFinish = onFinish;\n    this.onData = onData;\n    this.sendAutomaticallyWhen = sendAutomaticallyWhen;\n  }\n  /**\n   * Hook status:\n   *\n   * - `submitted`: The message has been sent to the API and we're awaiting the start of the response stream.\n   * - `streaming`: The response is actively streaming in from the API, receiving chunks of data.\n   * - `ready`: The full response has been received and processed; a new user message can be submitted.\n   * - `error`: An error occurred during the API request, preventing successful completion.\n   */\n  get status() {\n    return this.state.status;\n  }\n  setStatus({\n    status,\n    error\n  }) {\n    if (this.status === status)\n      return;\n    this.state.status = status;\n    this.state.error = error;\n  }\n  get error() {\n    return this.state.error;\n  }\n  get messages() {\n    return this.state.messages;\n  }\n  get lastMessage() {\n    return this.state.messages[this.state.messages.length - 1];\n  }\n  set messages(messages) {\n    this.state.messages = messages;\n  }\n  async makeRequest({\n    trigger,\n    metadata,\n    headers,\n    body,\n    messageId\n  }) {\n    var _a17, _b, _c;\n    this.setStatus({ status: \"submitted\", error: void 0 });\n    const lastMessage = this.lastMessage;\n    let isAbort = false;\n    let isDisconnect = false;\n    let isError = false;\n    try {\n      const activeResponse = {\n        state: createStreamingUIMessageState({\n          lastMessage: this.state.snapshot(lastMessage),\n          messageId: this.generateId()\n        }),\n        abortController: new AbortController()\n      };\n      activeResponse.abortController.signal.addEventListener(\"abort\", () => {\n        isAbort = true;\n      });\n      this.activeResponse = activeResponse;\n      let stream;\n      if (trigger === \"resume-stream\") {\n        const reconnect = await this.transport.reconnectToStream({\n          chatId: this.id,\n          metadata,\n          headers,\n          body\n        });\n        if (reconnect == null) {\n          this.setStatus({ status: \"ready\" });\n          return;\n        }\n        stream = reconnect;\n      } else {\n        stream = await this.transport.sendMessages({\n          chatId: this.id,\n          messages: this.state.messages,\n          abortSignal: activeResponse.abortController.signal,\n          metadata,\n          headers,\n          body,\n          trigger,\n          messageId\n        });\n      }\n      const runUpdateMessageJob = (job) => (\n        // serialize the job execution to avoid race conditions:\n        this.jobExecutor.run(\n          () => job({\n            state: activeResponse.state,\n            write: () => {\n              var _a18;\n              this.setStatus({ status: \"streaming\" });\n              const replaceLastMessage = activeResponse.state.message.id === ((_a18 = this.lastMessage) == null ? void 0 : _a18.id);\n              if (replaceLastMessage) {\n                this.state.replaceMessage(\n                  this.state.messages.length - 1,\n                  activeResponse.state.message\n                );\n              } else {\n                this.state.pushMessage(activeResponse.state.message);\n              }\n            }\n          })\n        )\n      );\n      await consumeStream({\n        stream: processUIMessageStream({\n          stream,\n          onToolCall: this.onToolCall,\n          onData: this.onData,\n          messageMetadataSchema: this.messageMetadataSchema,\n          dataPartSchemas: this.dataPartSchemas,\n          runUpdateMessageJob,\n          onError: (error) => {\n            throw error;\n          }\n        }),\n        onError: (error) => {\n          throw error;\n        }\n      });\n      this.setStatus({ status: \"ready\" });\n    } catch (err) {\n      if (isAbort || err.name === \"AbortError\") {\n        isAbort = true;\n        this.setStatus({ status: \"ready\" });\n        return null;\n      }\n      isError = true;\n      if (err instanceof TypeError && (err.message.toLowerCase().includes(\"fetch\") || err.message.toLowerCase().includes(\"network\"))) {\n        isDisconnect = true;\n      }\n      if (this.onError && err instanceof Error) {\n        this.onError(err);\n      }\n      this.setStatus({ status: \"error\", error: err });\n    } finally {\n      try {\n        (_a17 = this.onFinish) == null ? void 0 : _a17.call(this, {\n          message: this.activeResponse.state.message,\n          messages: this.state.messages,\n          isAbort,\n          isDisconnect,\n          isError\n        });\n      } catch (err) {\n        console.error(err);\n      }\n      this.activeResponse = void 0;\n    }\n    if ((_b = this.sendAutomaticallyWhen) == null ? void 0 : _b.call(this, { messages: this.state.messages })) {\n      await this.makeRequest({\n        trigger: \"submit-message\",\n        messageId: (_c = this.lastMessage) == null ? void 0 : _c.id,\n        metadata,\n        headers,\n        body\n      });\n    }\n  }\n};\n\n// src/ui/last-assistant-message-is-complete-with-tool-calls.ts\nfunction lastAssistantMessageIsCompleteWithToolCalls({\n  messages\n}) {\n  const message = messages[messages.length - 1];\n  if (!message) {\n    return false;\n  }\n  if (message.role !== \"assistant\") {\n    return false;\n  }\n  const lastStepStartIndex = message.parts.reduce((lastIndex, part, index) => {\n    return part.type === \"step-start\" ? index : lastIndex;\n  }, -1);\n  const lastStepToolInvocations = message.parts.slice(lastStepStartIndex + 1).filter(isToolOrDynamicToolUIPart);\n  return lastStepToolInvocations.length > 0 && lastStepToolInvocations.every((part) => part.state === \"output-available\");\n}\n\n// src/ui/transform-text-to-ui-message-stream.ts\nfunction transformTextToUiMessageStream({\n  stream\n}) {\n  return stream.pipeThrough(\n    new TransformStream({\n      start(controller) {\n        controller.enqueue({ type: \"start\" });\n        controller.enqueue({ type: \"start-step\" });\n        controller.enqueue({ type: \"text-start\", id: \"text-1\" });\n      },\n      async transform(part, controller) {\n        controller.enqueue({ type: \"text-delta\", id: \"text-1\", delta: part });\n      },\n      async flush(controller) {\n        controller.enqueue({ type: \"text-end\", id: \"text-1\" });\n        controller.enqueue({ type: \"finish-step\" });\n        controller.enqueue({ type: \"finish\" });\n      }\n    })\n  );\n}\n\n// src/ui/text-stream-chat-transport.ts\nvar TextStreamChatTransport = class extends HttpChatTransport {\n  constructor(options = {}) {\n    super(options);\n  }\n  processResponseStream(stream) {\n    return transformTextToUiMessageStream({\n      stream: stream.pipeThrough(new TextDecoderStream())\n    });\n  }\n};\n\n// src/ui/validate-ui-messages.ts\nimport { TypeValidationError as TypeValidationError4 } from \"@ai-sdk/provider\";\nimport {\n  validateTypes as validateTypes2\n} from \"@ai-sdk/provider-utils\";\nimport { z as z10 } from \"zod/v4\";\nvar textUIPartSchema = z10.object({\n  type: z10.literal(\"text\"),\n  text: z10.string(),\n  state: z10.enum([\"streaming\", \"done\"]).optional(),\n  providerMetadata: providerMetadataSchema.optional()\n});\nvar reasoningUIPartSchema = z10.object({\n  type: z10.literal(\"reasoning\"),\n  text: z10.string(),\n  state: z10.enum([\"streaming\", \"done\"]).optional(),\n  providerMetadata: providerMetadataSchema.optional()\n});\nvar sourceUrlUIPartSchema = z10.object({\n  type: z10.literal(\"source-url\"),\n  sourceId: z10.string(),\n  url: z10.string(),\n  title: z10.string().optional(),\n  providerMetadata: providerMetadataSchema.optional()\n});\nvar sourceDocumentUIPartSchema = z10.object({\n  type: z10.literal(\"source-document\"),\n  sourceId: z10.string(),\n  mediaType: z10.string(),\n  title: z10.string(),\n  filename: z10.string().optional(),\n  providerMetadata: providerMetadataSchema.optional()\n});\nvar fileUIPartSchema = z10.object({\n  type: z10.literal(\"file\"),\n  mediaType: z10.string(),\n  filename: z10.string().optional(),\n  url: z10.string(),\n  providerMetadata: providerMetadataSchema.optional()\n});\nvar stepStartUIPartSchema = z10.object({\n  type: z10.literal(\"step-start\")\n});\nvar dataUIPartSchema = z10.object({\n  type: z10.string().startsWith(\"data-\"),\n  id: z10.string().optional(),\n  data: z10.unknown()\n});\nvar dynamicToolUIPartSchemas = [\n  z10.object({\n    type: z10.literal(\"dynamic-tool\"),\n    toolName: z10.string(),\n    toolCallId: z10.string(),\n    state: z10.literal(\"input-streaming\"),\n    input: z10.unknown().optional(),\n    output: z10.never().optional(),\n    errorText: z10.never().optional()\n  }),\n  z10.object({\n    type: z10.literal(\"dynamic-tool\"),\n    toolName: z10.string(),\n    toolCallId: z10.string(),\n    state: z10.literal(\"input-available\"),\n    input: z10.unknown(),\n    output: z10.never().optional(),\n    errorText: z10.never().optional(),\n    callProviderMetadata: providerMetadataSchema.optional()\n  }),\n  z10.object({\n    type: z10.literal(\"dynamic-tool\"),\n    toolName: z10.string(),\n    toolCallId: z10.string(),\n    state: z10.literal(\"output-available\"),\n    input: z10.unknown(),\n    output: z10.unknown(),\n    errorText: z10.never().optional(),\n    callProviderMetadata: providerMetadataSchema.optional(),\n    preliminary: z10.boolean().optional()\n  }),\n  z10.object({\n    type: z10.literal(\"dynamic-tool\"),\n    toolName: z10.string(),\n    toolCallId: z10.string(),\n    state: z10.literal(\"output-error\"),\n    input: z10.unknown(),\n    output: z10.never().optional(),\n    errorText: z10.string(),\n    callProviderMetadata: providerMetadataSchema.optional()\n  })\n];\nvar toolUIPartSchemas = [\n  z10.object({\n    type: z10.string().startsWith(\"tool-\"),\n    toolCallId: z10.string(),\n    state: z10.literal(\"input-streaming\"),\n    input: z10.unknown().optional(),\n    output: z10.never().optional(),\n    errorText: z10.never().optional()\n  }),\n  z10.object({\n    type: z10.string().startsWith(\"tool-\"),\n    toolCallId: z10.string(),\n    state: z10.literal(\"input-available\"),\n    input: z10.unknown(),\n    output: z10.never().optional(),\n    errorText: z10.never().optional(),\n    callProviderMetadata: providerMetadataSchema.optional()\n  }),\n  z10.object({\n    type: z10.string().startsWith(\"tool-\"),\n    toolCallId: z10.string(),\n    state: z10.literal(\"output-available\"),\n    input: z10.unknown(),\n    output: z10.unknown(),\n    errorText: z10.never().optional(),\n    callProviderMetadata: providerMetadataSchema.optional(),\n    preliminary: z10.boolean().optional()\n  }),\n  z10.object({\n    type: z10.string().startsWith(\"tool-\"),\n    toolCallId: z10.string(),\n    state: z10.literal(\"output-error\"),\n    input: z10.unknown(),\n    output: z10.never().optional(),\n    errorText: z10.string(),\n    callProviderMetadata: providerMetadataSchema.optional()\n  })\n];\nvar uiMessageSchema = z10.object({\n  id: z10.string(),\n  role: z10.enum([\"system\", \"user\", \"assistant\"]),\n  metadata: z10.unknown().optional(),\n  parts: z10.array(\n    z10.union([\n      textUIPartSchema,\n      reasoningUIPartSchema,\n      sourceUrlUIPartSchema,\n      sourceDocumentUIPartSchema,\n      fileUIPartSchema,\n      stepStartUIPartSchema,\n      dataUIPartSchema,\n      ...dynamicToolUIPartSchemas,\n      ...toolUIPartSchemas\n    ])\n  )\n});\nasync function validateUIMessages({\n  messages,\n  metadataSchema,\n  dataSchemas,\n  tools\n}) {\n  if (messages == null) {\n    throw new InvalidArgumentError({\n      parameter: \"messages\",\n      value: messages,\n      message: \"messages parameter must be provided\"\n    });\n  }\n  const validatedMessages = await validateTypes2({\n    value: messages,\n    schema: z10.array(uiMessageSchema)\n  });\n  if (metadataSchema) {\n    for (const message of validatedMessages) {\n      await validateTypes2({\n        value: message.metadata,\n        schema: metadataSchema\n      });\n    }\n  }\n  if (dataSchemas) {\n    for (const message of validatedMessages) {\n      const dataParts = message.parts.filter(\n        (part) => part.type.startsWith(\"data-\")\n      );\n      for (const dataPart of dataParts) {\n        const dataName = dataPart.type.slice(5);\n        const dataSchema = dataSchemas[dataName];\n        if (!dataSchema) {\n          throw new TypeValidationError4({\n            value: dataPart.data,\n            cause: `No data schema found for data part ${dataName}`\n          });\n        }\n        await validateTypes2({\n          value: dataPart.data,\n          schema: dataSchema\n        });\n      }\n    }\n  }\n  if (tools) {\n    for (const message of validatedMessages) {\n      const toolParts = message.parts.filter(\n        (part) => part.type.startsWith(\"tool-\")\n      );\n      for (const toolPart of toolParts) {\n        const toolName = toolPart.type.slice(5);\n        const tool3 = tools[toolName];\n        if (!tool3) {\n          throw new TypeValidationError4({\n            value: toolPart.input,\n            cause: `No tool schema found for tool part ${toolName}`\n          });\n        }\n        if (toolPart.state === \"input-available\" || toolPart.state === \"output-available\" || toolPart.state === \"output-error\") {\n          await validateTypes2({\n            value: toolPart.input,\n            schema: tool3.inputSchema\n          });\n        }\n        if (toolPart.state === \"output-available\" && tool3.outputSchema) {\n          await validateTypes2({\n            value: toolPart.output,\n            schema: tool3.outputSchema\n          });\n        }\n      }\n    }\n  }\n  return validatedMessages;\n}\n\n// src/ui-message-stream/create-ui-message-stream.ts\nimport {\n  generateId as generateIdFunc2,\n  getErrorMessage as getErrorMessage8\n} from \"@ai-sdk/provider-utils\";\nfunction createUIMessageStream({\n  execute,\n  onError = getErrorMessage8,\n  originalMessages,\n  onFinish,\n  generateId: generateId3 = generateIdFunc2\n}) {\n  let controller;\n  const ongoingStreamPromises = [];\n  const stream = new ReadableStream({\n    start(controllerArg) {\n      controller = controllerArg;\n    }\n  });\n  function safeEnqueue(data) {\n    try {\n      controller.enqueue(data);\n    } catch (error) {\n    }\n  }\n  try {\n    const result = execute({\n      writer: {\n        write(part) {\n          safeEnqueue(part);\n        },\n        merge(streamArg) {\n          ongoingStreamPromises.push(\n            (async () => {\n              const reader = streamArg.getReader();\n              while (true) {\n                const { done, value } = await reader.read();\n                if (done)\n                  break;\n                safeEnqueue(value);\n              }\n            })().catch((error) => {\n              safeEnqueue({\n                type: \"error\",\n                errorText: onError(error)\n              });\n            })\n          );\n        },\n        onError\n      }\n    });\n    if (result) {\n      ongoingStreamPromises.push(\n        result.catch((error) => {\n          safeEnqueue({\n            type: \"error\",\n            errorText: onError(error)\n          });\n        })\n      );\n    }\n  } catch (error) {\n    safeEnqueue({\n      type: \"error\",\n      errorText: onError(error)\n    });\n  }\n  const waitForStreams = new Promise(async (resolve2) => {\n    while (ongoingStreamPromises.length > 0) {\n      await ongoingStreamPromises.shift();\n    }\n    resolve2();\n  });\n  waitForStreams.finally(() => {\n    try {\n      controller.close();\n    } catch (error) {\n    }\n  });\n  return handleUIMessageStreamFinish({\n    stream,\n    messageId: generateId3(),\n    originalMessages,\n    onFinish,\n    onError\n  });\n}\n\n// src/ui-message-stream/read-ui-message-stream.ts\nfunction readUIMessageStream({\n  message,\n  stream,\n  onError,\n  terminateOnError = false\n}) {\n  var _a17;\n  let controller;\n  let hasErrored = false;\n  const outputStream = new ReadableStream({\n    start(controllerParam) {\n      controller = controllerParam;\n    }\n  });\n  const state = createStreamingUIMessageState({\n    messageId: (_a17 = message == null ? void 0 : message.id) != null ? _a17 : \"\",\n    lastMessage: message\n  });\n  const handleError = (error) => {\n    onError == null ? void 0 : onError(error);\n    if (!hasErrored && terminateOnError) {\n      hasErrored = true;\n      controller == null ? void 0 : controller.error(error);\n    }\n  };\n  consumeStream({\n    stream: processUIMessageStream({\n      stream,\n      runUpdateMessageJob(job) {\n        return job({\n          state,\n          write: () => {\n            controller == null ? void 0 : controller.enqueue(structuredClone(state.message));\n          }\n        });\n      },\n      onError: handleError\n    }),\n    onError: handleError\n  }).finally(() => {\n    if (!hasErrored) {\n      controller == null ? void 0 : controller.close();\n    }\n  });\n  return createAsyncIterableStream(outputStream);\n}\nexport {\n  AISDKError17 as AISDKError,\n  APICallError,\n  AbstractChat,\n  DefaultChatTransport,\n  DownloadError,\n  EmptyResponseBodyError,\n  Agent as Experimental_Agent,\n  HttpChatTransport,\n  InvalidArgumentError,\n  InvalidDataContentError,\n  InvalidMessageRoleError,\n  InvalidPromptError,\n  InvalidResponseDataError,\n  InvalidStreamPartError,\n  InvalidToolInputError,\n  JSONParseError,\n  JsonToSseTransformStream,\n  LoadAPIKeyError,\n  MCPClientError,\n  MessageConversionError,\n  NoContentGeneratedError,\n  NoImageGeneratedError,\n  NoObjectGeneratedError,\n  NoOutputGeneratedError,\n  NoOutputSpecifiedError,\n  NoSuchModelError,\n  NoSuchProviderError,\n  NoSuchToolError,\n  output_exports as Output,\n  RetryError,\n  SerialJobExecutor,\n  TextStreamChatTransport,\n  TooManyEmbeddingValuesForCallError,\n  ToolCallRepairError,\n  TypeValidationError,\n  UI_MESSAGE_STREAM_HEADERS,\n  UnsupportedFunctionalityError,\n  UnsupportedModelVersionError,\n  asSchema5 as asSchema,\n  assistantModelMessageSchema,\n  callCompletionApi,\n  consumeStream,\n  convertFileListToFileUIParts,\n  convertToCoreMessages,\n  convertToModelMessages,\n  coreAssistantMessageSchema,\n  coreMessageSchema,\n  coreSystemMessageSchema,\n  coreToolMessageSchema,\n  coreUserMessageSchema,\n  cosineSimilarity,\n  createGateway,\n  createIdGenerator5 as createIdGenerator,\n  createProviderRegistry,\n  createTextStreamResponse,\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n  customProvider,\n  defaultSettingsMiddleware,\n  dynamicTool2 as dynamicTool,\n  embed,\n  embedMany,\n  createMCPClient as experimental_createMCPClient,\n  experimental_createProviderRegistry,\n  experimental_customProvider,\n  generateImage as experimental_generateImage,\n  generateSpeech as experimental_generateSpeech,\n  transcribe as experimental_transcribe,\n  extractReasoningMiddleware,\n  gateway2 as gateway,\n  generateId2 as generateId,\n  generateObject,\n  generateText,\n  getTextFromDataUrl,\n  getToolName,\n  getToolOrDynamicToolName,\n  hasToolCall,\n  isDeepEqualData,\n  isToolOrDynamicToolUIPart,\n  isToolUIPart,\n  jsonSchema2 as jsonSchema,\n  lastAssistantMessageIsCompleteWithToolCalls,\n  modelMessageSchema,\n  parsePartialJson,\n  pipeTextStreamToResponse,\n  pipeUIMessageStreamToResponse,\n  readUIMessageStream,\n  simulateReadableStream,\n  simulateStreamingMiddleware,\n  smoothStream,\n  stepCountIs,\n  streamObject,\n  streamText,\n  systemModelMessageSchema,\n  tool2 as tool,\n  toolModelMessageSchema,\n  userModelMessageSchema,\n  validateUIMessages,\n  wrapLanguageModel,\n  wrapProvider,\n  zodSchema\n};\n//# sourceMappingURL=index.mjs.map"],"names":["_a7","__defProp","Object","defineProperty","Symbol","for","_ai_sdk_provider__WEBPACK_IMPORTED_MODULE_0__","AX","name7","marker7","symbol7","NoObjectGeneratedError","constructor","message","cause","text","text2","response","usage","finishReason","name","isInstance","error","hasMarker","dataContentSchema","zod_v4__WEBPACK_IMPORTED_MODULE_1__","G0","Z_","Pp","Uint8Array","ArrayBuffer","PG","_a17","_b","globalThis","Buffer","isBuffer","value","jsonValueSchema","Vo","lB","Rx","O7","IM","IX","providerMetadataSchema","textPartSchema","Ry","type","i0","providerOptions","optional","imagePartSchema","image","URL","mediaType","filePartSchema","data","filename","reasoningPartSchema","toolCallPartSchema","toolCallId","toolName","input","_4","providerExecuted","outputSchema","VK","toolResultPartSchema","output","systemModelMessageSchema","role","content","userModelMessageSchema","assistantModelMessageSchema","toolModelMessageSchema","_ai_sdk_provider_utils__WEBPACK_IMPORTED_MODULE_2__","bF","prefix","size","TransformStream","uiMessageChunkSchema","cf","id","providerMetadata","delta","errorText","dynamic","inputTextDelta","preliminary","sourceId","url","title","startsWith","transient","messageId","messageMetadata","parsePartialJson","jsonText","state","result","NX","success","fixJson","stack","lastValidIndex","literalStart","processValueStart","char","i","swapState","pop","push","processAfterObjectValue","processAfterArrayValue","length","partialLiteral","substring","slice","isToolOrDynamicToolUIPart","part","isToolUIPart","__export","target","all","name17","get","enumerable","object","responseFormat","parsePartial","partial","parseOutput","schema","inputSchema","C3","jsonSchema","_exhaustiveCheck","context","parseResult","validationResult","pW","JJ","ClientOrServerImplementationSchema","d7","version","BaseParamsSchema","_meta","jt","loose","RequestSchema","method","params","ServerCapabilitiesSchema","experimental","logging","prompts","listChanged","resources","subscribe","tools","ResultSchema","extend","protocolVersion","capabilities","serverInfo","instructions","PaginatedResultSchema","nextCursor","ToolSchema","description","properties","TextContentSchema","ImageContentSchema","US","mimeType","ResourceContentsSchema","uri","TextResourceContentsSchema","BlobResourceContentsSchema","blob","EmbeddedResourceSchema","resource","isError","default","or","toolResult","JSONRPCRequestSchema","jsonrpc","int","merge","strict","JSONRPCResponseSchema","JSONRPCErrorSchema","code","JSONRPCNotificationSchema","HttpChatTransport","api","credentials","headers","body","fetch","fetch2","prepareSendMessagesRequest","prepareReconnectToStreamRequest","sendMessages","abortSignal","options","_c","_d","_e","resolvedBody","DB","resolvedHeaders","resolvedCredentials","preparedRequest","call","chatId","messages","requestMetadata","metadata","trigger","JSON","stringify","signal","ok","processResponseStream","reconnectToStream","status","DefaultChatTransport","stream","RF","pipeThrough","transform","chunk","controller","enqueue","lastAssistantMessageIsCompleteWithToolCalls","lastStepStartIndex","parts","reduce","lastIndex","index","lastStepToolInvocations","filter","every","textUIPartSchema","Km","reasoningUIPartSchema","sourceUrlUIPartSchema","sourceDocumentUIPartSchema","fileUIPartSchema","stepStartUIPartSchema","dataUIPartSchema","dynamicToolUIPartSchemas","Fi","callProviderMetadata","toolUIPartSchemas"],"sourceRoot":""}